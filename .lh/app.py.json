{
    "sourceFile": "app.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 60,
            "patches": [
                {
                    "date": 1765626353157,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1765627006652,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,14 +1,49 @@\n \r\n from flask import Flask, render_template, jsonify, request\r\n from dotenv import load_dotenv\r\n+import os\r\n+from src.helper import download_embeddings\r\n \r\n+from pinecone import Pinecone\r\n+from langchain_huggingface import HuggingFaceEmbeddings\r\n+from pinecone import Pinecone, ServerlessSpec\r\n+from langchain_pinecone import PineconeVectorStore\r\n+from operator import itemgetter\r\n+from langchain_core.prompts import ChatPromptTemplate\r\n+from langchain_core.runnables import RunnableLambda\r\n+from langchain_groq import ChatGroq\r\n+from src.prompt import *\r\n \r\n \r\n-\r\n-\r\n app=Flask(__name__)\r\n \r\n \r\n load_dotenv()\r\n \r\n \r\n+PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\r\n+GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\r\n+\r\n+\r\n+os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\r\n+os.environ[\"GROQ_API_KEY\"]=GROQ_API_KEY\r\n+\r\n+\r\n+embeddings=download_embeddings()\r\n+\r\n+index_name=\"medical_chatbot\"\r\n+#Embedding earch chunk and upsert the embeddings into your pinecone index.\r\n+\r\n+docsearch=PineconeVectorStore.from_existing_index(\r\n+    index_name=index_name,\r\n+    embedding=embeddings\r\n+)\r\n+\r\n+\r\n+\r\n+retriever=docsearch.as_retriever(\r\n+    search_type=\"similarity\",\r\n+    search_kwargs={\"k\":3})\r\n+\r\n+\r\n+chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n"
                },
                {
                    "date": 1765627864198,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,10 +2,8 @@\n from flask import Flask, render_template, jsonify, request\r\n from dotenv import load_dotenv\r\n import os\r\n from src.helper import download_embeddings\r\n-\r\n-from pinecone import Pinecone\r\n from langchain_huggingface import HuggingFaceEmbeddings\r\n from pinecone import Pinecone, ServerlessSpec\r\n from langchain_pinecone import PineconeVectorStore\r\n from operator import itemgetter\r\n@@ -46,4 +44,15 @@\n     search_kwargs={\"k\":3})\r\n \r\n \r\n chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+\r\n+prompt=ChatPromptTemplate.from_messages(\r\n+    [\r\n+        (\"system\",system_prompt),\r\n+        (\"human\",\"{input}\"),\r\n+    ]\r\n+)\r\n+\r\n+@app.route(\"/\")\r\n+def index():\r\n+    return render_template(\"chat.html\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765627887648,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,5 +54,11 @@\n )\r\n \r\n @app.route(\"/\")\r\n def index():\r\n-    return render_template(\"chat.html\")\n\\ No newline at end of file\n+    return render_template(\"chat.html\")\r\n+\r\n+\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765628869815,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,45 +15,9 @@\n \r\n app=Flask(__name__)\r\n \r\n \r\n-load_dotenv()\r\n \r\n-\r\n-PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\r\n-GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\r\n-\r\n-\r\n-os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\r\n-os.environ[\"GROQ_API_KEY\"]=GROQ_API_KEY\r\n-\r\n-\r\n-embeddings=download_embeddings()\r\n-\r\n-index_name=\"medical_chatbot\"\r\n-#Embedding earch chunk and upsert the embeddings into your pinecone index.\r\n-\r\n-docsearch=PineconeVectorStore.from_existing_index(\r\n-    index_name=index_name,\r\n-    embedding=embeddings\r\n-)\r\n-\r\n-\r\n-\r\n-retriever=docsearch.as_retriever(\r\n-    search_type=\"similarity\",\r\n-    search_kwargs={\"k\":3})\r\n-\r\n-\r\n-chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n-\r\n-prompt=ChatPromptTemplate.from_messages(\r\n-    [\r\n-        (\"system\",system_prompt),\r\n-        (\"human\",\"{input}\"),\r\n-    ]\r\n-)\r\n-\r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n"
                },
                {
                    "date": 1765629109767,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,9 +15,45 @@\n \r\n app=Flask(__name__)\r\n \r\n \r\n+load_dotenv()\r\n \r\n+\r\n+PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\r\n+GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\r\n+\r\n+\r\n+os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\r\n+os.environ[\"GROQ_API_KEY\"]=GROQ_API_KEY\r\n+\r\n+\r\n+embeddings=download_embeddings()\r\n+\r\n+index_name=\"medical_chatbot\"\r\n+#Embedding earch chunk and upsert the embeddings into your pinecone index.\r\n+\r\n+docsearch=PineconeVectorStore.from_existing_index(\r\n+    index_name=index_name,\r\n+    embedding=embeddings\r\n+)\r\n+\r\n+\r\n+\r\n+retriever=docsearch.as_retriever(\r\n+    search_type=\"similarity\",\r\n+    search_kwargs={\"k\":3})\r\n+\r\n+\r\n+chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+\r\n+prompt=ChatPromptTemplate.from_messages(\r\n+    [\r\n+        (\"system\",system_prompt),\r\n+        (\"human\",\"{input}\"),\r\n+    ]\r\n+)\r\n+\r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n"
                },
                {
                    "date": 1765629443763,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,45 +15,13 @@\n \r\n app=Flask(__name__)\r\n \r\n \r\n-load_dotenv()\r\n \r\n \r\n-PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\r\n-GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\r\n \r\n \r\n-os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\r\n-os.environ[\"GROQ_API_KEY\"]=GROQ_API_KEY\r\n \r\n-\r\n-embeddings=download_embeddings()\r\n-\r\n-index_name=\"medical_chatbot\"\r\n-#Embedding earch chunk and upsert the embeddings into your pinecone index.\r\n-\r\n-docsearch=PineconeVectorStore.from_existing_index(\r\n-    index_name=index_name,\r\n-    embedding=embeddings\r\n-)\r\n-\r\n-\r\n-\r\n-retriever=docsearch.as_retriever(\r\n-    search_type=\"similarity\",\r\n-    search_kwargs={\"k\":3})\r\n-\r\n-\r\n-chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n-\r\n-prompt=ChatPromptTemplate.from_messages(\r\n-    [\r\n-        (\"system\",system_prompt),\r\n-        (\"human\",\"{input}\"),\r\n-    ]\r\n-)\r\n-\r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n"
                },
                {
                    "date": 1765631802444,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,16 +17,62 @@\n \r\n \r\n \r\n \r\n+load_dotenv()\r\n \r\n \r\n+PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\r\n+GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\r\n \r\n+\r\n+os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\r\n+os.environ[\"GROQ_API_KEY\"]=GROQ_API_KEY\r\n+\r\n+\r\n+embeddings=download_embeddings()\r\n+\r\n+index_name=\"medical_chatbot\"\r\n+#Embedding earch chunk and upsert the embeddings into your pinecone index.\r\n+\r\n+docsearch=PineconeVectorStore.from_existing_index(\r\n+    index_name=index_name,\r\n+    embedding=embeddings\r\n+)\r\n+\r\n+\r\n+\r\n+retriever=docsearch.as_retriever(\r\n+    search_type=\"similarity\",\r\n+    search_kwargs={\"k\":3})\r\n+\r\n+\r\n+chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+\r\n+prompt=ChatPromptTemplate.from_messages(\r\n+    [\r\n+        (\"system\",system_prompt),\r\n+        (\"human\",\"{input}\"),\r\n+    ]\r\n+)\r\n+\r\n+\r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n \r\n+# def chat():\r\n+#     msg=request.form[\"msg\"]\r\n+#     input=msg\r\n+#     print(input)\r\n+#     response=rag_chain.invoke({\"input\":msg})\r\n+#     print(\"Response :\",response[\"answer\"])\r\n+#     return str(response[\"answer\"])\r\n \r\n \r\n+\r\n+\r\n+\r\n+\r\n if __name__ == '__main__':\r\n     app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765632112997,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,9 +54,12 @@\n         (\"human\",\"{input}\"),\r\n     ]\r\n )\r\n \r\n+question_answer_chain=format_docs(chatModel, prompt)\r\n \r\n+\r\n+\r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n"
                },
                {
                    "date": 1765632653629,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,8 +10,9 @@\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n from src.prompt import *\r\n+from langchain.chains.combine_documents import create_stuff_documents_chain\r\n \r\n \r\n app=Flask(__name__)\r\n \r\n@@ -54,12 +55,13 @@\n         (\"human\",\"{input}\"),\r\n     ]\r\n )\r\n \r\n-question_answer_chain=format_docs(chatModel, prompt)\r\n \r\n \r\n+question_answer_chain=\r\n \r\n+\r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n"
                },
                {
                    "date": 1765632667754,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n from src.prompt import *\r\n-from langchain.chains.combine_documents import create_stuff_documents_chain\r\n+from langchain.chains.combine_documents import create_stuff_documents_chain # type: ignore\r\n \r\n \r\n app=Flask(__name__)\r\n \r\n"
                },
                {
                    "date": 1765632746323,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,10 +11,10 @@\n from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n from src.prompt import *\r\n from langchain.chains.combine_documents import create_stuff_documents_chain # type: ignore\r\n+from langchain.chains import create_retrieval_chain\r\n \r\n-\r\n app=Flask(__name__)\r\n \r\n \r\n \r\n@@ -57,9 +57,10 @@\n )\r\n \r\n \r\n \r\n-question_answer_chain=\r\n+question_answer_chain=create_stuff_documents_chain(chatModel, prompt)\r\n+rag_chain=\r\n \r\n \r\n @app.route(\"/\")\r\n def index():\r\n"
                },
                {
                    "date": 1765632782530,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -58,9 +58,9 @@\n \r\n \r\n \r\n question_answer_chain=create_stuff_documents_chain(chatModel, prompt)\r\n-rag_chain=\r\n+rag_chain=create_retrieval_chain(retriever, question_answer_chain)\r\n \r\n \r\n @app.route(\"/\")\r\n def index():\r\n"
                },
                {
                    "date": 1765632794311,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,15 +66,15 @@\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n \r\n-# def chat():\r\n-#     msg=request.form[\"msg\"]\r\n-#     input=msg\r\n-#     print(input)\r\n-#     response=rag_chain.invoke({\"input\":msg})\r\n-#     print(\"Response :\",response[\"answer\"])\r\n-#     return str(response[\"answer\"])\r\n+def chat():\r\n+    msg=request.form[\"msg\"]\r\n+    input=msg\r\n+    print(input)\r\n+    response=rag_chain.invoke({\"input\":msg})\r\n+    print(\"Response :\",response[\"answer\"])\r\n+    return str(response[\"answer\"])\r\n \r\n \r\n \r\n \r\n"
                },
                {
                    "date": 1765632815380,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n from dotenv import load_dotenv\r\n import os\r\n from src.helper import download_embeddings\r\n from langchain_huggingface import HuggingFaceEmbeddings\r\n-from pinecone import Pinecone, ServerlessSpec\r\n+from pinecone import Pinecone\r\n from langchain_pinecone import PineconeVectorStore\r\n from operator import itemgetter\r\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_core.runnables import RunnableLambda\r\n"
                },
                {
                    "date": 1765632873142,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,14 +2,10 @@\n from flask import Flask, render_template, jsonify, request\r\n from dotenv import load_dotenv\r\n import os\r\n from src.helper import download_embeddings\r\n-from langchain_huggingface import HuggingFaceEmbeddings\r\n-from pinecone import Pinecone\r\n from langchain_pinecone import PineconeVectorStore\r\n-from operator import itemgetter\r\n from langchain_core.prompts import ChatPromptTemplate\r\n-from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n from src.prompt import *\r\n from langchain.chains.combine_documents import create_stuff_documents_chain # type: ignore\r\n from langchain.chains import create_retrieval_chain\r\n"
                },
                {
                    "date": 1765632923966,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -62,8 +62,9 @@\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n \r\n+@app.route(\"/get\", methods=[\"GET\", \"POST\"])\r\n def chat():\r\n     msg=request.form[\"msg\"]\r\n     input=msg\r\n     print(input)\r\n"
                },
                {
                    "date": 1765633030747,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_groq import ChatGroq\r\n from src.prompt import *\r\n from langchain.chains.combine_documents import create_stuff_documents_chain # type: ignore\r\n-from langchain.chains import create_retrieval_chain\r\n+from langchain.chains import create_retrieval_chain # type: ignore\r\n \r\n app=Flask(__name__)\r\n \r\n \r\n"
                },
                {
                    "date": 1765633201681,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n from langchain_pinecone import PineconeVectorStore\r\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_groq import ChatGroq\r\n from src.prompt import *\r\n-from langchain.chains.combine_documents import create_stuff_documents_chain # type: ignore\r\n+from langchain.chains.combine_documents.stuff import create_stuff_documents_chain# type: ignore\r\n from langchain.chains import create_retrieval_chain # type: ignore\r\n \r\n app=Flask(__name__)\r\n \r\n"
                },
                {
                    "date": 1765633251125,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_groq import ChatGroq\r\n from src.prompt import *\r\n from langchain.chains.combine_documents.stuff import create_stuff_documents_chain# type: ignore\r\n-from langchain.chains import create_retrieval_chain # type: ignore\r\n+from langchain.chains.retrieval import create_retrieval_chain \r\n \r\n app=Flask(__name__)\r\n \r\n \r\n"
                },
                {
                    "date": 1765633261021,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n from langchain_pinecone import PineconeVectorStore\r\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_groq import ChatGroq\r\n from src.prompt import *\r\n-from langchain.chains.combine_documents.stuff import create_stuff_documents_chain# type: ignore\r\n+from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\r\n from langchain.chains.retrieval import create_retrieval_chain \r\n \r\n app=Flask(__name__)\r\n \r\n"
                },
                {
                    "date": 1765633892845,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,11 +53,19 @@\n )\r\n \r\n \r\n \r\n-question_answer_chain=create_stuff_documents_chain(chatModel, prompt)\r\n-rag_chain=create_retrieval_chain(retriever, question_answer_chain)\r\n+question_answer_chain = (\r\n+    {\"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\r\n+     \"question\": lambda x: x[\"question\"]}\r\n+    | prompt\r\n+    | chatModel\r\n+)\r\n \r\n+rag_chain = (\r\n+    {\"context\": retriever, \"question\": lambda x: x[\"question\"]}\r\n+    | question_answer_chain\r\n+)\r\n \r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n"
                },
                {
                    "date": 1765633926782,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,10 +6,8 @@\n from langchain_pinecone import PineconeVectorStore\r\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_groq import ChatGroq\r\n from src.prompt import *\r\n-from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\r\n-from langchain.chains.retrieval import create_retrieval_chain \r\n \r\n app=Flask(__name__)\r\n \r\n \r\n"
                },
                {
                    "date": 1765634250442,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n \r\n \r\n embeddings=download_embeddings()\r\n \r\n-index_name=\"medical_chatbot\"\r\n+index_name=\"medical-chatbot\"\r\n #Embedding earch chunk and upsert the embeddings into your pinecone index.\r\n \r\n docsearch=PineconeVectorStore.from_existing_index(\r\n     index_name=index_name,\r\n"
                },
                {
                    "date": 1765634937196,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,15 +51,17 @@\n )\r\n \r\n \r\n \r\n+# Stuff chain: format docs + prompt + LLM\r\n question_answer_chain = (\r\n     {\"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\r\n      \"question\": lambda x: x[\"question\"]}\r\n     | prompt\r\n     | chatModel\r\n )\r\n \r\n+# Retrieval chain: retriever + question → pass to stuff chain\r\n rag_chain = (\r\n     {\"context\": retriever, \"question\": lambda x: x[\"question\"]}\r\n     | question_answer_chain\r\n )\r\n"
                },
                {
                    "date": 1765635469977,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,13 +60,21 @@\n     | chatModel\r\n )\r\n \r\n # Retrieval chain: retriever + question → pass to stuff chain\r\n+\r\n+\r\n rag_chain = (\r\n-    {\"context\": retriever, \"question\": lambda x: x[\"question\"]}\r\n+    {\r\n+        # retriever.invoke(question) हमेशा Document objects return करेगा\r\n+        \"context\": lambda x: retriever.invoke(x[\"question\"]),\r\n+        \"question\": lambda x: x[\"question\"]\r\n+    }\r\n     | question_answer_chain\r\n )\r\n \r\n+\r\n+\r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n"
                },
                {
                    "date": 1765635649260,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,10 +88,6 @@\n     print(\"Response :\",response[\"answer\"])\r\n     return str(response[\"answer\"])\r\n \r\n \r\n-\r\n-\r\n-\r\n-\r\n if __name__ == '__main__':\r\n     app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765635869328,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,28 +53,26 @@\n \r\n \r\n # Stuff chain: format docs + prompt + LLM\r\n question_answer_chain = (\r\n-    {\"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\r\n-     \"question\": lambda x: x[\"question\"]}\r\n+    {\r\n+        \"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\r\n+        \"question\": lambda x: x[\"question\"]\r\n+    }\r\n     | prompt\r\n     | chatModel\r\n )\r\n \r\n # Retrieval chain: retriever + question → pass to stuff chain\r\n-\r\n-\r\n rag_chain = (\r\n     {\r\n-        # retriever.invoke(question) हमेशा Document objects return करेगा\r\n-        \"context\": lambda x: retriever.invoke(x[\"question\"]),\r\n+        \"context\": retriever,   # ✅ retriever को सीधे डालो\r\n         \"question\": lambda x: x[\"question\"]\r\n     }\r\n     | question_answer_chain\r\n )\r\n \r\n \r\n-\r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n"
                },
                {
                    "date": 1765639577734,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -76,16 +76,16 @@\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n \r\n-@app.route(\"/get\", methods=[\"GET\", \"POST\"])\r\n-def chat():\r\n-    msg=request.form[\"msg\"]\r\n-    input=msg\r\n-    print(input)\r\n-    response=rag_chain.invoke({\"input\":msg})\r\n-    print(\"Response :\",response[\"answer\"])\r\n-    return str(response[\"answer\"])\r\n+# @app.route(\"/get\", methods=[\"GET\", \"POST\"])\r\n+# def chat():\r\n+#     msg=request.form[\"msg\"]\r\n+#     input=msg\r\n+#     print(input)\r\n+#     response=rag_chain.invoke({\"input\":msg})\r\n+#     print(\"Response :\",response[\"answer\"])\r\n+#     return str(response[\"answer\"])\r\n \r\n \r\n if __name__ == '__main__':\r\n     app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765639677392,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,7 +85,25 @@\n #     response=rag_chain.invoke({\"input\":msg})\r\n #     print(\"Response :\",response[\"answer\"])\r\n #     return str(response[\"answer\"])\r\n \r\n+@app.route(\"/get\", methods=[\"POST\"])\r\n+def chat():\r\n+    msg = request.form[\"msg\"]\r\n+    print(\"User:\", msg)\r\n \r\n+    # invoke rag_chain\r\n+    response = rag_chain.invoke({\"question\": msg})\r\n+\r\n+    # FIX: new runnable chain returns AIMessage or string\r\n+    if hasattr(response, \"content\"):\r\n+        answer = response.content\r\n+    elif isinstance(response, dict) and \"answer\" in response:\r\n+        answer = response[\"answer\"]\r\n+    else:\r\n+        answer = str(response)\r\n+\r\n+    print(\"Response:\", answer)\r\n+    return answer\r\n+\r\n if __name__ == '__main__':\r\n     app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765640126089,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,12 +90,12 @@\n def chat():\r\n     msg = request.form[\"msg\"]\r\n     print(\"User:\", msg)\r\n \r\n-    # invoke rag_chain\r\n+    # run rag_chain\r\n     response = rag_chain.invoke({\"question\": msg})\r\n \r\n-    # FIX: new runnable chain returns AIMessage or string\r\n+    # FIX: extract text\r\n     if hasattr(response, \"content\"):\r\n         answer = response.content\r\n     elif isinstance(response, dict) and \"answer\" in response:\r\n         answer = response[\"answer\"]\r\n"
                },
                {
                    "date": 1765640181541,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,8 +85,9 @@\n #     response=rag_chain.invoke({\"input\":msg})\r\n #     print(\"Response :\",response[\"answer\"])\r\n #     return str(response[\"answer\"])\r\n \r\n+\r\n @app.route(\"/get\", methods=[\"POST\"])\r\n def chat():\r\n     msg = request.form[\"msg\"]\r\n     print(\"User:\", msg)\r\n@@ -103,7 +104,6 @@\n         answer = str(response)\r\n \r\n     print(\"Response:\", answer)\r\n     return answer\r\n-\r\n if __name__ == '__main__':\r\n     app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765640396355,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,7 +103,10 @@\n     else:\r\n         answer = str(response)\r\n \r\n     print(\"Response:\", answer)\r\n-    return answer\r\n+    return str(answer)\r\n+\r\n+\r\n+\r\n if __name__ == '__main__':\r\n     app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765640734982,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,28 +85,30 @@\n #     response=rag_chain.invoke({\"input\":msg})\r\n #     print(\"Response :\",response[\"answer\"])\r\n #     return str(response[\"answer\"])\r\n \r\n-\r\n @app.route(\"/get\", methods=[\"POST\"])\r\n def chat():\r\n     msg = request.form[\"msg\"]\r\n     print(\"User:\", msg)\r\n \r\n-    # run rag_chain\r\n-    response = rag_chain.invoke({\"question\": msg})\r\n+    try:\r\n+        response = rag_chain.invoke({\"question\": msg})\r\n \r\n-    # FIX: extract text\r\n-    if hasattr(response, \"content\"):\r\n-        answer = response.content\r\n-    elif isinstance(response, dict) and \"answer\" in response:\r\n-        answer = response[\"answer\"]\r\n-    else:\r\n-        answer = str(response)\r\n+        # Always convert to string\r\n+        if hasattr(response, \"content\"):\r\n+            answer = response.content\r\n+        elif isinstance(response, dict) and \"answer\" in response:\r\n+            answer = response[\"answer\"]\r\n+        else:\r\n+            answer = str(response)\r\n \r\n-    print(\"Response:\", answer)\r\n-    return str(answer)\r\n+        print(\"Response:\", answer)\r\n+        return str(answer)   # ✅ ensure plain string\r\n+    except Exception as e:\r\n+        print(\"Error in rag_chain:\", e)\r\n+        return \"Error: Could not get response\"\r\n+   \r\n \r\n \r\n-\r\n if __name__ == '__main__':\r\n     app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765641127427,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,17 +42,16 @@\n \r\n \r\n chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-prompt=ChatPromptTemplate.from_messages(\r\n-    [\r\n-        (\"system\",system_prompt),\r\n-        (\"human\",\"{input}\"),\r\n-    ]\r\n-)\r\n+prompt = ChatPromptTemplate.from_messages([\r\n+    (\"system\", system_prompt),\r\n+    (\"human\", \"{question}\"),   # ✅ FIXED\r\n+])\r\n \r\n \r\n \r\n+\r\n # Stuff chain: format docs + prompt + LLM\r\n question_answer_chain = (\r\n     {\r\n         \"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\r\n"
                },
                {
                    "date": 1765641194668,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -47,12 +47,9 @@\n     (\"system\", system_prompt),\r\n     (\"human\", \"{question}\"),   # ✅ FIXED\r\n ])\r\n \r\n-\r\n-\r\n-\r\n-# Stuff chain: format docs + prompt + LLM\r\n+# Stuff chain\r\n question_answer_chain = (\r\n     {\r\n         \"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\r\n         \"question\": lambda x: x[\"question\"]\r\n@@ -60,54 +57,37 @@\n     | prompt\r\n     | chatModel\r\n )\r\n \r\n-# Retrieval chain: retriever + question → pass to stuff chain\r\n+# Retrieval chain\r\n rag_chain = (\r\n     {\r\n-        \"context\": retriever,   # ✅ retriever को सीधे डालो\r\n+        \"context\": retriever,\r\n         \"question\": lambda x: x[\"question\"]\r\n     }\r\n     | question_answer_chain\r\n )\r\n \r\n-\r\n-@app.route(\"/\")\r\n-def index():\r\n-    return render_template(\"chat.html\")\r\n-\r\n-\r\n-# @app.route(\"/get\", methods=[\"GET\", \"POST\"])\r\n-# def chat():\r\n-#     msg=request.form[\"msg\"]\r\n-#     input=msg\r\n-#     print(input)\r\n-#     response=rag_chain.invoke({\"input\":msg})\r\n-#     print(\"Response :\",response[\"answer\"])\r\n-#     return str(response[\"answer\"])\r\n-\r\n @app.route(\"/get\", methods=[\"POST\"])\r\n def chat():\r\n     msg = request.form[\"msg\"]\r\n     print(\"User:\", msg)\r\n \r\n     try:\r\n         response = rag_chain.invoke({\"question\": msg})\r\n \r\n-        # Always convert to string\r\n         if hasattr(response, \"content\"):\r\n             answer = response.content\r\n         elif isinstance(response, dict) and \"answer\" in response:\r\n             answer = response[\"answer\"]\r\n         else:\r\n             answer = str(response)\r\n \r\n         print(\"Response:\", answer)\r\n-        return str(answer)   # ✅ ensure plain string\r\n+        return str(answer)\r\n     except Exception as e:\r\n         print(\"Error in rag_chain:\", e)\r\n         return \"Error: Could not get response\"\r\n-   \r\n \r\n \r\n if __name__ == '__main__':\r\n     app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765641320000,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,28 +66,40 @@\n     }\r\n     | question_answer_chain\r\n )\r\n \r\n+# @app.route(\"/get\", methods=[\"POST\"])\r\n+# def chat():\r\n+#     msg = request.form[\"msg\"]\r\n+#     print(\"User:\", msg)\r\n+\r\n+#     try:\r\n+#         response = rag_chain.invoke({\"question\": msg})\r\n+\r\n+#         if hasattr(response, \"content\"):\r\n+#             answer = response.content\r\n+#         elif isinstance(response, dict) and \"answer\" in response:\r\n+#             answer = response[\"answer\"]\r\n+#         else:\r\n+#             answer = str(response)\r\n+\r\n+#         print(\"Response:\", answer)\r\n+#         return str(answer)\r\n+#     except Exception as e:\r\n+#         print(\"Error in rag_chain:\", e)\r\n+#         return \"Error: Could not get response\"\r\n+\r\n @app.route(\"/get\", methods=[\"POST\"])\r\n def chat():\r\n     msg = request.form[\"msg\"]\r\n     print(\"User:\", msg)\r\n \r\n     try:\r\n-        response = rag_chain.invoke({\"question\": msg})\r\n-\r\n-        if hasattr(response, \"content\"):\r\n-            answer = response.content\r\n-        elif isinstance(response, dict) and \"answer\" in response:\r\n-            answer = response[\"answer\"]\r\n-        else:\r\n-            answer = str(response)\r\n-\r\n+        response = chatModel.invoke(msg)   # ✅ retriever bypass\r\n+        answer = response.content if hasattr(response, \"content\") else str(response)\r\n         print(\"Response:\", answer)\r\n         return str(answer)\r\n     except Exception as e:\r\n-        print(\"Error in rag_chain:\", e)\r\n+        print(\"Error in chatModel:\", e)\r\n         return \"Error: Could not get response\"\r\n-\r\n-\r\n if __name__ == '__main__':\r\n     app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765641659629,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,52 +1,38 @@\n-\r\n-from flask import Flask, render_template, jsonify, request\r\n+from flask import Flask, render_template, request\r\n from dotenv import load_dotenv\r\n import os\r\n from src.helper import download_embeddings\r\n from langchain_pinecone import PineconeVectorStore\r\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_groq import ChatGroq\r\n-from src.prompt import *\r\n+from src.prompt import system_prompt\r\n \r\n-app=Flask(__name__)\r\n-\r\n-\r\n-\r\n-\r\n+app = Flask(__name__)\r\n load_dotenv()\r\n \r\n+# API keys\r\n+os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n+os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n \r\n-PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\r\n-GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\r\n+# Embeddings\r\n+embeddings = download_embeddings()\r\n \r\n-\r\n-os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\r\n-os.environ[\"GROQ_API_KEY\"]=GROQ_API_KEY\r\n-\r\n-\r\n-embeddings=download_embeddings()\r\n-\r\n-index_name=\"medical-chatbot\"\r\n-#Embedding earch chunk and upsert the embeddings into your pinecone index.\r\n-\r\n-docsearch=PineconeVectorStore.from_existing_index(\r\n+# Pinecone index\r\n+index_name = \"medical-chatbot\"\r\n+docsearch = PineconeVectorStore.from_existing_index(\r\n     index_name=index_name,\r\n     embedding=embeddings\r\n )\r\n+retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-\r\n-\r\n-retriever=docsearch.as_retriever(\r\n-    search_type=\"similarity\",\r\n-    search_kwargs={\"k\":3})\r\n-\r\n-\r\n+# LLM\r\n chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n+# Prompt\r\n prompt = ChatPromptTemplate.from_messages([\r\n     (\"system\", system_prompt),\r\n-    (\"human\", \"{question}\"),   # ✅ FIXED\r\n+    (\"human\", \"{question}\"),\r\n ])\r\n \r\n # Stuff chain\r\n question_answer_chain = (\r\n@@ -66,40 +52,32 @@\n     }\r\n     | question_answer_chain\r\n )\r\n \r\n-# @app.route(\"/get\", methods=[\"POST\"])\r\n-# def chat():\r\n-#     msg = request.form[\"msg\"]\r\n-#     print(\"User:\", msg)\r\n+@app.route(\"/\")\r\n+def index():\r\n+    return render_template(\"chat.html\")\r\n \r\n-#     try:\r\n-#         response = rag_chain.invoke({\"question\": msg})\r\n-\r\n-#         if hasattr(response, \"content\"):\r\n-#             answer = response.content\r\n-#         elif isinstance(response, dict) and \"answer\" in response:\r\n-#             answer = response[\"answer\"]\r\n-#         else:\r\n-#             answer = str(response)\r\n-\r\n-#         print(\"Response:\", answer)\r\n-#         return str(answer)\r\n-#     except Exception as e:\r\n-#         print(\"Error in rag_chain:\", e)\r\n-#         return \"Error: Could not get response\"\r\n-\r\n @app.route(\"/get\", methods=[\"POST\"])\r\n def chat():\r\n     msg = request.form[\"msg\"]\r\n     print(\"User:\", msg)\r\n \r\n     try:\r\n-        response = chatModel.invoke(msg)   # ✅ retriever bypass\r\n-        answer = response.content if hasattr(response, \"content\") else str(response)\r\n-        print(\"Response:\", answer)\r\n+        response = rag_chain.invoke({\"question\": msg})\r\n+        print(\"Raw response:\", response)\r\n+\r\n\\ No newline at end of file\n+        if hasattr(response, \"content\"):\r\n+            answer = response.content\r\n+        elif isinstance(response, dict) and \"answer\" in response:\r\n+            answer = response[\"answer\"]\r\n+        else:\r\n+            answer = str(response)\r\n+\r\n+        print(\"Final Answer:\", answer)\r\n         return str(answer)\r\n     except Exception as e:\r\n-        print(\"Error in chatModel:\", e)\r\n+        print(\"Error in rag_chain:\", e)\r\n         return \"Error: Could not get response\"\r\n+\r\n if __name__ == '__main__':\r\n-    app.run(host=\"0.0.0.0\", port= 8080, debug= True)\n+    app.run(host=\"0.0.0.0\", port=8080, debug=True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765641941328,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,41 +1,43 @@\n from flask import Flask, render_template, request\r\n+from src.helper import download_hugging_face_embeddings\r\n+from langchain_pinecone import PineconeVectorStore\r\n+from langchain_groq import ChatGroq\r\n+from langchain_core.prompts import ChatPromptTemplate\r\n from dotenv import load_dotenv\r\n+from src.prompt import *\r\n import os\r\n-from src.helper import download_embeddings\r\n-from langchain_pinecone import PineconeVectorStore\r\n-from langchain_core.prompts import ChatPromptTemplate\r\n-from langchain_groq import ChatGroq\r\n-from src.prompt import system_prompt\r\n \r\n app = Flask(__name__)\r\n+\r\n load_dotenv()\r\n \r\n-# API keys\r\n-os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n-os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n+PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\r\n+GROQ_API_KEY = os.environ.get('GROQ_API_KEY')\r\n \r\n-# Embeddings\r\n-embeddings = download_embeddings()\r\n+os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n+os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n \r\n-# Pinecone index\r\n+embeddings = download_hugging_face_embeddings()\r\n+\r\n index_name = \"medical-chatbot\"\r\n docsearch = PineconeVectorStore.from_existing_index(\r\n     index_name=index_name,\r\n     embedding=embeddings\r\n )\r\n+\r\n retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-# LLM\r\n+# ✅ Groq model\r\n chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-# Prompt\r\n+# ✅ Prompt variable को {question} कर दो\r\n prompt = ChatPromptTemplate.from_messages([\r\n     (\"system\", system_prompt),\r\n     (\"human\", \"{question}\"),\r\n ])\r\n \r\n-# Stuff chain\r\n+# Runnable style chain\r\n question_answer_chain = (\r\n     {\r\n         \"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\r\n         \"question\": lambda x: x[\"question\"]\r\n@@ -43,9 +45,8 @@\n     | prompt\r\n     | chatModel\r\n )\r\n \r\n-# Retrieval chain\r\n rag_chain = (\r\n     {\r\n         \"context\": retriever,\r\n         \"question\": lambda x: x[\"question\"]\r\n@@ -54,9 +55,9 @@\n )\r\n \r\n @app.route(\"/\")\r\n def index():\r\n-    return render_template(\"chat.html\")\r\n+    return render_template('chat.html')\r\n \r\n @app.route(\"/get\", methods=[\"POST\"])\r\n def chat():\r\n     msg = request.form[\"msg\"]\r\n"
                },
                {
                    "date": 1765642142624,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,43 +1,41 @@\n from flask import Flask, render_template, request\r\n-from src.helper import download_hugging_face_embeddings\r\n+from dotenv import load_dotenv\r\n+import os\r\n+from src.helper import download_embeddings\r\n from langchain_pinecone import PineconeVectorStore\r\n+from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_groq import ChatGroq\r\n-from langchain_core.prompts import ChatPromptTemplate\r\n-from dotenv import load_dotenv\r\n-from src.prompt import *\r\n-import os\r\n+from src.prompt import system_prompt\r\n \r\n app = Flask(__name__)\r\n-\r\n load_dotenv()\r\n \r\n-PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\r\n-GROQ_API_KEY = os.environ.get('GROQ_API_KEY')\r\n+# API keys\r\n+os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n+os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n \r\n-os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n-os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n+# Embeddings\r\n+embeddings = download_embeddings()\r\n \r\n-embeddings = download_hugging_face_embeddings()\r\n-\r\n+# Pinecone index\r\n index_name = \"medical-chatbot\"\r\n docsearch = PineconeVectorStore.from_existing_index(\r\n     index_name=index_name,\r\n     embedding=embeddings\r\n )\r\n-\r\n retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-# ✅ Groq model\r\n+# LLM\r\n chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-# ✅ Prompt variable को {question} कर दो\r\n+# Prompt\r\n prompt = ChatPromptTemplate.from_messages([\r\n     (\"system\", system_prompt),\r\n     (\"human\", \"{question}\"),\r\n ])\r\n \r\n-# Runnable style chain\r\n+# Stuff chain\r\n question_answer_chain = (\r\n     {\r\n         \"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\r\n         \"question\": lambda x: x[\"question\"]\r\n@@ -45,8 +43,9 @@\n     | prompt\r\n     | chatModel\r\n )\r\n \r\n+# Retrieval chain\r\n rag_chain = (\r\n     {\r\n         \"context\": retriever,\r\n         \"question\": lambda x: x[\"question\"]\r\n@@ -55,9 +54,9 @@\n )\r\n \r\n @app.route(\"/\")\r\n def index():\r\n-    return render_template('chat.html')\r\n+    return render_template(\"chat.html\")\r\n \r\n @app.route(\"/get\", methods=[\"POST\"])\r\n def chat():\r\n     msg = request.form[\"msg\"]\r\n"
                },
                {
                    "date": 1765642386144,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,59 +1,57 @@\n from flask import Flask, render_template, request\r\n from dotenv import load_dotenv\r\n import os\r\n-from src.helper import download_embeddings\r\n+from operator import itemgetter\r\n+\r\n+from src.helper import download_hugging_face_embeddings\r\n+from src.prompt import system_prompt\r\n from langchain_pinecone import PineconeVectorStore\r\n from langchain_core.prompts import ChatPromptTemplate\r\n+from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n-from src.prompt import system_prompt\r\n \r\n app = Flask(__name__)\r\n load_dotenv()\r\n \r\n # API keys\r\n-os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n-os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n+PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n+GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n+os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n+os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n \r\n-# Embeddings\r\n-embeddings = download_embeddings()\r\n-\r\n-# Pinecone index\r\n+# Embeddings + Pinecone retriever\r\n+embeddings = download_hugging_face_embeddings()\r\n index_name = \"medical-chatbot\"\r\n docsearch = PineconeVectorStore.from_existing_index(\r\n     index_name=index_name,\r\n     embedding=embeddings\r\n )\r\n retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-# LLM\r\n+# Groq LLM\r\n chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-# Prompt\r\n+# Prompt template\r\n prompt = ChatPromptTemplate.from_messages([\r\n     (\"system\", system_prompt),\r\n-    (\"human\", \"{question}\"),\r\n+    (\"human\", \"{input}\"),\r\n ])\r\n \r\n-# Stuff chain\r\n-question_answer_chain = (\r\n-    {\r\n-        \"context\": lambda x: \"\\n\\n\".join(doc.page_content for doc in x[\"context\"]),\r\n-        \"question\": lambda x: x[\"question\"]\r\n-    }\r\n+# Format docs function\r\n+def format_docs(docs):\r\n+    return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+\r\n+# Context branch\r\n+context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+\r\n+# RAG chain\r\n+rag_chain = (\r\n+    {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n     | prompt\r\n     | chatModel\r\n )\r\n \r\n-# Retrieval chain\r\n-rag_chain = (\r\n-    {\r\n-        \"context\": retriever,\r\n-        \"question\": lambda x: x[\"question\"]\r\n-    }\r\n-    | question_answer_chain\r\n-)\r\n-\r\n @app.route(\"/\")\r\n def index():\r\n     return render_template(\"chat.html\")\r\n \r\n@@ -62,15 +60,14 @@\n     msg = request.form[\"msg\"]\r\n     print(\"User:\", msg)\r\n \r\n     try:\r\n-        response = rag_chain.invoke({\"question\": msg})\r\n+        response = rag_chain.invoke({\"input\": msg})\r\n         print(\"Raw response:\", response)\r\n \r\n+        # AIMessage object → .content\r\n         if hasattr(response, \"content\"):\r\n             answer = response.content\r\n-        elif isinstance(response, dict) and \"answer\" in response:\r\n-            answer = response[\"answer\"]\r\n         else:\r\n             answer = str(response)\r\n \r\n         print(\"Final Answer:\", answer)\r\n@@ -78,6 +75,6 @@\n     except Exception as e:\r\n         print(\"Error in rag_chain:\", e)\r\n         return \"Error: Could not get response\"\r\n \r\n-if __name__ == '__main__':\r\n+if __name__ == \"__main__\":\r\n     app.run(host=\"0.0.0.0\", port=8080, debug=True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765642576099,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n from dotenv import load_dotenv\r\n import os\r\n from operator import itemgetter\r\n \r\n-from src.helper import download_hugging_face_embeddings\r\n+from src.helper import download_embeddings\r\n from src.prompt import system_prompt\r\n from langchain_pinecone import PineconeVectorStore\r\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_core.runnables import RunnableLambda\r\n@@ -19,9 +19,9 @@\n os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n \r\n # Embeddings + Pinecone retriever\r\n-embeddings = download_hugging_face_embeddings()\r\n+embeddings = download_embeddings()\r\n index_name = \"medical-chatbot\"\r\n docsearch = PineconeVectorStore.from_existing_index(\r\n     index_name=index_name,\r\n     embedding=embeddings\r\n"
                },
                {
                    "date": 1765643794024,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,21 +57,21 @@\n \r\n @app.route(\"/get\", methods=[\"POST\"])\r\n def chat():\r\n     msg = request.form[\"msg\"]\r\n-    print(\"User:\", msg)\r\n+    #print(\"User:\", msg)\r\n \r\n     try:\r\n         response = rag_chain.invoke({\"input\": msg})\r\n-        print(\"Raw response:\", response)\r\n+        #print(\"Raw response:\", response)\r\n \r\n         # AIMessage object → .content\r\n         if hasattr(response, \"content\"):\r\n             answer = response.content\r\n         else:\r\n             answer = str(response)\r\n \r\n-        print(\"Final Answer:\", answer)\r\n+        #print(\"Final Answer:\", answer)\r\n         return str(answer)\r\n     except Exception as e:\r\n         print(\"Error in rag_chain:\", e)\r\n         return \"Error: Could not get response\"\r\n"
                },
                {
                    "date": 1765644069423,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,88 @@\n-from flask import Flask, render_template, request\r\n+# from flask import Flask, render_template, request\r\n+# from dotenv import load_dotenv\r\n+# import os\r\n+# from operator import itemgetter\r\n+\r\n+# from src.helper import download_embeddings\r\n+# from src.prompt import system_prompt\r\n+# from langchain_pinecone import PineconeVectorStore\r\n+# from langchain_core.prompts import ChatPromptTemplate\r\n+# from langchain_core.runnables import RunnableLambda\r\n+# from langchain_groq import ChatGroq\r\n+\r\n+# app = Flask(__name__)\r\n+# load_dotenv()\r\n+\r\n+# # API keys\r\n+# PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n+# GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n+# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n+# os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n+\r\n+# # Embeddings + Pinecone retriever\r\n+# embeddings = download_embeddings()\r\n+# index_name = \"medical-chatbot\"\r\n+# docsearch = PineconeVectorStore.from_existing_index(\r\n+#     index_name=index_name,\r\n+#     embedding=embeddings\r\n+# )\r\n+# retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+\r\n+# # Groq LLM\r\n+# chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+\r\n+# # Prompt template\r\n+# prompt = ChatPromptTemplate.from_messages([\r\n+#     (\"system\", system_prompt),\r\n+#     (\"human\", \"{input}\"),\r\n+# ])\r\n+\r\n+# # Format docs function\r\n+# def format_docs(docs):\r\n+#     return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+\r\n+# # Context branch\r\n+# context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+\r\n+# # RAG chain\r\n+# rag_chain = (\r\n+#     {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+#     | prompt\r\n+#     | chatModel\r\n+# )\r\n+\r\n+# @app.route(\"/\")\r\n+# def index():\r\n+#     return render_template(\"chat.html\")\r\n+\r\n+# @app.route(\"/get\", methods=[\"POST\"])\r\n+# def chat():\r\n+#     msg = request.form[\"msg\"]\r\n+#     #print(\"User:\", msg)\r\n+\r\n+#     try:\r\n+#         response = rag_chain.invoke({\"input\": msg})\r\n+#         #print(\"Raw response:\", response)\r\n+\r\n+#         # AIMessage object → .content\r\n+#         if hasattr(response, \"content\"):\r\n+#             answer = response.content\r\n+#         else:\r\n+#             answer = str(response)\r\n+\r\n+#         #print(\"Final Answer:\", answer)\r\n+#         return str(answer)\r\n+#     except Exception as e:\r\n+#         print(\"Error in rag_chain:\", e)\r\n+#         return \"Error: Could not get response\"\r\n+\r\n+# if __name__ == \"__main__\":\r\n+#     app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n+\r\n+\r\n+\r\n+import streamlit as st\r\n from dotenv import load_dotenv\r\n import os\r\n from operator import itemgetter\r\n \r\n@@ -9,17 +92,13 @@\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n \r\n-app = Flask(__name__)\r\n+# Load environment variables\r\n load_dotenv()\r\n+os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n+os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n \r\n-# API keys\r\n-PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n-GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n-os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n-os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n-\r\n # Embeddings + Pinecone retriever\r\n embeddings = download_embeddings()\r\n index_name = \"medical-chatbot\"\r\n docsearch = PineconeVectorStore.from_existing_index(\r\n@@ -50,31 +129,39 @@\n     | prompt\r\n     | chatModel\r\n )\r\n \r\n-@app.route(\"/\")\r\n-def index():\r\n-    return render_template(\"chat.html\")\r\n+# ---------------- Streamlit UI ----------------\r\n+st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n+st.title(\"🩺 Medical Chatbot (Groq + Pinecone)\")\r\n \r\n-@app.route(\"/get\", methods=[\"POST\"])\r\n-def chat():\r\n-    msg = request.form[\"msg\"]\r\n-    #print(\"User:\", msg)\r\n+# Chat history\r\n+if \"messages\" not in st.session_state:\r\n+    st.session_state.messages = []\r\n \r\n-    try:\r\n-        response = rag_chain.invoke({\"input\": msg})\r\n-        #print(\"Raw response:\", response)\r\n+# Display chat history\r\n+for msg in st.session_state.messages:\r\n+    if msg[\"role\"] == \"user\":\r\n+        st.markdown(f\"**You:** {msg['content']}\")\r\n+    else:\r\n+        st.markdown(f\"**Bot:** {msg['content']}\")\r\n \r\n-        # AIMessage object → .content\r\n+# User input\r\n\\ No newline at end of file\n+user_input = st.text_input(\"Type your message:\", key=\"input\")\r\n+\r\n+if st.button(\"Send\") and user_input:\r\n+    # Add user message\r\n+    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\r\n+\r\n+    # Get bot response\r\n+    with st.spinner(\"Thinking...\"):\r\n+        response = rag_chain.invoke({\"input\": user_input})\r\n         if hasattr(response, \"content\"):\r\n             answer = response.content\r\n         else:\r\n             answer = str(response)\r\n \r\n-        #print(\"Final Answer:\", answer)\r\n-        return str(answer)\r\n-    except Exception as e:\r\n-        print(\"Error in rag_chain:\", e)\r\n-        return \"Error: Could not get response\"\r\n+    # Add bot message\r\n+    st.session_state.messages.append({\"role\": \"bot\", \"content\": answer})\r\n \r\n-if __name__ == \"__main__\":\r\n-    app.run(host=\"0.0.0.0\", port=8080, debug=True)\n+    # Rerun to show updated chat\r\n+    st.experimental_rerun()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765644642796,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -92,45 +92,59 @@\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n \r\n-# Load environment variables\r\n+# ---------------- Setup ----------------\r\n load_dotenv()\r\n os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n \r\n-# Embeddings + Pinecone retriever\r\n-embeddings = download_embeddings()\r\n-index_name = \"medical-chatbot\"\r\n-docsearch = PineconeVectorStore.from_existing_index(\r\n-    index_name=index_name,\r\n-    embedding=embeddings\r\n-)\r\n-retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+# ✅ Meta tensor fix: cache pipeline so it doesn't reload every rerun\r\n+@st.cache_resource\r\n+def load_pipeline():\r\n+    # Embeddings + Pinecone retriever\r\n+    embeddings = download_embeddings()\r\n+    index_name = \"medical-chatbot\"\r\n+    docsearch = PineconeVectorStore.from_existing_index(\r\n+        index_name=index_name,\r\n+        embedding=embeddings\r\n+    )\r\n+    retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-# Groq LLM\r\n-chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+    # Groq LLM\r\n+    chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-# Prompt template\r\n-prompt = ChatPromptTemplate.from_messages([\r\n-    (\"system\", system_prompt),\r\n-    (\"human\", \"{input}\"),\r\n-])\r\n+    # ✅ Meta tensor safeguard: if model accidentally loads on meta device\r\n+    try:\r\n+        if hasattr(chatModel, \"to\") and hasattr(chatModel, \"device\") and str(chatModel.device) == \"meta\":\r\n+            # move empty structure to CPU\r\n+            chatModel = chatModel.to_empty(device=\"cpu\")\r\n+    except Exception as e:\r\n+        st.warning(f\"Meta tensor fix applied: {e}\")\r\n \r\n-# Format docs function\r\n-def format_docs(docs):\r\n-    return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+    # Prompt template\r\n+    prompt = ChatPromptTemplate.from_messages([\r\n+        (\"system\", system_prompt),\r\n+        (\"human\", \"{input}\"),\r\n+    ])\r\n \r\n-# Context branch\r\n-context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+    # Format docs function\r\n+    def format_docs(docs):\r\n+        return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n \r\n-# RAG chain\r\n-rag_chain = (\r\n-    {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-    | prompt\r\n-    | chatModel\r\n-)\r\n+    # Context branch\r\n+    context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n \r\n+    # RAG chain\r\n+    rag_chain = (\r\n+        {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+        | prompt\r\n+        | chatModel\r\n+    )\r\n+    return rag_chain\r\n+\r\n+rag_chain = load_pipeline()\r\n+\r\n # ---------------- Streamlit UI ----------------\r\n st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n st.title(\"🩺 Medical Chatbot (Groq + Pinecone)\")\r\n \r\n@@ -140,11 +154,11 @@\n \r\n # Display chat history\r\n for msg in st.session_state.messages:\r\n     if msg[\"role\"] == \"user\":\r\n-        st.markdown(f\"**You:** {msg['content']}\")\r\n+        st.markdown(f\"<div style='text-align:right; background:#007bff; color:white; padding:8px; border-radius:8px; margin:4px;'>You: {msg['content']}</div>\", unsafe_allow_html=True)\r\n     else:\r\n-        st.markdown(f\"**Bot:** {msg['content']}\")\r\n+        st.markdown(f\"<div style='text-align:left; background:#f1f1f1; padding:8px; border-radius:8px; margin:4px;'>Bot: {msg['content']}</div>\", unsafe_allow_html=True)\r\n \r\n # User input\r\n user_input = st.text_input(\"Type your message:\", key=\"input\")\r\n \r\n@@ -154,12 +168,9 @@\n \r\n     # Get bot response\r\n     with st.spinner(\"Thinking...\"):\r\n         response = rag_chain.invoke({\"input\": user_input})\r\n-        if hasattr(response, \"content\"):\r\n-            answer = response.content\r\n-        else:\r\n-            answer = str(response)\r\n+        answer = response.content if hasattr(response, \"content\") else str(response)\r\n \r\n     # Add bot message\r\n     st.session_state.messages.append({\"role\": \"bot\", \"content\": answer})\r\n \r\n"
                },
                {
                    "date": 1765645100125,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -174,5 +174,5 @@\n     # Add bot message\r\n     st.session_state.messages.append({\"role\": \"bot\", \"content\": answer})\r\n \r\n     # Rerun to show updated chat\r\n-    st.experimental_rerun()\n\\ No newline at end of file\n+    st.rerun()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765645883701,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -80,8 +80,104 @@\n #     app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n \r\n \r\n \r\n+# import streamlit as st\r\n+# from dotenv import load_dotenv\r\n+# import os\r\n+# from operator import itemgetter\r\n+\r\n+# from src.helper import download_embeddings\r\n+# from src.prompt import system_prompt\r\n+# from langchain_pinecone import PineconeVectorStore\r\n+# from langchain_core.prompts import ChatPromptTemplate\r\n+# from langchain_core.runnables import RunnableLambda\r\n+# from langchain_groq import ChatGroq\r\n+\r\n+# # ---------------- Setup ----------------\r\n+# load_dotenv()\r\n+# os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n+# os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n+\r\n+# # ✅ Meta tensor fix: cache pipeline so it doesn't reload every rerun\r\n+# @st.cache_resource\r\n+# def load_pipeline():\r\n+#     # Embeddings + Pinecone retriever\r\n+#     embeddings = download_embeddings()\r\n+#     index_name = \"medical-chatbot\"\r\n+#     docsearch = PineconeVectorStore.from_existing_index(\r\n+#         index_name=index_name,\r\n+#         embedding=embeddings\r\n+#     )\r\n+#     retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+\r\n+#     # Groq LLM\r\n+#     chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+\r\n+#     # ✅ Meta tensor safeguard: if model accidentally loads on meta device\r\n+#     try:\r\n+#         if hasattr(chatModel, \"to\") and hasattr(chatModel, \"device\") and str(chatModel.device) == \"meta\":\r\n+#             # move empty structure to CPU\r\n+#             chatModel = chatModel.to_empty(device=\"cpu\")\r\n+#     except Exception as e:\r\n+#         st.warning(f\"Meta tensor fix applied: {e}\")\r\n+\r\n+#     # Prompt template\r\n+#     prompt = ChatPromptTemplate.from_messages([\r\n+#         (\"system\", system_prompt),\r\n+#         (\"human\", \"{input}\"),\r\n+#     ])\r\n+\r\n+#     # Format docs function\r\n+#     def format_docs(docs):\r\n+#         return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+\r\n+#     # Context branch\r\n+#     context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+\r\n+#     # RAG chain\r\n+#     rag_chain = (\r\n+#         {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+#         | prompt\r\n+#         | chatModel\r\n+#     )\r\n+#     return rag_chain\r\n+\r\n+# rag_chain = load_pipeline()\r\n+\r\n+# # ---------------- Streamlit UI ----------------\r\n+# st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n+# st.title(\"🩺 Medical Chatbot (Groq + Pinecone)\")\r\n+\r\n+# # Chat history\r\n+# if \"messages\" not in st.session_state:\r\n+#     st.session_state.messages = []\r\n+\r\n+# # Display chat history\r\n+# for msg in st.session_state.messages:\r\n+#     if msg[\"role\"] == \"user\":\r\n+#         st.markdown(f\"<div style='text-align:right; background:#007bff; color:white; padding:8px; border-radius:8px; margin:4px;'>You: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+#     else:\r\n+#         st.markdown(f\"<div style='text-align:left; background:#f1f1f1; padding:8px; border-radius:8px; margin:4px;'>Bot: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+\r\n+# # User input\r\n+# user_input = st.text_input(\"Type your message:\", key=\"input\")\r\n+\r\n+# if st.button(\"Send\") and user_input:\r\n+#     # Add user message\r\n+#     st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\r\n+\r\n+#     # Get bot response\r\n+#     with st.spinner(\"Thinking...\"):\r\n+#         response = rag_chain.invoke({\"input\": user_input})\r\n+#         answer = response.content if hasattr(response, \"content\") else str(response)\r\n+\r\n+#     # Add bot message\r\n+#     st.session_state.messages.append({\"role\": \"bot\", \"content\": answer})\r\n+\r\n+#     # Rerun to show updated chat\r\n+#     st.rerun()\r\n+\r\n import streamlit as st\r\n from dotenv import load_dotenv\r\n import os\r\n from operator import itemgetter\r\n"
                },
                {
                    "date": 1765646404621,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -180,8 +180,9 @@\n import streamlit as st\r\n from dotenv import load_dotenv\r\n import os\r\n from operator import itemgetter\r\n+from datetime import datetime\r\n \r\n from src.helper import download_embeddings\r\n from src.prompt import system_prompt\r\n from langchain_pinecone import PineconeVectorStore\r\n@@ -193,82 +194,98 @@\n load_dotenv()\r\n os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n \r\n-# ✅ Meta tensor fix: cache pipeline so it doesn't reload every rerun\r\n @st.cache_resource\r\n def load_pipeline():\r\n-    # Embeddings + Pinecone retriever\r\n     embeddings = download_embeddings()\r\n     index_name = \"medical-chatbot\"\r\n-    docsearch = PineconeVectorStore.from_existing_index(\r\n-        index_name=index_name,\r\n-        embedding=embeddings\r\n-    )\r\n+    docsearch = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)\r\n     retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-    # Groq LLM\r\n     chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-    # ✅ Meta tensor safeguard: if model accidentally loads on meta device\r\n     try:\r\n         if hasattr(chatModel, \"to\") and hasattr(chatModel, \"device\") and str(chatModel.device) == \"meta\":\r\n-            # move empty structure to CPU\r\n             chatModel = chatModel.to_empty(device=\"cpu\")\r\n     except Exception as e:\r\n         st.warning(f\"Meta tensor fix applied: {e}\")\r\n \r\n-    # Prompt template\r\n     prompt = ChatPromptTemplate.from_messages([\r\n         (\"system\", system_prompt),\r\n         (\"human\", \"{input}\"),\r\n     ])\r\n \r\n-    # Format docs function\r\n     def format_docs(docs):\r\n         return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n \r\n-    # Context branch\r\n     context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-\r\n-    # RAG chain\r\n-    rag_chain = (\r\n-        {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-        | prompt\r\n-        | chatModel\r\n-    )\r\n+    rag_chain = {\"context\": context_branch, \"input\": itemgetter(\"input\")} | prompt | chatModel\r\n     return rag_chain\r\n \r\n rag_chain = load_pipeline()\r\n \r\n # ---------------- Streamlit UI ----------------\r\n st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n-st.title(\"🩺 Medical Chatbot (Groq + Pinecone)\")\r\n \r\n+st.markdown(\"\"\"\r\n+    <div class=\"container\">\r\n+        <div class=\"card shadow\">\r\n+            <div class=\"card-header bg-primary text-white d-flex align-items-center\">\r\n+                <img src=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\" class=\"rounded-circle mr-2\" width=\"40\" height=\"40\">\r\n+                <div>\r\n+                    <h5 class=\"mb-0\">Medical Chatbot</h5>\r\n+                    <small>Ask me anything!</small>\r\n+                </div>\r\n+            </div>\r\n+        </div>\r\n+    </div>\r\n+\"\"\", unsafe_allow_html=True)\r\n+\r\n # Chat history\r\n if \"messages\" not in st.session_state:\r\n     st.session_state.messages = []\r\n \r\n-# Display chat history\r\n+# Chat body\r\n+chat_body = \"\"\r\n for msg in st.session_state.messages:\r\n+    time = msg.get(\"time\", \"\")\r\n     if msg[\"role\"] == \"user\":\r\n-        st.markdown(f\"<div style='text-align:right; background:#007bff; color:white; padding:8px; border-radius:8px; margin:4px;'>You: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+        chat_body += f\"\"\"\r\n+        <div class=\"d-flex justify-content-end mb-3\">\r\n+            <div class=\"bg-primary text-white p-2 rounded\">\r\n+                {msg['content']}\r\n+                <div class=\"text-right\"><small>{time}</small></div>\r\n+            </div>\r\n+        </div>\r\n+        \"\"\"\r\n     else:\r\n-        st.markdown(f\"<div style='text-align:left; background:#f1f1f1; padding:8px; border-radius:8px; margin:4px;'>Bot: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+        chat_body += f\"\"\"\r\n+        <div class=\"d-flex justify-content-start mb-3\">\r\n+            <div class=\"bg-light border p-2 rounded\">\r\n+                {msg['content']}\r\n+                <div class=\"text-left\"><small>{time}</small></div>\r\n+            </div>\r\n+        </div>\r\n+        \"\"\"\r\n \r\n-# User input\r\n-user_input = st.text_input(\"Type your message:\", key=\"input\")\r\n+st.markdown(f\"\"\"\r\n+    <div class=\"card-body overflow-auto\" style=\"height: 400px;\">\r\n+        {chat_body}\r\n+    </div>\r\n+\"\"\", unsafe_allow_html=True)\r\n \r\n-if st.button(\"Send\") and user_input:\r\n-    # Add user message\r\n-    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\r\n+# Input form\r\n+with st.form(\"chat_form\", clear_on_submit=True):\r\n+    user_input = st.text_input(\"Type your message:\", \"\")\r\n+    submitted = st.form_submit_button(\"Send\")\r\n \r\n-    # Get bot response\r\n+if submitted and user_input:\r\n+    now = datetime.now().strftime(\"%I:%M %p\")\r\n+    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input, \"time\": now})\r\n+\r\n     with st.spinner(\"Thinking...\"):\r\n         response = rag_chain.invoke({\"input\": user_input})\r\n         answer = response.content if hasattr(response, \"content\") else str(response)\r\n \r\n-    # Add bot message\r\n-    st.session_state.messages.append({\"role\": \"bot\", \"content\": answer})\r\n-\r\n-    # Rerun to show updated chat\r\n+    st.session_state.messages.append({\"role\": \"bot\", \"content\": answer, \"time\": now})\r\n     st.rerun()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765646661105,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,179 @@\n+# from flask import Flask, render_template, request\r\n+# from dotenv import load_dotenv\r\n+# import os\r\n+# from operator import itemgetter\r\n+\r\n+# from src.helper import download_embeddings\r\n+# from src.prompt import system_prompt\r\n+# from langchain_pinecone import PineconeVectorStore\r\n+# from langchain_core.prompts import ChatPromptTemplate\r\n+# from langchain_core.runnables import RunnableLambda\r\n+# from langchain_groq import ChatGroq\r\n+\r\n+# app = Flask(__name__)\r\n+# load_dotenv()\r\n+\r\n+# # API keys\r\n+# PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n+# GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n+# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n+# os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n+\r\n+# # Embeddings + Pinecone retriever\r\n+# embeddings = download_embeddings()\r\n+# index_name = \"medical-chatbot\"\r\n+# docsearch = PineconeVectorStore.from_existing_index(\r\n+#     index_name=index_name,\r\n+#     embedding=embeddings\r\n+# )\r\n+# retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+\r\n+# # Groq LLM\r\n+# chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+\r\n+# # Prompt template\r\n+# prompt = ChatPromptTemplate.from_messages([\r\n+#     (\"system\", system_prompt),\r\n+#     (\"human\", \"{input}\"),\r\n+# ])\r\n+\r\n+# # Format docs function\r\n+# def format_docs(docs):\r\n+#     return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+\r\n+# # Context branch\r\n+# context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+\r\n+# # RAG chain\r\n+# rag_chain = (\r\n+#     {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+#     | prompt\r\n+#     | chatModel\r\n+# )\r\n+\r\n+# @app.route(\"/\")\r\n+# def index():\r\n+#     return render_template(\"chat.html\")\r\n+\r\n+# @app.route(\"/get\", methods=[\"POST\"])\r\n+# def chat():\r\n+#     msg = request.form[\"msg\"]\r\n+#     #print(\"User:\", msg)\r\n+\r\n+#     try:\r\n+#         response = rag_chain.invoke({\"input\": msg})\r\n+#         #print(\"Raw response:\", response)\r\n+\r\n+#         # AIMessage object → .content\r\n+#         if hasattr(response, \"content\"):\r\n+#             answer = response.content\r\n+#         else:\r\n+#             answer = str(response)\r\n+\r\n+#         #print(\"Final Answer:\", answer)\r\n+#         return str(answer)\r\n+#     except Exception as e:\r\n+#         print(\"Error in rag_chain:\", e)\r\n+#         return \"Error: Could not get response\"\r\n+\r\n+# if __name__ == \"__main__\":\r\n+#     app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n+\r\n+\r\n+\r\n+import streamlit as st\r\n+from dotenv import load_dotenv\r\n+import os\r\n+from operator import itemgetter\r\n+\r\n+from src.helper import download_embeddings\r\n+from src.prompt import system_prompt\r\n+from langchain_pinecone import PineconeVectorStore\r\n+from langchain_core.prompts import ChatPromptTemplate\r\n+from langchain_core.runnables import RunnableLambda\r\n+from langchain_groq import ChatGroq\r\n+\r\n+# ---------------- Setup ----------------\r\n+load_dotenv()\r\n+os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n+os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n+\r\n+# ✅ Meta tensor fix: cache pipeline so it doesn't reload every rerun\r\n+@st.cache_resource\r\n+def load_pipeline():\r\n+    # Embeddings + Pinecone retriever\r\n+    embeddings = download_embeddings()\r\n+    index_name = \"medical-chatbot\"\r\n+    docsearch = PineconeVectorStore.from_existing_index(\r\n+        index_name=index_name,\r\n+        embedding=embeddings\r\n+    )\r\n+    retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+\r\n+    # Groq LLM\r\n+    chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+\r\n+    # ✅ Meta tensor safeguard: if model accidentally loads on meta device\r\n+    try:\r\n+        if hasattr(chatModel, \"to\") and hasattr(chatModel, \"device\") and str(chatModel.device) == \"meta\":\r\n+            # move empty structure to CPU\r\n+            chatModel = chatModel.to_empty(device=\"cpu\")\r\n+    except Exception as e:\r\n+        st.warning(f\"Meta tensor fix applied: {e}\")\r\n+\r\n+    # Prompt template\r\n+    prompt = ChatPromptTemplate.from_messages([\r\n+        (\"system\", system_prompt),\r\n+        (\"human\", \"{input}\"),\r\n+    ])\r\n+\r\n+    # Format docs function\r\n+    def format_docs(docs):\r\n+        return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+\r\n+    # Context branch\r\n+    context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+\r\n+    # RAG chain\r\n+    rag_chain = (\r\n+        {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+        | prompt\r\n+        | chatModel\r\n+    )\r\n+    return rag_chain\r\n+\r\n+rag_chain = load_pipeline()\r\n+\r\n+# ---------------- Streamlit UI ----------------\r\n+st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n+st.title(\"🩺 Medical Chatbot (Groq + Pinecone)\")\r\n+\r\n+# Chat history\r\n+if \"messages\" not in st.session_state:\r\n+    st.session_state.messages = []\r\n+\r\n+# Display chat history\r\n+for msg in st.session_state.messages:\r\n+    if msg[\"role\"] == \"user\":\r\n+        st.markdown(f\"<div style='text-align:right; background:#007bff; color:white; padding:8px; border-radius:8px; margin:4px;'>You: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+    else:\r\n+        st.markdown(f\"<div style='text-align:left; background:#f1f1f1; padding:8px; border-radius:8px; margin:4px;'>Bot: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+\r\n+# User input\r\n+user_input = st.text_input(\"Type your message:\", key=\"input\")\r\n+\r\n+if st.button(\"Send\") and user_input:\r\n+    # Add user message\r\n+    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\r\n+\r\n+    # Get bot response\r\n+    with st.spinner(\"Thinking...\"):\r\n+        response = rag_chain.invoke({\"input\": user_input})\r\n+        answer = response.content if hasattr(response, \"content\") else str(response)\r\n+\r\n+    # Add bot message\r\n+    st.session_state.messages.append({\"role\": \"bot\", \"content\": answer})\r\n+\r\n+    # Rerun to show updated chat\r\n+    st.rerun()\r\n+\r\n"
                },
                {
                    "date": 1765646821351,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -80,12 +80,110 @@\n #     app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n \r\n \r\n \r\n+# import streamlit as st\r\n+# from dotenv import load_dotenv\r\n+# import os\r\n+# from operator import itemgetter\r\n+\r\n+# from src.helper import download_embeddings\r\n+# from src.prompt import system_prompt\r\n+# from langchain_pinecone import PineconeVectorStore\r\n+# from langchain_core.prompts import ChatPromptTemplate\r\n+# from langchain_core.runnables import RunnableLambda\r\n+# from langchain_groq import ChatGroq\r\n+\r\n+# # ---------------- Setup ----------------\r\n+# load_dotenv()\r\n+# os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n+# os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n+\r\n+# # ✅ Meta tensor fix: cache pipeline so it doesn't reload every rerun\r\n+# @st.cache_resource\r\n+# def load_pipeline():\r\n+#     # Embeddings + Pinecone retriever\r\n+#     embeddings = download_embeddings()\r\n+#     index_name = \"medical-chatbot\"\r\n+#     docsearch = PineconeVectorStore.from_existing_index(\r\n+#         index_name=index_name,\r\n+#         embedding=embeddings\r\n+#     )\r\n+#     retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+\r\n+#     # Groq LLM\r\n+#     chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+\r\n+#     # ✅ Meta tensor safeguard: if model accidentally loads on meta device\r\n+#     try:\r\n+#         if hasattr(chatModel, \"to\") and hasattr(chatModel, \"device\") and str(chatModel.device) == \"meta\":\r\n+#             # move empty structure to CPU\r\n+#             chatModel = chatModel.to_empty(device=\"cpu\")\r\n+#     except Exception as e:\r\n+#         st.warning(f\"Meta tensor fix applied: {e}\")\r\n+\r\n+#     # Prompt template\r\n+#     prompt = ChatPromptTemplate.from_messages([\r\n+#         (\"system\", system_prompt),\r\n+#         (\"human\", \"{input}\"),\r\n+#     ])\r\n+\r\n+#     # Format docs function\r\n+#     def format_docs(docs):\r\n+#         return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+\r\n+#     # Context branch\r\n+#     context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+\r\n+#     # RAG chain\r\n+#     rag_chain = (\r\n+#         {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+#         | prompt\r\n+#         | chatModel\r\n+#     )\r\n+#     return rag_chain\r\n+\r\n+# rag_chain = load_pipeline()\r\n+\r\n+# # ---------------- Streamlit UI ----------------\r\n+# st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n+# st.title(\"🩺 Medical Chatbot (Groq + Pinecone)\")\r\n+\r\n+# # Chat history\r\n+# if \"messages\" not in st.session_state:\r\n+#     st.session_state.messages = []\r\n+\r\n+# # Display chat history\r\n+# for msg in st.session_state.messages:\r\n+#     if msg[\"role\"] == \"user\":\r\n+#         st.markdown(f\"<div style='text-align:right; background:#007bff; color:white; padding:8px; border-radius:8px; margin:4px;'>You: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+#     else:\r\n+#         st.markdown(f\"<div style='text-align:left; background:#f1f1f1; padding:8px; border-radius:8px; margin:4px;'>Bot: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+\r\n+# # User input\r\n+# user_input = st.text_input(\"Type your message:\", key=\"input\")\r\n+\r\n+# if st.button(\"Send\") and user_input:\r\n+#     # Add user message\r\n+#     st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\r\n+\r\n+#     # Get bot response\r\n+#     with st.spinner(\"Thinking...\"):\r\n+#         response = rag_chain.invoke({\"input\": user_input})\r\n+#         answer = response.content if hasattr(response, \"content\") else str(response)\r\n+\r\n+#     # Add bot message\r\n+#     st.session_state.messages.append({\"role\": \"bot\", \"content\": answer})\r\n+\r\n+#     # Rerun to show updated chat\r\n+#     st.rerun()\r\n+\r\n+\r\n import streamlit as st\r\n from dotenv import load_dotenv\r\n import os\r\n from operator import itemgetter\r\n+from datetime import datetime\r\n \r\n from src.helper import download_embeddings\r\n from src.prompt import system_prompt\r\n from langchain_pinecone import PineconeVectorStore\r\n@@ -97,83 +195,97 @@\n load_dotenv()\r\n os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n \r\n-# ✅ Meta tensor fix: cache pipeline so it doesn't reload every rerun\r\n @st.cache_resource\r\n def load_pipeline():\r\n-    # Embeddings + Pinecone retriever\r\n     embeddings = download_embeddings()\r\n     index_name = \"medical-chatbot\"\r\n-    docsearch = PineconeVectorStore.from_existing_index(\r\n-        index_name=index_name,\r\n-        embedding=embeddings\r\n-    )\r\n+    docsearch = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)\r\n     retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-    # Groq LLM\r\n     chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-    # ✅ Meta tensor safeguard: if model accidentally loads on meta device\r\n+    # Meta tensor safeguard\r\n     try:\r\n         if hasattr(chatModel, \"to\") and hasattr(chatModel, \"device\") and str(chatModel.device) == \"meta\":\r\n-            # move empty structure to CPU\r\n             chatModel = chatModel.to_empty(device=\"cpu\")\r\n     except Exception as e:\r\n         st.warning(f\"Meta tensor fix applied: {e}\")\r\n \r\n-    # Prompt template\r\n     prompt = ChatPromptTemplate.from_messages([\r\n         (\"system\", system_prompt),\r\n         (\"human\", \"{input}\"),\r\n     ])\r\n \r\n-    # Format docs function\r\n     def format_docs(docs):\r\n         return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n \r\n-    # Context branch\r\n     context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-\r\n-    # RAG chain\r\n-    rag_chain = (\r\n-        {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-        | prompt\r\n-        | chatModel\r\n-    )\r\n+    rag_chain = {\"context\": context_branch, \"input\": itemgetter(\"input\")} | prompt | chatModel\r\n     return rag_chain\r\n \r\n rag_chain = load_pipeline()\r\n \r\n # ---------------- Streamlit UI ----------------\r\n st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n-st.title(\"🩺 Medical Chatbot (Groq + Pinecone)\")\r\n \r\n+# Header\r\n+st.markdown(\"\"\"\r\n+<div class=\"card shadow mb-3\">\r\n+  <div class=\"card-header bg-primary text-white d-flex align-items-center\">\r\n+    <img src=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\" class=\"rounded-circle mr-2\" width=\"40\" height=\"40\">\r\n+    <div>\r\n+      <h5 class=\"mb-0\">Medical Chatbot</h5>\r\n+      <small>Ask me anything!</small>\r\n+    </div>\r\n+  </div>\r\n+</div>\r\n+\"\"\", unsafe_allow_html=True)\r\n+\r\n # Chat history\r\n if \"messages\" not in st.session_state:\r\n     st.session_state.messages = []\r\n \r\n-# Display chat history\r\n+chat_html = \"\"\r\n for msg in st.session_state.messages:\r\n+    time = msg.get(\"time\", \"\")\r\n     if msg[\"role\"] == \"user\":\r\n-        st.markdown(f\"<div style='text-align:right; background:#007bff; color:white; padding:8px; border-radius:8px; margin:4px;'>You: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+        chat_html += f\"\"\"\r\n+        <div class=\"d-flex justify-content-end mb-3\">\r\n+          <div class=\"bg-primary text-white p-2 rounded\">\r\n+            {msg['content']}\r\n+            <div class=\"text-right\"><small>{time}</small></div>\r\n+          </div>\r\n+        </div>\r\n+        \"\"\"\r\n     else:\r\n-        st.markdown(f\"<div style='text-align:left; background:#f1f1f1; padding:8px; border-radius:8px; margin:4px;'>Bot: {msg['content']}</div>\", unsafe_allow_html=True)\r\n+        chat_html += f\"\"\"\r\n+        <div class=\"d-flex justify-content-start mb-3\">\r\n+          <div class=\"bg-light border p-2 rounded\">\r\n+            {msg['content']}\r\n+            <div class=\"text-left\"><small>{time}</small></div>\r\n+          </div>\r\n+        </div>\r\n+        \"\"\"\r\n \r\n-# User input\r\n-user_input = st.text_input(\"Type your message:\", key=\"input\")\r\n+st.markdown(f\"\"\"\r\n+<div class=\"card-body overflow-auto\" style=\"height:400px;\">\r\n+  {chat_html}\r\n+</div>\r\n+\"\"\", unsafe_allow_html=True)\r\n \r\n-if st.button(\"Send\") and user_input:\r\n-    # Add user message\r\n-    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\r\n+# Input form\r\n+with st.form(\"chat_form\", clear_on_submit=True):\r\n+    user_input = st.text_input(\"Type your message:\", \"\")\r\n+    submitted = st.form_submit_button(\"Send\")\r\n \r\n-    # Get bot response\r\n+if submitted and user_input:\r\n+    now = datetime.now().strftime(\"%I:%M %p\")\r\n+    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input, \"time\": now})\r\n+\r\n     with st.spinner(\"Thinking...\"):\r\n         response = rag_chain.invoke({\"input\": user_input})\r\n         answer = response.content if hasattr(response, \"content\") else str(response)\r\n \r\n-    # Add bot message\r\n-    st.session_state.messages.append({\"role\": \"bot\", \"content\": answer})\r\n-\r\n-    # Rerun to show updated chat\r\n+    st.session_state.messages.append({\"role\": \"bot\", \"content\": answer, \"time\": now})\r\n     st.rerun()\r\n-\r\n"
                },
                {
                    "date": 1765646988445,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,378 +1,87 @@\n-# from flask import Flask, render_template, request\r\n-# from dotenv import load_dotenv\r\n-# import os\r\n-# from operator import itemgetter\r\n-\r\n-# from src.helper import download_embeddings\r\n-# from src.prompt import system_prompt\r\n-# from langchain_pinecone import PineconeVectorStore\r\n-# from langchain_core.prompts import ChatPromptTemplate\r\n-# from langchain_core.runnables import RunnableLambda\r\n-# from langchain_groq import ChatGroq\r\n-\r\n-# app = Flask(__name__)\r\n-# load_dotenv()\r\n-\r\n-# # API keys\r\n-# PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n-# GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n-# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n-# os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n-\r\n-# # Embeddings + Pinecone retriever\r\n-# embeddings = download_embeddings()\r\n-# index_name = \"medical-chatbot\"\r\n-# docsearch = PineconeVectorStore.from_existing_index(\r\n-#     index_name=index_name,\r\n-#     embedding=embeddings\r\n-# )\r\n-# retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n-\r\n-# # Groq LLM\r\n-# chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n-\r\n-# # Prompt template\r\n-# prompt = ChatPromptTemplate.from_messages([\r\n-#     (\"system\", system_prompt),\r\n-#     (\"human\", \"{input}\"),\r\n-# ])\r\n-\r\n-# # Format docs function\r\n-# def format_docs(docs):\r\n-#     return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n-\r\n-# # Context branch\r\n-# context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-\r\n-# # RAG chain\r\n-# rag_chain = (\r\n-#     {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-#     | prompt\r\n-#     | chatModel\r\n-# )\r\n-\r\n-# @app.route(\"/\")\r\n-# def index():\r\n-#     return render_template(\"chat.html\")\r\n-\r\n-# @app.route(\"/get\", methods=[\"POST\"])\r\n-# def chat():\r\n-#     msg = request.form[\"msg\"]\r\n-#     #print(\"User:\", msg)\r\n-\r\n-#     try:\r\n-#         response = rag_chain.invoke({\"input\": msg})\r\n-#         #print(\"Raw response:\", response)\r\n-\r\n-#         # AIMessage object → .content\r\n-#         if hasattr(response, \"content\"):\r\n-#             answer = response.content\r\n-#         else:\r\n-#             answer = str(response)\r\n-\r\n-#         #print(\"Final Answer:\", answer)\r\n-#         return str(answer)\r\n-#     except Exception as e:\r\n-#         print(\"Error in rag_chain:\", e)\r\n-#         return \"Error: Could not get response\"\r\n-\r\n-# if __name__ == \"__main__\":\r\n-#     app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n-\r\n-\r\n-\r\n-# import streamlit as st\r\n-# from dotenv import load_dotenv\r\n-# import os\r\n-# from operator import itemgetter\r\n-\r\n-# from src.helper import download_embeddings\r\n-# from src.prompt import system_prompt\r\n-# from langchain_pinecone import PineconeVectorStore\r\n-# from langchain_core.prompts import ChatPromptTemplate\r\n-# from langchain_core.runnables import RunnableLambda\r\n-# from langchain_groq import ChatGroq\r\n-\r\n-# # ---------------- Setup ----------------\r\n-# load_dotenv()\r\n-# os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n-# os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n-\r\n-# # ✅ Meta tensor fix: cache pipeline so it doesn't reload every rerun\r\n-# @st.cache_resource\r\n-# def load_pipeline():\r\n-#     # Embeddings + Pinecone retriever\r\n-#     embeddings = download_embeddings()\r\n-#     index_name = \"medical-chatbot\"\r\n-#     docsearch = PineconeVectorStore.from_existing_index(\r\n-#         index_name=index_name,\r\n-#         embedding=embeddings\r\n-#     )\r\n-#     retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n-\r\n-#     # Groq LLM\r\n-#     chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n-\r\n-#     # ✅ Meta tensor safeguard: if model accidentally loads on meta device\r\n-#     try:\r\n-#         if hasattr(chatModel, \"to\") and hasattr(chatModel, \"device\") and str(chatModel.device) == \"meta\":\r\n-#             # move empty structure to CPU\r\n-#             chatModel = chatModel.to_empty(device=\"cpu\")\r\n-#     except Exception as e:\r\n-#         st.warning(f\"Meta tensor fix applied: {e}\")\r\n-\r\n-#     # Prompt template\r\n-#     prompt = ChatPromptTemplate.from_messages([\r\n-#         (\"system\", system_prompt),\r\n-#         (\"human\", \"{input}\"),\r\n-#     ])\r\n-\r\n-#     # Format docs function\r\n-#     def format_docs(docs):\r\n-#         return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n-\r\n-#     # Context branch\r\n-#     context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-\r\n-#     # RAG chain\r\n-#     rag_chain = (\r\n-#         {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-#         | prompt\r\n-#         | chatModel\r\n-#     )\r\n-#     return rag_chain\r\n-\r\n-# rag_chain = load_pipeline()\r\n-\r\n-# # ---------------- Streamlit UI ----------------\r\n-# st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n-# st.title(\"🩺 Medical Chatbot (Groq + Pinecone)\")\r\n-\r\n-# # Chat history\r\n-# if \"messages\" not in st.session_state:\r\n-#     st.session_state.messages = []\r\n-\r\n-# # Display chat history\r\n-# for msg in st.session_state.messages:\r\n-#     if msg[\"role\"] == \"user\":\r\n-#         st.markdown(f\"<div style='text-align:right; background:#007bff; color:white; padding:8px; border-radius:8px; margin:4px;'>You: {msg['content']}</div>\", unsafe_allow_html=True)\r\n-#     else:\r\n-#         st.markdown(f\"<div style='text-align:left; background:#f1f1f1; padding:8px; border-radius:8px; margin:4px;'>Bot: {msg['content']}</div>\", unsafe_allow_html=True)\r\n-\r\n-# # User input\r\n-# user_input = st.text_input(\"Type your message:\", key=\"input\")\r\n-\r\n-# if st.button(\"Send\") and user_input:\r\n-#     # Add user message\r\n-#     st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\r\n-\r\n-#     # Get bot response\r\n-#     with st.spinner(\"Thinking...\"):\r\n-#         response = rag_chain.invoke({\"input\": user_input})\r\n-#         answer = response.content if hasattr(response, \"content\") else str(response)\r\n-\r\n-#     # Add bot message\r\n-#     st.session_state.messages.append({\"role\": \"bot\", \"content\": answer})\r\n-\r\n-#     # Rerun to show updated chat\r\n-#     st.rerun()\r\n-\r\n-\r\n-import streamlit as st\r\n+from flask import Flask, render_template, request\r\n from dotenv import load_dotenv\r\n import os\r\n from operator import itemgetter\r\n-from datetime import datetime\r\n \r\n from src.helper import download_embeddings\r\n from src.prompt import system_prompt\r\n from langchain_pinecone import PineconeVectorStore\r\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n \r\n-# ---------------- Setup ----------------\r\n+app = Flask(__name__)\r\n load_dotenv()\r\n-os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n-os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n \r\n-@st.cache_resource\r\n-def load_pipeline():\r\n-    embeddings = download_embeddings()\r\n-    index_name = \"medical-chatbot\"\r\n-    docsearch = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)\r\n-    retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+# API keys\r\n+PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n+GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n+os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n+os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n \r\n-    chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+# Embeddings + Pinecone retriever\r\n+embeddings = download_embeddings()\r\n+index_name = \"medical-chatbot\"\r\n+docsearch = PineconeVectorStore.from_existing_index(\r\n+    index_name=index_name,\r\n+    embedding=embeddings\r\n+)\r\n+retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-    # Meta tensor safeguard\r\n-    try:\r\n-        if hasattr(chatModel, \"to\") and hasattr(chatModel, \"device\") and str(chatModel.device) == \"meta\":\r\n-            chatModel = chatModel.to_empty(device=\"cpu\")\r\n-    except Exception as e:\r\n-        st.warning(f\"Meta tensor fix applied: {e}\")\r\n+# Groq LLM\r\n+chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-    prompt = ChatPromptTemplate.from_messages([\r\n-        (\"system\", system_prompt),\r\n-        (\"human\", \"{input}\"),\r\n-    ])\r\n+# Prompt template\r\n+prompt = ChatPromptTemplate.from_messages([\r\n+    (\"system\", system_prompt),\r\n+    (\"human\", \"{input}\"),\r\n+])\r\n \r\n-    def format_docs(docs):\r\n-        return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+# Format docs function\r\n+def format_docs(docs):\r\n+    return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n \r\n-    context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-    rag_chain = {\"context\": context_branch, \"input\": itemgetter(\"input\")} | prompt | chatModel\r\n-    return rag_chain\r\n+# Context branch\r\n+context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n \r\n-rag_chain = load_pipeline()\r\n+# RAG chain\r\n+rag_chain = (\r\n+    {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+    | prompt\r\n+    | chatModel\r\n+)\r\n \r\n-# ---------------- Streamlit UI ----------------\r\n-st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n+@app.route(\"/\")\r\n+def index():\r\n+    return render_template(\"chat.html\")\r\n \r\n-# Header\r\n-st.markdown(\"\"\"\r\n-<div class=\"card shadow mb-3\">\r\n-  <div class=\"card-header bg-primary text-white d-flex align-items-center\">\r\n-    <img src=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\" class=\"rounded-circle mr-2\" width=\"40\" height=\"40\">\r\n-    <div>\r\n-      <h5 class=\"mb-0\">Medical Chatbot</h5>\r\n-      <small>Ask me anything!</small>\r\n-    </div>\r\n-  </div>\r\n-</div>\r\n-\"\"\", unsafe_allow_html=True)\r\n+@app.route(\"/get\", methods=[\"POST\"])\r\n+def chat():\r\n+    msg = request.form[\"msg\"]\r\n+    #print(\"User:\", msg)\r\n \r\n-# Chat history\r\n-if \"messages\" not in st.session_state:\r\n-    st.session_state.messages = []\r\n+    try:\r\n+        response = rag_chain.invoke({\"input\": msg})\r\n+        #print(\"Raw response:\", response)\r\n \r\n-chat_html = \"\"\r\n-for msg in st.session_state.messages:\r\n-    time = msg.get(\"time\", \"\")\r\n-    if msg[\"role\"] == \"user\":\r\n-        chat_html += f\"\"\"\r\n-        <div class=\"d-flex justify-content-end mb-3\">\r\n-          <div class=\"bg-primary text-white p-2 rounded\">\r\n-            {msg['content']}\r\n-            <div class=\"text-right\"><small>{time}</small></div>\r\n-          </div>\r\n-        </div>\r\n-        \"\"\"\r\n-    else:\r\n-        chat_html += f\"\"\"\r\n-        <div class=\"d-flex justify-content-start mb-3\">\r\n-          <div class=\"bg-light border p-2 rounded\">\r\n-            {msg['content']}\r\n-            <div class=\"text-left\"><small>{time}</small></div>\r\n-          </div>\r\n-        </div>\r\n-        \"\"\"\r\n+        # AIMessage object → .content\r\n+        if hasattr(response, \"content\"):\r\n+            answer = response.content\r\n+        else:\r\n+            answer = str(response)\r\n \r\n-st.markdown(f\"\"\"\r\n-<div class=\"card-body overflow-auto\" style=\"height:400px;\">\r\n-  {chat_html}\r\n-</div>\r\n-\"\"\", unsafe_allow_html=True)\r\n+        #print(\"Final Answer:\", answer)\r\n+        return str(answer)\r\n+    except Exception as e:\r\n+        print(\"Error in rag_chain:\", e)\r\n+        return \"Error: Could not get response\"\r\n \r\n-# Input form\r\n-with st.form(\"chat_form\", clear_on_submit=True):\r\n-    user_input = st.text_input(\"Type your message:\", \"\")\r\n-    submitted = st.form_submit_button(\"Send\")\r\n+if __name__ == \"__main__\":\r\n+    app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n \r\n-if submitted and user_input:\r\n-    now = datetime.now().strftime(\"%I:%M %p\")\r\n-    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input, \"time\": now})\r\n \r\n-    with st.spinner(\"Thinking...\"):\r\n-        response = rag_chain.invoke({\"input\": user_input})\r\n-        answer = response.content if hasattr(response, \"content\") else str(response)\r\n \r\n-    st.session_state.messages.append({\"role\": \"bot\", \"content\": answer, \"time\": now})\r\n-    st.rerun()\r\n-# from flask import Flask, render_template, request\r\n-# from dotenv import load_dotenv\r\n-# import os\r\n-# from operator import itemgetter\r\n-\r\n-# from src.helper import download_embeddings\r\n-# from src.prompt import system_prompt\r\n-# from langchain_pinecone import PineconeVectorStore\r\n-# from langchain_core.prompts import ChatPromptTemplate\r\n-# from langchain_core.runnables import RunnableLambda\r\n-# from langchain_groq import ChatGroq\r\n-\r\n-# app = Flask(__name__)\r\n-# load_dotenv()\r\n-\r\n-# # API keys\r\n-# PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n-# GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n-# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n-# os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n-\r\n-# # Embeddings + Pinecone retriever\r\n-# embeddings = download_embeddings()\r\n-# index_name = \"medical-chatbot\"\r\n-# docsearch = PineconeVectorStore.from_existing_index(\r\n-#     index_name=index_name,\r\n-#     embedding=embeddings\r\n-# )\r\n-# retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n-\r\n-# # Groq LLM\r\n-# chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n-\r\n-# # Prompt template\r\n-# prompt = ChatPromptTemplate.from_messages([\r\n-#     (\"system\", system_prompt),\r\n-#     (\"human\", \"{input}\"),\r\n-# ])\r\n-\r\n-# # Format docs function\r\n-# def format_docs(docs):\r\n-#     return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n-\r\n-# # Context branch\r\n-# context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-\r\n-# # RAG chain\r\n-# rag_chain = (\r\n-#     {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-#     | prompt\r\n-#     | chatModel\r\n-# )\r\n-\r\n-# @app.route(\"/\")\r\n-# def index():\r\n-#     return render_template(\"chat.html\")\r\n-\r\n-# @app.route(\"/get\", methods=[\"POST\"])\r\n-# def chat():\r\n-#     msg = request.form[\"msg\"]\r\n-#     #print(\"User:\", msg)\r\n-\r\n-#     try:\r\n-#         response = rag_chain.invoke({\"input\": msg})\r\n-#         #print(\"Raw response:\", response)\r\n-\r\n-#         # AIMessage object → .content\r\n-#         if hasattr(response, \"content\"):\r\n-#             answer = response.content\r\n-#         else:\r\n-#             answer = str(response)\r\n-\r\n-#         #print(\"Final Answer:\", answer)\r\n-#         return str(answer)\r\n-#     except Exception as e:\r\n-#         print(\"Error in rag_chain:\", e)\r\n-#         return \"Error: Could not get response\"\r\n-\r\n-# if __name__ == \"__main__\":\r\n-#     app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n-\r\n-\r\n-\r\n # import streamlit as st\r\n # from dotenv import load_dotenv\r\n # import os\r\n # from operator import itemgetter\r\n@@ -467,116 +176,5 @@\n \r\n #     # Rerun to show updated chat\r\n #     st.rerun()\r\n \r\n-import streamlit as st\r\n-from dotenv import load_dotenv\r\n-import os\r\n-from operator import itemgetter\r\n-from datetime import datetime\r\n \r\n-from src.helper import download_embeddings\r\n-from src.prompt import system_prompt\r\n-from langchain_pinecone import PineconeVectorStore\r\n-from langchain_core.prompts import ChatPromptTemplate\r\n-from langchain_core.runnables import RunnableLambda\r\n-from langchain_groq import ChatGroq\r\n-\r\n-# ---------------- Setup ----------------\r\n-load_dotenv()\r\n-os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\r\n-os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\r\n-\r\n-@st.cache_resource\r\n-def load_pipeline():\r\n-    embeddings = download_embeddings()\r\n-    index_name = \"medical-chatbot\"\r\n-    docsearch = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)\r\n-    retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n-\r\n-    chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n-\r\n-    try:\r\n-        if hasattr(chatModel, \"to\") and hasattr(chatModel, \"device\") and str(chatModel.device) == \"meta\":\r\n-            chatModel = chatModel.to_empty(device=\"cpu\")\r\n-    except Exception as e:\r\n-        st.warning(f\"Meta tensor fix applied: {e}\")\r\n-\r\n-    prompt = ChatPromptTemplate.from_messages([\r\n-        (\"system\", system_prompt),\r\n-        (\"human\", \"{input}\"),\r\n-    ])\r\n-\r\n-    def format_docs(docs):\r\n-        return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n-\r\n-    context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-    rag_chain = {\"context\": context_branch, \"input\": itemgetter(\"input\")} | prompt | chatModel\r\n-    return rag_chain\r\n-\r\n-rag_chain = load_pipeline()\r\n-\r\n-# ---------------- Streamlit UI ----------------\r\n-st.set_page_config(page_title=\"Medical Chatbot\", page_icon=\"🩺\", layout=\"centered\")\r\n-\r\n-st.markdown(\"\"\"\r\n-    <div class=\"container\">\r\n-        <div class=\"card shadow\">\r\n-            <div class=\"card-header bg-primary text-white d-flex align-items-center\">\r\n-                <img src=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\" class=\"rounded-circle mr-2\" width=\"40\" height=\"40\">\r\n-                <div>\r\n-                    <h5 class=\"mb-0\">Medical Chatbot</h5>\r\n-                    <small>Ask me anything!</small>\r\n-                </div>\r\n-            </div>\r\n-        </div>\r\n-    </div>\r\n-\"\"\", unsafe_allow_html=True)\r\n-\r\n-# Chat history\r\n-if \"messages\" not in st.session_state:\r\n-    st.session_state.messages = []\r\n-\r\n-# Chat body\r\n-chat_body = \"\"\r\n-for msg in st.session_state.messages:\r\n-    time = msg.get(\"time\", \"\")\r\n-    if msg[\"role\"] == \"user\":\r\n-        chat_body += f\"\"\"\r\n-        <div class=\"d-flex justify-content-end mb-3\">\r\n-            <div class=\"bg-primary text-white p-2 rounded\">\r\n-                {msg['content']}\r\n-                <div class=\"text-right\"><small>{time}</small></div>\r\n-            </div>\r\n-        </div>\r\n-        \"\"\"\r\n-    else:\r\n-        chat_body += f\"\"\"\r\n-        <div class=\"d-flex justify-content-start mb-3\">\r\n-            <div class=\"bg-light border p-2 rounded\">\r\n-                {msg['content']}\r\n-                <div class=\"text-left\"><small>{time}</small></div>\r\n-            </div>\r\n-        </div>\r\n-        \"\"\"\r\n-\r\n-st.markdown(f\"\"\"\r\n-    <div class=\"card-body overflow-auto\" style=\"height: 400px;\">\r\n-        {chat_body}\r\n-    </div>\r\n-\"\"\", unsafe_allow_html=True)\r\n-\r\n-# Input form\r\n-with st.form(\"chat_form\", clear_on_submit=True):\r\n-    user_input = st.text_input(\"Type your message:\", \"\")\r\n-    submitted = st.form_submit_button(\"Send\")\r\n-\r\n-if submitted and user_input:\r\n-    now = datetime.now().strftime(\"%I:%M %p\")\r\n-    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input, \"time\": now})\r\n-\r\n-    with st.spinner(\"Thinking...\"):\r\n-        response = rag_chain.invoke({\"input\": user_input})\r\n-        answer = response.content if hasattr(response, \"content\") else str(response)\r\n-\r\n-    st.session_state.messages.append({\"role\": \"bot\", \"content\": answer, \"time\": now})\r\n-    st.rerun()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765647383697,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,84 +1,84 @@\n-from flask import Flask, render_template, request\r\n-from dotenv import load_dotenv\r\n-import os\r\n-from operator import itemgetter\r\n+# from flask import Flask, render_template, request\r\n+# from dotenv import load_dotenv\r\n+# import os\r\n+# from operator import itemgetter\r\n \r\n-from src.helper import download_embeddings\r\n-from src.prompt import system_prompt\r\n-from langchain_pinecone import PineconeVectorStore\r\n-from langchain_core.prompts import ChatPromptTemplate\r\n-from langchain_core.runnables import RunnableLambda\r\n-from langchain_groq import ChatGroq\r\n+# from src.helper import download_embeddings\r\n+# from src.prompt import system_prompt\r\n+# from langchain_pinecone import PineconeVectorStore\r\n+# from langchain_core.prompts import ChatPromptTemplate\r\n+# from langchain_core.runnables import RunnableLambda\r\n+# from langchain_groq import ChatGroq\r\n \r\n-app = Flask(__name__)\r\n-load_dotenv()\r\n+# app = Flask(__name__)\r\n+# load_dotenv()\r\n \r\n-# API keys\r\n-PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n-GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n-os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n-os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n+# # API keys\r\n+# PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n+# GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n+# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n+# os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n \r\n-# Embeddings + Pinecone retriever\r\n-embeddings = download_embeddings()\r\n-index_name = \"medical-chatbot\"\r\n-docsearch = PineconeVectorStore.from_existing_index(\r\n-    index_name=index_name,\r\n-    embedding=embeddings\r\n-)\r\n-retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+# # Embeddings + Pinecone retriever\r\n+# embeddings = download_embeddings()\r\n+# index_name = \"medical-chatbot\"\r\n+# docsearch = PineconeVectorStore.from_existing_index(\r\n+#     index_name=index_name,\r\n+#     embedding=embeddings\r\n+# )\r\n+# retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-# Groq LLM\r\n-chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+# # Groq LLM\r\n+# chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-# Prompt template\r\n-prompt = ChatPromptTemplate.from_messages([\r\n-    (\"system\", system_prompt),\r\n-    (\"human\", \"{input}\"),\r\n-])\r\n+# # Prompt template\r\n+# prompt = ChatPromptTemplate.from_messages([\r\n+#     (\"system\", system_prompt),\r\n+#     (\"human\", \"{input}\"),\r\n+# ])\r\n \r\n-# Format docs function\r\n-def format_docs(docs):\r\n-    return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+# # Format docs function\r\n+# def format_docs(docs):\r\n+#     return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n \r\n-# Context branch\r\n-context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+# # Context branch\r\n+# context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n \r\n-# RAG chain\r\n-rag_chain = (\r\n-    {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-    | prompt\r\n-    | chatModel\r\n-)\r\n+# # RAG chain\r\n+# rag_chain = (\r\n+#     {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+#     | prompt\r\n+#     | chatModel\r\n+# )\r\n \r\n-@app.route(\"/\")\r\n-def index():\r\n-    return render_template(\"chat.html\")\r\n+# @app.route(\"/\")\r\n+# def index():\r\n+#     return render_template(\"chat.html\")\r\n \r\n-@app.route(\"/get\", methods=[\"POST\"])\r\n-def chat():\r\n-    msg = request.form[\"msg\"]\r\n-    #print(\"User:\", msg)\r\n+# @app.route(\"/get\", methods=[\"POST\"])\r\n+# def chat():\r\n+#     msg = request.form[\"msg\"]\r\n+#     #print(\"User:\", msg)\r\n \r\n-    try:\r\n-        response = rag_chain.invoke({\"input\": msg})\r\n-        #print(\"Raw response:\", response)\r\n+#     try:\r\n+#         response = rag_chain.invoke({\"input\": msg})\r\n+#         #print(\"Raw response:\", response)\r\n \r\n-        # AIMessage object → .content\r\n-        if hasattr(response, \"content\"):\r\n-            answer = response.content\r\n-        else:\r\n-            answer = str(response)\r\n+#         # AIMessage object → .content\r\n+#         if hasattr(response, \"content\"):\r\n+#             answer = response.content\r\n+#         else:\r\n+#             answer = str(response)\r\n \r\n-        #print(\"Final Answer:\", answer)\r\n-        return str(answer)\r\n-    except Exception as e:\r\n-        print(\"Error in rag_chain:\", e)\r\n-        return \"Error: Could not get response\"\r\n+#         #print(\"Final Answer:\", answer)\r\n+#         return str(answer)\r\n+#     except Exception as e:\r\n+#         print(\"Error in rag_chain:\", e)\r\n+#         return \"Error: Could not get response\"\r\n \r\n-if __name__ == \"__main__\":\r\n-    app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n+# if __name__ == \"__main__\":\r\n+#     app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n \r\n \r\n \r\n # import streamlit as st\r\n@@ -177,4 +177,191 @@\n #     # Rerun to show updated chat\r\n #     st.rerun()\r\n \r\n \r\n+\r\n+\r\n+\r\n+\"\"\"===================\"\"\"\r\n+import streamlit as st\r\n+import os\r\n+import time\r\n+from operator import itemgetter\r\n+\r\n+# Langchain/RAG components\r\n+from dotenv import load_dotenv\r\n+from langchain_pinecone import PineconeVectorStore\r\n+from langchain_core.prompts import ChatPromptTemplate\r\n+from langchain_core.runnables import RunnableLambda\r\n+from langchain_groq import ChatGroq\r\n+\r\n+# सुनिश्चित करें कि ये फ़ाइलें/मॉड्यूल उपलब्ध हैं:\r\n+# 1. src/helper.py (जिसमें download_embeddings फ़ंक्शन हो)\r\n+# 2. src/prompt.py (जिसमें system_prompt स्ट्रिंग हो)\r\n+try:\r\n+    from src.helper import download_embeddings\r\n+    from src.prompt import system_prompt\r\n+except ImportError:\r\n+    st.error(\"Error: `src/helper.py` or `src/prompt.py` module not found.\")\r\n+    st.stop()\r\n+\r\n+\r\n+# 🔒 1. पर्यावरण चर लोड करें\r\n+load_dotenv()\r\n+\r\n+# ⚠️ महत्वपूर्ण: सुनिश्चित करें कि API keys मौजूद हैं\r\n+PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n+GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n+\r\n+if not PINECONE_API_KEY or not GROQ_API_KEY:\r\n+    st.error(\"API Keys missing! Please set PINECONE_API_KEY and GROQ_API_KEY in your .env file.\")\r\n+    st.stop()\r\n+\r\n+# ⚙️ 2. RAG Chain को परिभाषित और इनिशियलाइज़ करें (केवल एक बार)\r\n+@st.cache_resource\r\n+def initialize_rag_chain():\r\n+    \"\"\"RAG Chain को सभी आवश्यक कॉम्पोनेंट्स के साथ इनिशियलाइज़ करता है।\"\"\"\r\n+    \r\n+    with st.spinner(\"Initializing Chatbot components...\"):\r\n+        # Embeddings + Pinecone retriever\r\n+        embeddings = download_embeddings()\r\n+        index_name = \"medical-chatbot\"\r\n+        \r\n+        try:\r\n+            docsearch = PineconeVectorStore.from_existing_index(\r\n+                index_name=index_name,\r\n+                embedding=embeddings\r\n+            )\r\n+            retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+        except Exception as e:\r\n+            st.error(f\"Failed to connect to Pinecone index '{index_name}': {e}\")\r\n+            st.stop()\r\n+\r\n+        # Groq LLM\r\n+        chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+\r\n+        # Prompt template\r\n+        prompt = ChatPromptTemplate.from_messages([\r\n+            (\"system\", system_prompt),\r\n+            (\"human\", \"{input}\"),\r\n+        ])\r\n+\r\n+        # Format docs function\r\n+        def format_docs(docs):\r\n+            return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+\r\n+        # Context branch\r\n+        context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+\r\n+        # RAG chain\r\n+        rag_chain = (\r\n+            {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+            | prompt\r\n+            | chatModel\r\n+        )\r\n+        \r\n+        return rag_chain\r\n+\r\n+# RAG Chain को लोड करें\r\n+rag_chain = initialize_rag_chain()\r\n+\r\n+\r\n+# 🎨 3. Streamlit UI (आपके पिछले डिज़ाइन पर आधारित)\r\n+st.set_page_config(page_title=\"Streamlit Medical Chatbot\", layout=\"centered\")\r\n+\r\n+# कस्टम CSS स्टाइलिंग (आपके पिछले उत्तर से कॉपी किया गया)\r\n+st.markdown(\r\n+    \"\"\"\r\n+    <style>\r\n+    .stApp { background-color: #f0f2f6; }\r\n+    .main .block-container { max-width: 700px; padding-top: 1rem; padding-bottom: 5rem; }\r\n+    .chat-header {\r\n+        background-color: #007bff; color: white; padding: 15px; border-radius: 10px 10px 0 0; \r\n+        display: flex; align-items: center; margin-top: -20px; margin-left: -20px; margin-right: -20px;\r\n+    }\r\n+    /* User Message (Blue) */\r\n+    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n+        background-color: #007bff; color: white; border-radius: 18px 18px 5px 18px; \r\n+        padding: 10px 15px; margin-left: 30%; text-align: right;\r\n+    }\r\n+    /* Assistant Message (White) */\r\n+    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n+        background-color: #ffffff; color: #333; border: 1px solid #ddd;\r\n+        border-radius: 18px 18px 18px 5px; padding: 10px 15px; margin-right: 30%; text-align: left;\r\n+    }\r\n+    .stChatInputContainer {\r\n+        position: fixed; bottom: 0; width: 100%; max-width: 700px; \r\n+        background-color: #f0f2f6; padding: 10px 0; box-shadow: 0 -2px 10px rgba(0,0,0,0.1); z-index: 10; \r\n+    }\r\n+    .timestamp { font-size: 0.75rem; color: #888; margin-top: 5px; display: block; }\r\n+    </style>\r\n+    \"\"\",\r\n+    unsafe_allow_html=True\r\n+)\r\n+\r\n+# Chat Header\r\n+st.markdown(\r\n+    \"\"\"\r\n+    <div class=\"chat-header\">\r\n+        <img src=\"https://i.imgur.com/xT8Z3yW.png\" style=\"width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;\">\r\n+        <div>\r\n+            <strong>Medical Chatbot</strong><br>\r\n+            <small>Ask me anything!</small>\r\n+        </div>\r\n+    </div>\r\n+    \"\"\", \r\n+    unsafe_allow_html=True\r\n+)\r\n+\r\n+# 📜 4. चैट इतिहास (Session State)\r\n+if \"messages\" not in st.session_state:\r\n+    st.session_state.messages = [\r\n+        {\"role\": \"assistant\", \"content\": \"Hello! I am a Medical RAG Chatbot, ready to answer your health-related queries.\", \"time\": time.strftime(\"%I:%M %p\")}\r\n+    ]\r\n+\r\n+# 💬 पुराने संदेशों को प्रदर्शित करें\r\n+for message in st.session_state.messages:\r\n+    with st.chat_message(message[\"role\"]):\r\n+        st.markdown(message[\"content\"])\r\n+        st.markdown(f'<span class=\"timestamp\">{message[\"time\"]}</span>', unsafe_allow_html=True)\r\n+\r\n+\r\n+# ⌨️ 5. नया उपयोगकर्ता इनपुट हैंडल करें\r\n+if prompt := st.chat_input(\"Type your message...\"):\r\n+    current_time = time.strftime(\"%I:%M %p\")\r\n+    \r\n+    # उपयोगकर्ता का संदेश इतिहास में जोड़ें\r\n+    user_message = {\"role\": \"user\", \"content\": prompt, \"time\": current_time}\r\n+    st.session_state.messages.append(user_message)\r\n+    \r\n+    # उपयोगकर्ता का संदेश चैट में तुरंत दिखाएँ\r\n+    with st.chat_message(\"user\"):\r\n+        st.markdown(prompt)\r\n+        st.markdown(f'<span class=\"timestamp\">{current_time}</span>', unsafe_allow_html=True)\r\n+\r\n+    # असिस्टेंट का जवाब प्राप्त करें\r\n+    with st.chat_message(\"assistant\"):\r\n+        with st.spinner(\"Processing your query...\"):\r\n+            try:\r\n+                # Flask code में rag_chain.invoke\r\n+                response = rag_chain.invoke({\"input\": prompt})\r\n+\r\n+                # AIMessage object → .content\r\n+                if hasattr(response, \"content\"):\r\n+                    answer = response.content\r\n+                else:\r\n+                    answer = str(response)\r\n+\r\n+            except Exception as e:\r\n+                answer = f\"Error: Could not get response from Groq/RAG chain. Details: {e}\"\r\n+                st.error(answer)\r\n+\r\n+        # जवाब प्रदर्शित करें\r\n+        st.markdown(answer)\r\n+        assistant_time = time.strftime(\"%I:%M %p\")\r\n+        st.markdown(f'<span class=\"timestamp\">{assistant_time}</span>', unsafe_allow_html=True)\r\n+\r\n+    # असिस्टेंट का संदेश इतिहास में जोड़ें\r\n+    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"time\": assistant_time})\r\n+    \r\n+    # नए संदेश के साथ UI को री-रन करें\r\n+    st.rerun()\r\n"
                },
                {
                    "date": 1765647912832,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -180,9 +180,8 @@\n \r\n \r\n \r\n \r\n-\"\"\"===================\"\"\"\r\n import streamlit as st\r\n import os\r\n import time\r\n from operator import itemgetter\r\n@@ -194,36 +193,40 @@\n from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n \r\n # सुनिश्चित करें कि ये फ़ाइलें/मॉड्यूल उपलब्ध हैं:\r\n-# 1. src/helper.py (जिसमें download_embeddings फ़ंक्शन हो)\r\n-# 2. src/prompt.py (जिसमें system_prompt स्ट्रिंग हो)\r\n try:\r\n     from src.helper import download_embeddings\r\n     from src.prompt import system_prompt\r\n except ImportError:\r\n-    st.error(\"Error: `src/helper.py` or `src/prompt.py` module not found.\")\r\n-    st.stop()\r\n+    # यदि आप RAG functionality को छोड़ना चाहते हैं, तो इन पंक्तियों को हटा दें \r\n+    # और `initialize_rag_chain` फ़ंक्शन को सरल कर दें।\r\n+    st.error(\"Error: `src/helper.py` or `src/prompt.py` module not found. Skipping RAG setup.\")\r\n+    # RAG chain को एक डमी फंक्शन से बदलें ताकि कोड चल सके\r\n+    def initialize_rag_chain():\r\n+        return lambda x: f\"DUMMY RESPONSE: {x['input']}\"\r\n+    \r\n+    # यदि आप इसे production में चला रहे हैं, तो RAG setup को ठीक करें।\r\n+    # अन्यथा, हम जारी रखने के लिए dummy RAG chain का उपयोग करेंगे।\r\n+    if not os.path.exists(\"src/helper.py\") or not os.path.exists(\"src/prompt.py\"):\r\n+        system_prompt = \"You are a helpful assistant.\"\r\n \r\n-\r\n-# 🔒 1. पर्यावरण चर लोड करें\r\n load_dotenv()\r\n-\r\n-# ⚠️ महत्वपूर्ण: सुनिश्चित करें कि API keys मौजूद हैं\r\n PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n \r\n if not PINECONE_API_KEY or not GROQ_API_KEY:\r\n-    st.error(\"API Keys missing! Please set PINECONE_API_KEY and GROQ_API_KEY in your .env file.\")\r\n-    st.stop()\r\n-\r\n-# ⚙️ 2. RAG Chain को परिभाषित और इनिशियलाइज़ करें (केवल एक बार)\r\n+    # यदि keys missing हैं, तो RAG chain को डमी पर स्विच करें\r\n+    def initialize_rag_chain():\r\n+        st.warning(\"API Keys missing. Using Dummy Chat Response.\")\r\n+        return lambda x: f\"Dummy RAG Response: Please set PINECONE_API_KEY and GROQ_API_KEY. You asked: {x['input']}\"\r\n+    \r\n+# ⚙️ RAG Chain Initialization (Unchanged, uses st.cache_resource)\r\n @st.cache_resource\r\n-def initialize_rag_chain():\r\n+def initialize_rag_chain_full():\r\n     \"\"\"RAG Chain को सभी आवश्यक कॉम्पोनेंट्स के साथ इनिशियलाइज़ करता है।\"\"\"\r\n-    \r\n+    # ... (Your previous RAG initialization logic here) ...\r\n     with st.spinner(\"Initializing Chatbot components...\"):\r\n-        # Embeddings + Pinecone retriever\r\n         embeddings = download_embeddings()\r\n         index_name = \"medical-chatbot\"\r\n         \r\n         try:\r\n@@ -235,88 +238,130 @@\n         except Exception as e:\r\n             st.error(f\"Failed to connect to Pinecone index '{index_name}': {e}\")\r\n             st.stop()\r\n \r\n-        # Groq LLM\r\n         chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n-\r\n-        # Prompt template\r\n         prompt = ChatPromptTemplate.from_messages([\r\n             (\"system\", system_prompt),\r\n             (\"human\", \"{input}\"),\r\n         ])\r\n-\r\n-        # Format docs function\r\n         def format_docs(docs):\r\n             return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n-\r\n-        # Context branch\r\n         context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-\r\n-        # RAG chain\r\n         rag_chain = (\r\n             {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n             | prompt\r\n             | chatModel\r\n         )\r\n-        \r\n         return rag_chain\r\n \r\n-# RAG Chain को लोड करें\r\n-rag_chain = initialize_rag_chain()\r\n+# API keys की उपलब्धता के आधार पर फ़ंक्शन कॉल करें\r\n+if PINECONE_API_KEY and GROQ_API_KEY and os.path.exists(\"src/helper.py\") and os.path.exists(\"src/prompt.py\"):\r\n+    rag_chain = initialize_rag_chain_full()\r\n+else:\r\n+    rag_chain = initialize_rag_chain() # डमी फ़ंक्शन\r\n \r\n-\r\n-# 🎨 3. Streamlit UI (आपके पिछले डिज़ाइन पर आधारित)\r\n+# 🎨 3. Streamlit UI (Updated CSS for fixed header and border)\r\n st.set_page_config(page_title=\"Streamlit Medical Chatbot\", layout=\"centered\")\r\n \r\n-# कस्टम CSS स्टाइलिंग (आपके पिछले उत्तर से कॉपी किया गया)\r\n+# ⚠️ CSS CHANGES START HERE ⚠️\r\n st.markdown(\r\n     \"\"\"\r\n     <style>\r\n+    /* 1. मुख्य Streamlit ऐप का बैकग्राउंड */\r\n     .stApp { background-color: #f0f2f6; }\r\n-    .main .block-container { max-width: 700px; padding-top: 1rem; padding-bottom: 5rem; }\r\n+\r\n+    /* 2. मुख्य कंटेनर (Chatbox Border) */\r\n+    .main .block-container { \r\n+        max-width: 700px;\r\n+        /* पूरे चैटबॉक्स पर बॉर्डर बनाने के लिए एक कंटेनर */\r\n+        background-color: white; \r\n+        border: 1px solid #ddd; \r\n+        border-radius: 10px; \r\n+        box-shadow: 0 0 10px rgba(0,0,0,0.1);\r\n+        \r\n+        /* टॉप और बॉटम पैडिंग को समायोजित करें ताकि फिक्स्ड हेडर और इनपुट बार के लिए जगह बन सके */\r\n+        padding-top: 60px; /* फिक्स्ड हेडर की ऊंचाई से थोड़ा अधिक */\r\n+        padding-bottom: 70px; /* फिक्स्ड इनपुट बार की ऊंचाई से थोड़ा अधिक */\r\n+        \r\n+        position: relative; /* हेडर को कंटेनर के शीर्ष पर फिक्स करने के लिए */\r\n+        margin-top: 20px; /* कंटेनर को ऊपर से थोड़ा नीचे लाएं */\r\n+        margin-bottom: 20px; /* कंटेनर को नीचे से थोड़ा ऊपर लाएं */\r\n+    }\r\n+\r\n+    /* 3. चैट हेडर स्टाइल (FIXED/निश्चित) */\r\n     .chat-header {\r\n-        background-color: #007bff; color: white; padding: 15px; border-radius: 10px 10px 0 0; \r\n-        display: flex; align-items: center; margin-top: -20px; margin-left: -20px; margin-right: -20px;\r\n+        background-color: #007bff; color: white; padding: 15px; \r\n+        display: flex; align-items: center;\r\n+        \r\n+        /* फिक्स्ड हेडर के लिए मुख्य CSS */\r\n+        position: fixed; \r\n+        top: 20px; /* .main .block-container के top margin के बराबर */\r\n+        width: calc(min(700px, 100vw - 40px)); /* चौड़ाई को block-container के max-width और padding के अनुरूप करें */\r\n+        max-width: 700px;\r\n+        z-index: 11; /* यह सुनिश्चित करने के लिए कि यह बाकी सब के ऊपर रहे */\r\n+        border-radius: 10px 10px 0 0;\r\n     }\r\n-    /* User Message (Blue) */\r\n+    \r\n+    /* 4. चैट इनपुट बार फिक्स (नीचे चिपका हुआ) */\r\n+    .stChatInputContainer {\r\n+        position: fixed; \r\n+        bottom: 20px; /* .main .block-container के bottom margin के बराबर */\r\n+        /* यह सुनिश्चित करने के लिए कि यह ठीक कंटेनर के अंदर फिट हो */\r\n+        width: calc(min(700px, 100vw - 40px)); \r\n+        max-width: 700px; \r\n+        background-color: white; /* कंटेनर के अंदर होने के कारण, कंटेनर का रंग उपयोग करें */\r\n+        padding: 10px 0;\r\n+        z-index: 10; \r\n+        border-radius: 0 0 10px 10px;\r\n+    }\r\n+\r\n+    /* 5. संदेश बबल और टाइमस्टैम्प स्टाइल (Unchanged) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n         background-color: #007bff; color: white; border-radius: 18px 18px 5px 18px; \r\n         padding: 10px 15px; margin-left: 30%; text-align: right;\r\n     }\r\n-    /* Assistant Message (White) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n         background-color: #ffffff; color: #333; border: 1px solid #ddd;\r\n         border-radius: 18px 18px 18px 5px; padding: 10px 15px; margin-right: 30%; text-align: left;\r\n     }\r\n-    .stChatInputContainer {\r\n-        position: fixed; bottom: 0; width: 100%; max-width: 700px; \r\n-        background-color: #f0f2f6; padding: 10px 0; box-shadow: 0 -2px 10px rgba(0,0,0,0.1); z-index: 10; \r\n+    .timestamp { font-size: 0.75rem; color: #888; margin-top: 5px; display: block; }\r\n+\r\n+    /* **बहुत महत्वपूर्ण:** चैट मैसेज कंटेनर को फिक्स्ड हेडर के नीचे से शुरू करें */\r\n+    .main div[data-testid=\"stVerticalBlock\"] {\r\n+        padding-top: 50px; \r\n     }\r\n-    .timestamp { font-size: 0.75rem; color: #888; margin-top: 5px; display: block; }\r\n     </style>\r\n     \"\"\",\r\n     unsafe_allow_html=True\r\n )\r\n+# ⚠️ CSS CHANGES END HERE ⚠️\r\n \r\n-# Chat Header\r\n-st.markdown(\r\n-    \"\"\"\r\n-    <div class=\"chat-header\">\r\n-        <img src=\"https://i.imgur.com/xT8Z3yW.png\" style=\"width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;\">\r\n-        <div>\r\n-            <strong>Medical Chatbot</strong><br>\r\n-            <small>Ask me anything!</small>\r\n+\r\n+# 🕒 टाइमस्टैम्प फंक्शन\r\n+def get_current_time():\r\n+    return time.strftime(\"%I:%M %p\")\r\n+\r\n+# Chat Header (HTML) - `st.empty` का उपयोग करके फिक्स्ड हेडर के लिए जगह बनाएं\r\n+header_placeholder = st.empty()\r\n+with header_placeholder.container():\r\n+    st.markdown(\r\n+        \"\"\"\r\n+        <div class=\"chat-header\">\r\n+            <img src=\"https://i.imgur.com/xT8Z3yW.png\" style=\"width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;\">\r\n+            <div>\r\n+                <strong>Medical Chatbot</strong><br>\r\n+                <small>Ask me anything!</small>\r\n+            </div>\r\n         </div>\r\n-    </div>\r\n-    \"\"\", \r\n-    unsafe_allow_html=True\r\n-)\r\n+        \"\"\", \r\n+        unsafe_allow_html=True\r\n+    )\r\n \r\n-# 📜 4. चैट इतिहास (Session State)\r\n+# 📜 4. चैट इतिहास (Session State) - (Unchanged)\r\n if \"messages\" not in st.session_state:\r\n     st.session_state.messages = [\r\n-        {\"role\": \"assistant\", \"content\": \"Hello! I am a Medical RAG Chatbot, ready to answer your health-related queries.\", \"time\": time.strftime(\"%I:%M %p\")}\r\n+        {\"role\": \"assistant\", \"content\": \"Hello! I am a Medical RAG Chatbot, ready to answer your health-related queries.\", \"time\": get_current_time()}\r\n     ]\r\n \r\n # 💬 पुराने संदेशों को प्रदर्शित करें\r\n for message in st.session_state.messages:\r\n@@ -324,29 +369,23 @@\n         st.markdown(message[\"content\"])\r\n         st.markdown(f'<span class=\"timestamp\">{message[\"time\"]}</span>', unsafe_allow_html=True)\r\n \r\n \r\n-# ⌨️ 5. नया उपयोगकर्ता इनपुट हैंडल करें\r\n+# ⌨️ 5. नया उपयोगकर्ता इनपुट हैंडल करें - (Unchanged)\r\n if prompt := st.chat_input(\"Type your message...\"):\r\n-    current_time = time.strftime(\"%I:%M %p\")\r\n+    current_time = get_current_time()\r\n     \r\n-    # उपयोगकर्ता का संदेश इतिहास में जोड़ें\r\n     user_message = {\"role\": \"user\", \"content\": prompt, \"time\": current_time}\r\n     st.session_state.messages.append(user_message)\r\n     \r\n-    # उपयोगकर्ता का संदेश चैट में तुरंत दिखाएँ\r\n     with st.chat_message(\"user\"):\r\n         st.markdown(prompt)\r\n         st.markdown(f'<span class=\"timestamp\">{current_time}</span>', unsafe_allow_html=True)\r\n \r\n-    # असिस्टेंट का जवाब प्राप्त करें\r\n     with st.chat_message(\"assistant\"):\r\n         with st.spinner(\"Processing your query...\"):\r\n             try:\r\n-                # Flask code में rag_chain.invoke\r\n                 response = rag_chain.invoke({\"input\": prompt})\r\n-\r\n-                # AIMessage object → .content\r\n                 if hasattr(response, \"content\"):\r\n                     answer = response.content\r\n                 else:\r\n                     answer = str(response)\r\n@@ -354,14 +393,11 @@\n             except Exception as e:\r\n                 answer = f\"Error: Could not get response from Groq/RAG chain. Details: {e}\"\r\n                 st.error(answer)\r\n \r\n-        # जवाब प्रदर्शित करें\r\n         st.markdown(answer)\r\n-        assistant_time = time.strftime(\"%I:%M %p\")\r\n+        assistant_time = get_current_time()\r\n         st.markdown(f'<span class=\"timestamp\">{assistant_time}</span>', unsafe_allow_html=True)\r\n \r\n-    # असिस्टेंट का संदेश इतिहास में जोड़ें\r\n     st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"time\": assistant_time})\r\n     \r\n-    # नए संदेश के साथ UI को री-रन करें\r\n-    st.rerun()\r\n+    st.rerun()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765648508204,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,51 +185,40 @@\n import os\r\n import time\r\n from operator import itemgetter\r\n \r\n-# Langchain/RAG components\r\n+# Langchain/RAG components (Import statements remain the same)\r\n from dotenv import load_dotenv\r\n from langchain_pinecone import PineconeVectorStore\r\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_core.runnables import RunnableLambda\r\n from langchain_groq import ChatGroq\r\n \r\n-# सुनिश्चित करें कि ये फ़ाइलें/मॉड्यूल उपलब्ध हैं:\r\n+# ⚠️ सुनिश्चित करें कि src/helper.py और src/prompt.py उपलब्ध हैं\r\n try:\r\n     from src.helper import download_embeddings\r\n     from src.prompt import system_prompt\r\n except ImportError:\r\n-    # यदि आप RAG functionality को छोड़ना चाहते हैं, तो इन पंक्तियों को हटा दें \r\n-    # और `initialize_rag_chain` फ़ंक्शन को सरल कर दें।\r\n-    st.error(\"Error: `src/helper.py` or `src/prompt.py` module not found. Skipping RAG setup.\")\r\n-    # RAG chain को एक डमी फंक्शन से बदलें ताकि कोड चल सके\r\n+    st.warning(\"RAG files (src/helper.py, src/prompt.py) not found. Using Dummy Chat Response.\")\r\n+    system_prompt = \"You are a helpful assistant.\"\r\n+    \r\n     def initialize_rag_chain():\r\n-        return lambda x: f\"DUMMY RESPONSE: {x['input']}\"\r\n+        return lambda x: f\"Dummy RAG Response: Please set up your RAG chain correctly. You asked: {x['input']}\"\r\n     \r\n-    # यदि आप इसे production में चला रहे हैं, तो RAG setup को ठीक करें।\r\n-    # अन्यथा, हम जारी रखने के लिए dummy RAG chain का उपयोग करेंगे।\r\n-    if not os.path.exists(\"src/helper.py\") or not os.path.exists(\"src/prompt.py\"):\r\n-        system_prompt = \"You are a helpful assistant.\"\r\n+    initialize_rag_chain_full = initialize_rag_chain # Dummy assignment\r\n \r\n load_dotenv()\r\n PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n \r\n-if not PINECONE_API_KEY or not GROQ_API_KEY:\r\n-    # यदि keys missing हैं, तो RAG chain को डमी पर स्विच करें\r\n-    def initialize_rag_chain():\r\n-        st.warning(\"API Keys missing. Using Dummy Chat Response.\")\r\n-        return lambda x: f\"Dummy RAG Response: Please set PINECONE_API_KEY and GROQ_API_KEY. You asked: {x['input']}\"\r\n-    \r\n-# ⚙️ RAG Chain Initialization (Unchanged, uses st.cache_resource)\r\n+# RAG Chain Initialization (Unchanged - uses st.cache_resource)\r\n @st.cache_resource\r\n def initialize_rag_chain_full():\r\n-    \"\"\"RAG Chain को सभी आवश्यक कॉम्पोनेंट्स के साथ इनिशियलाइज़ करता है।\"\"\"\r\n     # ... (Your previous RAG initialization logic here) ...\r\n-    with st.spinner(\"Initializing Chatbot components...\"):\r\n+    # This section remains unchanged, using your Pinecone/Groq setup\r\n+    with st.spinner(\"Initializing RAG Chatbot...\"):\r\n         embeddings = download_embeddings()\r\n         index_name = \"medical-chatbot\"\r\n-        \r\n         try:\r\n             docsearch = PineconeVectorStore.from_existing_index(\r\n                 index_name=index_name,\r\n                 embedding=embeddings\r\n@@ -237,9 +226,8 @@\n             retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n         except Exception as e:\r\n             st.error(f\"Failed to connect to Pinecone index '{index_name}': {e}\")\r\n             st.stop()\r\n-\r\n         chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n         prompt = ChatPromptTemplate.from_messages([\r\n             (\"system\", system_prompt),\r\n             (\"human\", \"{input}\"),\r\n@@ -254,114 +242,149 @@\n         )\r\n         return rag_chain\r\n \r\n # API keys की उपलब्धता के आधार पर फ़ंक्शन कॉल करें\r\n-if PINECONE_API_KEY and GROQ_API_KEY and os.path.exists(\"src/helper.py\") and os.path.exists(\"src/prompt.py\"):\r\n+if PINECONE_API_KEY and GROQ_API_KEY and 'initialize_rag_chain_full' in locals():\r\n     rag_chain = initialize_rag_chain_full()\r\n else:\r\n-    rag_chain = initialize_rag_chain() # डमी फ़ंक्शन\r\n+    rag_chain = initialize_rag_chain()\r\n \r\n-# 🎨 3. Streamlit UI (Updated CSS for fixed header and border)\r\n-st.set_page_config(page_title=\"Streamlit Medical Chatbot\", layout=\"centered\")\r\n \r\n-# ⚠️ CSS CHANGES START HERE ⚠️\r\n+# 🎨 3. Streamlit UI and Styling (MATCHING THE IMAGE)\r\n+st.set_page_config(page_title=\"Medical Chatbot\", layout=\"centered\")\r\n+\r\n+# 🖼️ CUSTOM CSS STYLING\r\n st.markdown(\r\n     \"\"\"\r\n     <style>\r\n-    /* 1. मुख्य Streamlit ऐप का बैकग्राउंड */\r\n-    .stApp { background-color: #f0f2f6; }\r\n+    /* 1. मुख्य Streamlit ऐप का बैकग्राउंड (हल्का ग्रे/सफेद ताकि बॉर्डर बॉक्स दिख सके) */\r\n+    .stApp { \r\n+        background-color: #F8F8F8; \r\n+    }\r\n \r\n-    /* 2. मुख्य कंटेनर (Chatbox Border) */\r\n+    /* 2. मुख्य कंटेनर (The Chatbox Border/Box) */\r\n     .main .block-container { \r\n-        max-width: 700px;\r\n-        /* पूरे चैटबॉक्स पर बॉर्डर बनाने के लिए एक कंटेनर */\r\n+        max-width: 650px; /* चौड़ाई को थोड़ा छोटा किया गया */\r\n+        height: 80vh; /* ऊंचाई को फिक्स करें ताकि स्क्रॉलिंग स्पष्ट हो */\r\n+        \r\n+        /* Attractive Border Box */\r\n         background-color: white; \r\n-        border: 1px solid #ddd; \r\n+        border: 1px solid #E0E0E0; \r\n         border-radius: 10px; \r\n-        box-shadow: 0 0 10px rgba(0,0,0,0.1);\r\n+        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); /* गहरा Shadow */\r\n         \r\n-        /* टॉप और बॉटम पैडिंग को समायोजित करें ताकि फिक्स्ड हेडर और इनपुट बार के लिए जगह बन सके */\r\n-        padding-top: 60px; /* फिक्स्ड हेडर की ऊंचाई से थोड़ा अधिक */\r\n-        padding-bottom: 70px; /* फिक्स्ड इनपुट बार की ऊंचाई से थोड़ा अधिक */\r\n+        /* फिक्स्ड हेडर और बॉटम इनपुट के लिए पैडिंग */\r\n+        padding-top: 0 !important; /* हेडर को कंटेनर के किनारे से शुरू करने के लिए */\r\n+        padding-bottom: 0 !important;\r\n+        padding-left: 0 !important;\r\n+        padding-right: 0 !important;\r\n         \r\n-        position: relative; /* हेडर को कंटेनर के शीर्ष पर फिक्स करने के लिए */\r\n-        margin-top: 20px; /* कंटेनर को ऊपर से थोड़ा नीचे लाएं */\r\n-        margin-bottom: 20px; /* कंटेनर को नीचे से थोड़ा ऊपर लाएं */\r\n+        position: relative; \r\n+        margin-top: 5vh; \r\n+        margin-bottom: 5vh;\r\n+        overflow: hidden; /* हेडर और इनपुट को बॉक्स के अंदर ही रखें */\r\n     }\r\n-\r\n-    /* 3. चैट हेडर स्टाइल (FIXED/निश्चित) */\r\n+    \r\n+    /* 3. चैट हेडर स्टाइल (FIXED/निश्चित - IMAGE COLOR) */\r\n     .chat-header {\r\n-        background-color: #007bff; color: white; padding: 15px; \r\n-        display: flex; align-items: center;\r\n+        background-color: #1E90FF; /* Bright Blue, Matching Image */\r\n+        color: white; \r\n+        padding: 15px; \r\n+        display: flex; \r\n+        align-items: center;\r\n         \r\n-        /* फिक्स्ड हेडर के लिए मुख्य CSS */\r\n-        position: fixed; \r\n-        top: 20px; /* .main .block-container के top margin के बराबर */\r\n-        width: calc(min(700px, 100vw - 40px)); /* चौड़ाई को block-container के max-width और padding के अनुरूप करें */\r\n-        max-width: 700px;\r\n-        z-index: 11; /* यह सुनिश्चित करने के लिए कि यह बाकी सब के ऊपर रहे */\r\n+        position: sticky; /* यह सुनिश्चित करने के लिए कि यह container के भीतर fixed रहे */\r\n+        top: 0; \r\n+        width: 100%;\r\n+        z-index: 11;\r\n         border-radius: 10px 10px 0 0;\r\n+        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\r\n     }\r\n     \r\n-    /* 4. चैट इनपुट बार फिक्स (नीचे चिपका हुआ) */\r\n+    /* 4. USER Message (Right Aligned, Blue Background - Matching Image) */\r\n+    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n+        background-color: #1E90FF; /* Bright Blue */\r\n+        color: white;\r\n+        border-radius: 15px 15px 0 15px; /* Corner style matching image */\r\n+        padding: 10px 15px; \r\n+        margin-left: 30%; \r\n+        text-align: right;\r\n+        margin-right: 15px; /* Container padding */\r\n+        margin-top: 10px;\r\n+    }\r\n+\r\n+    /* 5. ASSISTANT Message (Left Aligned, White Background - Matching Image) */\r\n+    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n+        background-color: #FFFFFF; /* White */\r\n+        color: #333;\r\n+        border: 1px solid #E0E0E0; /* Subtle border */\r\n+        border-radius: 15px 15px 15px 0; /* Corner style matching image */\r\n+        padding: 10px 15px; \r\n+        margin-right: 30%; \r\n+        text-align: left;\r\n+        margin-left: 15px; /* Container padding */\r\n+        margin-top: 10px;\r\n+        box-shadow: 0 1px 3px rgba(0,0,0,0.05); /* Soft shadow */\r\n+    }\r\n+\r\n+    /* 6. चैट इनपुट बार (FIXED/निश्चित) */\r\n     .stChatInputContainer {\r\n-        position: fixed; \r\n-        bottom: 20px; /* .main .block-container के bottom margin के बराबर */\r\n-        /* यह सुनिश्चित करने के लिए कि यह ठीक कंटेनर के अंदर फिट हो */\r\n-        width: calc(min(700px, 100vw - 40px)); \r\n-        max-width: 700px; \r\n-        background-color: white; /* कंटेनर के अंदर होने के कारण, कंटेनर का रंग उपयोग करें */\r\n-        padding: 10px 0;\r\n+        position: sticky; /* Sticky inside the fixed height container */\r\n+        bottom: 0; \r\n+        width: 100%; \r\n+        background-color: white; \r\n+        padding: 10px 15px; /* Padding for input bar */\r\n+        box-shadow: 0 -2px 5px rgba(0,0,0,0.05);\r\n         z-index: 10; \r\n-        border-radius: 0 0 10px 10px;\r\n     }\r\n-\r\n-    /* 5. संदेश बबल और टाइमस्टैम्प स्टाइल (Unchanged) */\r\n-    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n-        background-color: #007bff; color: white; border-radius: 18px 18px 5px 18px; \r\n-        padding: 10px 15px; margin-left: 30%; text-align: right;\r\n+    \r\n+    /* 7. टाइमस्टैम्प (Smaller and lighter) */\r\n+    .timestamp { \r\n+        font-size: 0.7rem; \r\n+        color: rgba(255, 255, 255, 0.7); /* White for blue bubbles */\r\n+        margin-top: 5px; \r\n+        display: block; \r\n+        line-height: 1;\r\n     }\r\n-    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n-        background-color: #ffffff; color: #333; border: 1px solid #ddd;\r\n-        border-radius: 18px 18px 18px 5px; padding: 10px 15px; margin-right: 30%; text-align: left;\r\n+    /* Assistant timestamp color */\r\n+    .stChatMessage:has(> div > [data-testid=\"stImage\"]):last-child .timestamp {\r\n+        color: #999; /* Grey for white bubbles */\r\n     }\r\n-    .timestamp { font-size: 0.75rem; color: #888; margin-top: 5px; display: block; }\r\n \r\n-    /* **बहुत महत्वपूर्ण:** चैट मैसेज कंटेनर को फिक्स्ड हेडर के नीचे से शुरू करें */\r\n+    /* 8. Scrollable Chat Content Area */\r\n     .main div[data-testid=\"stVerticalBlock\"] {\r\n-        padding-top: 50px; \r\n+        height: calc(80vh - 120px); /* Total container height minus header and input bar height */\r\n+        overflow-y: auto; /* सिर्फ यही हिस्सा स्क्रॉल होगा */\r\n+        padding-top: 15px; /* Top padding for the first message */\r\n     }\r\n     </style>\r\n     \"\"\",\r\n     unsafe_allow_html=True\r\n )\r\n-# ⚠️ CSS CHANGES END HERE ⚠️\r\n \r\n \r\n-# 🕒 टाइमस्टैम्प फंक्शन\r\n+# 🕒 टाइमस्टैम्प फंक्शन (Unchanged)\r\n def get_current_time():\r\n     return time.strftime(\"%I:%M %p\")\r\n \r\n-# Chat Header (HTML) - `st.empty` का उपयोग करके फिक्स्ड हेडर के लिए जगह बनाएं\r\n-header_placeholder = st.empty()\r\n-with header_placeholder.container():\r\n-    st.markdown(\r\n-        \"\"\"\r\n-        <div class=\"chat-header\">\r\n-            <img src=\"https://i.imgur.com/xT8Z3yW.png\" style=\"width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;\">\r\n-            <div>\r\n-                <strong>Medical Chatbot</strong><br>\r\n-                <small>Ask me anything!</small>\r\n-            </div>\r\n+# Chat Header (HTML) - Fixed Header के लिए st.empty की आवश्यकता नहीं है क्योंकि हमने `position: sticky` का उपयोग किया है।\r\n+st.markdown(\r\n+    \"\"\"\r\n+    <div class=\"chat-header\">\r\n+        <img src=\"https://i.imgur.com/xT8Z3yW.png\" style=\"width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;\">\r\n+        <div>\r\n+            <strong>Medical Chatbot</strong><br>\r\n+            <small>Ask me anything!</small>\r\n         </div>\r\n-        \"\"\", \r\n-        unsafe_allow_html=True\r\n-    )\r\n+    </div>\r\n+    \"\"\", \r\n+    unsafe_allow_html=True\r\n+)\r\n \r\n # 📜 4. चैट इतिहास (Session State) - (Unchanged)\r\n if \"messages\" not in st.session_state:\r\n     st.session_state.messages = [\r\n-        {\"role\": \"assistant\", \"content\": \"Hello! I am a Medical RAG Chatbot, ready to answer your health-related queries.\", \"time\": get_current_time()}\r\n+        {\"role\": \"assistant\", \"content\": \"Hello. How can I assist you today, particularly in relation to medical topics?\", \"time\": get_current_time()}\r\n     ]\r\n \r\n # 💬 पुराने संदेशों को प्रदर्शित करें\r\n for message in st.session_state.messages:\r\n@@ -378,26 +401,30 @@\n     st.session_state.messages.append(user_message)\r\n     \r\n     with st.chat_message(\"user\"):\r\n         st.markdown(prompt)\r\n+        # User message timestamp (White color defined in CSS)\r\n         st.markdown(f'<span class=\"timestamp\">{current_time}</span>', unsafe_allow_html=True)\r\n \r\n     with st.chat_message(\"assistant\"):\r\n         with st.spinner(\"Processing your query...\"):\r\n             try:\r\n+                # Invoke RAG Chain\r\n                 response = rag_chain.invoke({\"input\": prompt})\r\n                 if hasattr(response, \"content\"):\r\n                     answer = response.content\r\n                 else:\r\n                     answer = str(response)\r\n \r\n             except Exception as e:\r\n-                answer = f\"Error: Could not get response from Groq/RAG chain. Details: {e}\"\r\n+                answer = f\"Error: Could not get response. Details: {e}\"\r\n                 st.error(answer)\r\n \r\n         st.markdown(answer)\r\n         assistant_time = get_current_time()\r\n+        # Assistant message timestamp (Grey color defined in CSS)\r\n         st.markdown(f'<span class=\"timestamp\">{assistant_time}</span>', unsafe_allow_html=True)\r\n \r\n     st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"time\": assistant_time})\r\n     \r\n+    # Rerun to scroll to the newest message\r\n     st.rerun()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765649050856,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,9 +185,9 @@\n import os\r\n import time\r\n from operator import itemgetter\r\n \r\n-# Langchain/RAG components (Import statements remain the same)\r\n+# Langchain/RAG components\r\n from dotenv import load_dotenv\r\n from langchain_pinecone import PineconeVectorStore\r\n from langchain_core.prompts import ChatPromptTemplate\r\n from langchain_core.runnables import RunnableLambda\r\n@@ -198,9 +198,9 @@\n     from src.helper import download_embeddings\r\n     from src.prompt import system_prompt\r\n except ImportError:\r\n     st.warning(\"RAG files (src/helper.py, src/prompt.py) not found. Using Dummy Chat Response.\")\r\n-    system_prompt = \"You are a helpful assistant.\"\r\n+    system_prompt = \"You are a helpful medical assistant.\"\r\n     \r\n     def initialize_rag_chain():\r\n         return lambda x: f\"Dummy RAG Response: Please set up your RAG chain correctly. You asked: {x['input']}\"\r\n     \r\n@@ -209,14 +209,14 @@\n load_dotenv()\r\n PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n \r\n-# RAG Chain Initialization (Unchanged - uses st.cache_resource)\r\n+# ⚙️ RAG Chain Initialization (Unchanged - uses st.cache_resource)\r\n @st.cache_resource\r\n def initialize_rag_chain_full():\r\n-    # ... (Your previous RAG initialization logic here) ...\r\n-    # This section remains unchanged, using your Pinecone/Groq setup\r\n-    with st.spinner(\"Initializing RAG Chatbot...\"):\r\n+    \"\"\"RAG Chain को सभी आवश्यक कॉम्पोनेंट्स के साथ इनिशियलाइज़ करता है।\"\"\"\r\n+    with st.spinner(\"Initializing Medical RAG Chatbot...\"):\r\n+        # ... (Your Pinecone/Groq RAG setup logic remains here) ...\r\n         embeddings = download_embeddings()\r\n         index_name = \"medical-chatbot\"\r\n         try:\r\n             docsearch = PineconeVectorStore.from_existing_index(\r\n@@ -241,190 +241,185 @@\n             | chatModel\r\n         )\r\n         return rag_chain\r\n \r\n-# API keys की उपलब्धता के आधार पर फ़ंक्शन कॉल करें\r\n if PINECONE_API_KEY and GROQ_API_KEY and 'initialize_rag_chain_full' in locals():\r\n     rag_chain = initialize_rag_chain_full()\r\n else:\r\n     rag_chain = initialize_rag_chain()\r\n \r\n \r\n-# 🎨 3. Streamlit UI and Styling (MATCHING THE IMAGE)\r\n-st.set_page_config(page_title=\"Medical Chatbot\", layout=\"centered\")\r\n+# 🎨 3. Streamlit UI and Styling (GEMINI STYLE)\r\n+st.set_page_config(page_title=\"Medical Chatbot (Gemini Style)\", layout=\"wide\") # Layout wide for full screen experience\r\n \r\n-# 🖼️ CUSTOM CSS STYLING\r\n+# 🖼️ CUSTOM CSS STYLING (Gemini Look)\r\n st.markdown(\r\n     \"\"\"\r\n     <style>\r\n-    /* 1. मुख्य Streamlit ऐप का बैकग्राउंड (हल्का ग्रे/सफेद ताकि बॉर्डर बॉक्स दिख सके) */\r\n+    /* 1. मुख्य Streamlit ऐप का बैकग्राउंड (पूरी तरह से सफेद) */\r\n     .stApp { \r\n-        background-color: #F8F8F8; \r\n+        background-color: white; \r\n+        color: #1f1f1f; /* Dark text */\r\n     }\r\n \r\n-    /* 2. मुख्य कंटेनर (The Chatbox Border/Box) */\r\n+    /* 2. मुख्य कंटेनर (चैटबॉक्स - केंद्र में, चौड़ा) */\r\n     .main .block-container { \r\n-        max-width: 650px; /* चौड़ाई को थोड़ा छोटा किया गया */\r\n-        height: 80vh; /* ऊंचाई को फिक्स करें ताकि स्क्रॉलिंग स्पष्ट हो */\r\n-        \r\n-        /* Attractive Border Box */\r\n-        background-color: white; \r\n-        border: 1px solid #E0E0E0; \r\n-        border-radius: 10px; \r\n-        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); /* गहरा Shadow */\r\n-        \r\n-        /* फिक्स्ड हेडर और बॉटम इनपुट के लिए पैडिंग */\r\n-        padding-top: 0 !important; /* हेडर को कंटेनर के किनारे से शुरू करने के लिए */\r\n-        padding-bottom: 0 !important;\r\n-        padding-left: 0 !important;\r\n-        padding-right: 0 !important;\r\n-        \r\n-        position: relative; \r\n-        margin-top: 5vh; \r\n-        margin-bottom: 5vh;\r\n-        overflow: hidden; /* हेडर और इनपुट को बॉक्स के अंदर ही रखें */\r\n+        max-width: 800px; /* थोड़ा चौड़ा चैट एरिया */\r\n+        padding-top: 2rem;\r\n+        padding-bottom: 5rem; /* इनपुट बार के लिए जगह */\r\n     }\r\n     \r\n-    /* 3. चैट हेडर स्टाइल (FIXED/निश्चित - IMAGE COLOR) */\r\n+    /* 3. चैट हेडर/टाइटल (फिक्स्ड नहीं, साधारण टेक्स्ट) */\r\n     .chat-header {\r\n-        background-color: #1E90FF; /* Bright Blue, Matching Image */\r\n-        color: white; \r\n-        padding: 15px; \r\n-        display: flex; \r\n-        align-items: center;\r\n-        \r\n-        position: sticky; /* यह सुनिश्चित करने के लिए कि यह container के भीतर fixed रहे */\r\n-        top: 0; \r\n-        width: 100%;\r\n-        z-index: 11;\r\n-        border-radius: 10px 10px 0 0;\r\n-        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\r\n+        color: #1f1f1f;\r\n+        padding: 10px 0;\r\n+        margin-bottom: 20px;\r\n+        text-align: center;\r\n+        font-size: 2rem;\r\n+        font-weight: 500;\r\n     }\r\n     \r\n-    /* 4. USER Message (Right Aligned, Blue Background - Matching Image) */\r\n+    /* 4. USER Message (Left Aligned, NO BUBBLE/CARD) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n-        background-color: #1E90FF; /* Bright Blue */\r\n-        color: white;\r\n-        border-radius: 15px 15px 0 15px; /* Corner style matching image */\r\n-        padding: 10px 15px; \r\n-        margin-left: 30%; \r\n-        text-align: right;\r\n-        margin-right: 15px; /* Container padding */\r\n-        margin-top: 10px;\r\n+        background-color: transparent; /* कोई बैकग्राउंड नहीं */\r\n+        color: #1f1f1f;\r\n+        border-radius: 0;\r\n+        padding: 0 0 10px 0; /* टॉप पैडिंग कम */\r\n+        margin-left: 0; \r\n+        margin-right: 0; \r\n+        text-align: left;\r\n+        order: 1; /* यह सुनिश्चित करता है कि संदेश चैट की तरह ही दिखाई दें */\r\n+        border: none;\r\n     }\r\n \r\n-    /* 5. ASSISTANT Message (Left Aligned, White Background - Matching Image) */\r\n+    /* 5. ASSISTANT Message (Left Aligned, Card/Bubble Style) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n-        background-color: #FFFFFF; /* White */\r\n-        color: #333;\r\n-        border: 1px solid #E0E0E0; /* Subtle border */\r\n-        border-radius: 15px 15px 15px 0; /* Corner style matching image */\r\n-        padding: 10px 15px; \r\n-        margin-right: 30%; \r\n+        background-color: #F0F4F9; /* Very light grey/blue card background */\r\n+        color: #1f1f1f;\r\n+        border: none;\r\n+        border-radius: 12px;\r\n+        padding: 15px; \r\n+        margin-left: 0; \r\n+        margin-right: 0; \r\n         text-align: left;\r\n-        margin-left: 15px; /* Container padding */\r\n-        margin-top: 10px;\r\n-        box-shadow: 0 1px 3px rgba(0,0,0,0.05); /* Soft shadow */\r\n+        box-shadow: none; /* No shadow for minimalist look */\r\n+        margin-bottom: 20px; /* संदेशों के बीच अधिक स्पेस */\r\n     }\r\n \r\n-    /* 6. चैट इनपुट बार (FIXED/निश्चित) */\r\n+    /* 6. चैट इनपुट बार (FIXED/निश्चित, गोल) */\r\n     .stChatInputContainer {\r\n-        position: sticky; /* Sticky inside the fixed height container */\r\n-        bottom: 0; \r\n+        position: fixed; \r\n+        bottom: 20px; /* नीचे से थोड़ा ऊपर */\r\n+        /* यह सुनिश्चित करने के लिए कि यह ठीक कंटेनर के अंदर फिट हो */\r\n         width: 100%; \r\n+        max-width: 800px; \r\n         background-color: white; \r\n-        padding: 10px 15px; /* Padding for input bar */\r\n-        box-shadow: 0 -2px 5px rgba(0,0,0,0.05);\r\n+        padding: 10px 0;\r\n         z-index: 10; \r\n+        \r\n+        /* इनपुट बॉक्स को गोल करने के लिए */\r\n+        border-radius: 25px;\r\n+        border: 1px solid #ddd;\r\n+        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08); /* Soft shadow */\r\n     }\r\n+    /* Input field inside the container */\r\n+    .stChatInput > div > div > input {\r\n+        border-radius: 25px !important;\r\n+        border: none !important;\r\n+        box-shadow: none !important;\r\n+        padding-left: 20px;\r\n+    }\r\n+\r\n+    /* 7. Avatar (Optional but recommended for Gemini style) */\r\n+    .stChatInput > div > div:last-child {\r\n+        background-color: #4285F4; /* Google Blue */\r\n+        border-radius: 50%;\r\n+        width: 32px;\r\n+        height: 32px;\r\n+        display: flex;\r\n+        align-items: center;\r\n+        justify-content: center;\r\n+    }\r\n     \r\n-    /* 7. टाइमस्टैम्प (Smaller and lighter) */\r\n+    /* 8. Timestamp (Hidden for minimalist Gemini look) */\r\n     .timestamp { \r\n-        font-size: 0.7rem; \r\n-        color: rgba(255, 255, 255, 0.7); /* White for blue bubbles */\r\n-        margin-top: 5px; \r\n-        display: block; \r\n-        line-height: 1;\r\n+        display: none !important;\r\n     }\r\n-    /* Assistant timestamp color */\r\n-    .stChatMessage:has(> div > [data-testid=\"stImage\"]):last-child .timestamp {\r\n-        color: #999; /* Grey for white bubbles */\r\n-    }\r\n-\r\n-    /* 8. Scrollable Chat Content Area */\r\n+    \r\n+    /* 9. Scrollable Chat Content Area */\r\n     .main div[data-testid=\"stVerticalBlock\"] {\r\n-        height: calc(80vh - 120px); /* Total container height minus header and input bar height */\r\n-        overflow-y: auto; /* सिर्फ यही हिस्सा स्क्रॉल होगा */\r\n-        padding-top: 15px; /* Top padding for the first message */\r\n+        max-height: 80vh; \r\n+        overflow-y: auto; \r\n     }\r\n+    \r\n+    /* RAG icon/reload button hide */\r\n+    .stChatMessage button {\r\n+        display: none;\r\n+    }\r\n     </style>\r\n     \"\"\",\r\n     unsafe_allow_html=True\r\n )\r\n \r\n-\r\n-# 🕒 टाइमस्टैम्प फंक्शन (Unchanged)\r\n+# 🕒 टाइमस्टैम्प फंक्शन\r\n def get_current_time():\r\n     return time.strftime(\"%I:%M %p\")\r\n \r\n-# Chat Header (HTML) - Fixed Header के लिए st.empty की आवश्यकता नहीं है क्योंकि हमने `position: sticky` का उपयोग किया है।\r\n-st.markdown(\r\n-    \"\"\"\r\n-    <div class=\"chat-header\">\r\n-        <img src=\"https://i.imgur.com/xT8Z3yW.png\" style=\"width: 40px; height: 40px; border-radius: 50%; margin-right: 10px;\">\r\n-        <div>\r\n-            <strong>Medical Chatbot</strong><br>\r\n-            <small>Ask me anything!</small>\r\n-        </div>\r\n-    </div>\r\n-    \"\"\", \r\n-    unsafe_allow_html=True\r\n-)\r\n+# Chat Header (Gemini Style)\r\n+st.markdown(\"<div class='chat-header'>Medical Chatbot</div>\", unsafe_allow_html=True)\r\n+st.caption(\"I am an AI assistant specialized in medical and health-related topics.\")\r\n \r\n-# 📜 4. चैट इतिहास (Session State) - (Unchanged)\r\n+# 📜 4. चैट इतिहास (Session State)\r\n if \"messages\" not in st.session_state:\r\n     st.session_state.messages = [\r\n-        {\"role\": \"assistant\", \"content\": \"Hello. How can I assist you today, particularly in relation to medical topics?\", \"time\": get_current_time()}\r\n+        {\"role\": \"assistant\", \"content\": \"Hello! I am your Medical RAG Chatbot, ready to assist with your health-related queries.\", \"time\": get_current_time()}\r\n     ]\r\n \r\n # 💬 पुराने संदेशों को प्रदर्शित करें\r\n for message in st.session_state.messages:\r\n-    with st.chat_message(message[\"role\"]):\r\n+    # Gemini Style: User avatar is typically a simple circle, Assistant is often a logo/icon.\r\n+    # User is represented by 'user', Assistant by 'model' (or 'assistant' in Streamlit)\r\n+    avatar = \"🧑‍⚕️\" if message[\"role\"] == \"assistant\" else \"👤\"\r\n+    with st.chat_message(message[\"role\"], avatar=avatar):\r\n         st.markdown(message[\"content\"])\r\n-        st.markdown(f'<span class=\"timestamp\">{message[\"time\"]}</span>', unsafe_allow_html=True)\r\n+        # Timestamp is hidden via CSS, but we keep it in session state\r\n+        # st.markdown(f'<span class=\"timestamp\">{message[\"time\"]}</span>', unsafe_allow_html=True) \r\n \r\n \r\n-# ⌨️ 5. नया उपयोगकर्ता इनपुट हैंडल करें - (Unchanged)\r\n-if prompt := st.chat_input(\"Type your message...\"):\r\n+# ⌨️ 5. नया उपयोगकर्ता इनपुट हैंडल करें\r\n+if prompt := st.chat_input(\"Ask a medical question...\"):\r\n     current_time = get_current_time()\r\n     \r\n+    # 1. User Message\r\n     user_message = {\"role\": \"user\", \"content\": prompt, \"time\": current_time}\r\n     st.session_state.messages.append(user_message)\r\n     \r\n-    with st.chat_message(\"user\"):\r\n+    with st.chat_message(\"user\", avatar=\"👤\"):\r\n         st.markdown(prompt)\r\n-        # User message timestamp (White color defined in CSS)\r\n-        st.markdown(f'<span class=\"timestamp\">{current_time}</span>', unsafe_allow_html=True)\r\n \r\n-    with st.chat_message(\"assistant\"):\r\n-        with st.spinner(\"Processing your query...\"):\r\n+    # 2. Assistant Response\r\n+    with st.chat_message(\"assistant\", avatar=\"🧑‍⚕️\"):\r\n+        with st.spinner(\"Getting medical insights...\"):\r\n             try:\r\n-                # Invoke RAG Chain\r\n                 response = rag_chain.invoke({\"input\": prompt})\r\n                 if hasattr(response, \"content\"):\r\n                     answer = response.content\r\n                 else:\r\n                     answer = str(response)\r\n \r\n             except Exception as e:\r\n-                answer = f\"Error: Could not get response. Details: {e}\"\r\n+                answer = f\"Error: Could not get response from RAG chain. Details: {e}\"\r\n                 st.error(answer)\r\n \r\n-        st.markdown(answer)\r\n-        assistant_time = get_current_time()\r\n-        # Assistant message timestamp (Grey color defined in CSS)\r\n-        st.markdown(f'<span class=\"timestamp\">{assistant_time}</span>', unsafe_allow_html=True)\r\n+        # Stream the response word by word (Gemini-like effect)\r\n+        full_response = \"\"\r\n+        placeholder = st.empty()\r\n+        for chunk in answer.split():\r\n+            full_response += chunk + \" \"\r\n+            placeholder.markdown(full_response + \"▌\") # Typing cursor\r\n+            time.sleep(0.02) # Fast typing speed\r\n+        placeholder.markdown(full_response) # Final response\r\n \r\n-    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"time\": assistant_time})\r\n+    # 3. Add to History\r\n+    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"time\": get_current_time()})\r\n     \r\n-    # Rerun to scroll to the newest message\r\n     st.rerun()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765649970686,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -247,160 +247,222 @@\n else:\r\n     rag_chain = initialize_rag_chain()\r\n \r\n \r\n-# 🎨 3. Streamlit UI and Styling (GEMINI STYLE)\r\n-st.set_page_config(page_title=\"Medical Chatbot (Gemini Style)\", layout=\"wide\") # Layout wide for full screen experience\r\n+# 🎨 3. Streamlit UI and Styling (DARK/MODERN STYLE)\r\n+st.set_page_config(page_title=\"Medical Chatbot (Dark Style)\", layout=\"centered\")\r\n \r\n-# 🖼️ CUSTOM CSS STYLING (Gemini Look)\r\n+# 🖼️ CUSTOM CSS STYLING (Matching your provided CSS)\r\n st.markdown(\r\n     \"\"\"\r\n     <style>\r\n-    /* 1. मुख्य Streamlit ऐप का बैकग्राउंड (पूरी तरह से सफेद) */\r\n+    /* 1. Global Background (Matching your CSS gradient) */\r\n     .stApp { \r\n-        background-color: white; \r\n-        color: #1f1f1f; /* Dark text */\r\n+        background: linear-gradient(to right, #26333d, #323741, #21214e); \r\n+        color: white;\r\n     }\r\n \r\n-    /* 2. मुख्य कंटेनर (चैटबॉक्स - केंद्र में, चौड़ा) */\r\n+    /* 2. Main Container (The Chatbox Border/Box) */\r\n     .main .block-container { \r\n-        max-width: 800px; /* थोड़ा चौड़ा चैट एरिया */\r\n-        padding-top: 2rem;\r\n-        padding-bottom: 5rem; /* इनपुट बार के लिए जगह */\r\n+        max-width: 600px;\r\n+        height: 85vh; \r\n+        \r\n+        /* Dark Card Style */\r\n+        background-color: rgba(0,0,0,0.4) !important; \r\n+        border-radius: 15px !important; \r\n+        box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5); \r\n+        \r\n+        padding-top: 0 !important;\r\n+        padding-bottom: 0 !important;\r\n+        padding-left: 0 !important;\r\n+        padding-right: 0 !important;\r\n+        \r\n+        margin-top: 30px; \r\n+        margin-bottom: 30px;\r\n+        overflow: hidden; \r\n     }\r\n     \r\n-    /* 3. चैट हेडर/टाइटल (फिक्स्ड नहीं, साधारण टेक्स्ट) */\r\n+    /* 3. Chat Header (FIXED) */\r\n     .chat-header {\r\n-        color: #1f1f1f;\r\n-        padding: 10px 0;\r\n-        margin-bottom: 20px;\r\n-        text-align: center;\r\n-        font-size: 2rem;\r\n-        font-weight: 500;\r\n+        background-color: #007bff; /* Primary Blue from HTML */\r\n+        color: white; \r\n+        padding: 15px; \r\n+        display: flex; \r\n+        align-items: center;\r\n+        \r\n+        position: sticky; \r\n+        top: 0; \r\n+        width: 100%;\r\n+        z-index: 11;\r\n+        border-radius: 15px 15px 0 0 !important;\r\n+        border-bottom: 0 !important;\r\n     }\r\n     \r\n-    /* 4. USER Message (Left Aligned, NO BUBBLE/CARD) */\r\n+    /* 4. USER Message (Right Aligned, Green Bubble - #58cc71) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n-        background-color: transparent; /* कोई बैकग्राउंड नहीं */\r\n-        color: #1f1f1f;\r\n-        border-radius: 0;\r\n-        padding: 0 0 10px 0; /* टॉप पैडिंग कम */\r\n-        margin-left: 0; \r\n-        margin-right: 0; \r\n-        text-align: left;\r\n-        order: 1; /* यह सुनिश्चित करता है कि संदेश चैट की तरह ही दिखाई दें */\r\n-        border: none;\r\n+        background-color: #58cc71; /* Green Bubble from your CSS */\r\n+        color: white;\r\n+        border-radius: 25px !important; /* Fully rounded */\r\n+        padding: 10px; \r\n+        margin-left: 30%; \r\n+        text-align: right;\r\n+        margin-right: 15px;\r\n+        margin-top: 10px;\r\n     }\r\n+    /* Hide avatar for user messages */\r\n+    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stImage {\r\n+        display: none;\r\n+    }\r\n \r\n-    /* 5. ASSISTANT Message (Left Aligned, Card/Bubble Style) */\r\n+    /* 5. ASSISTANT Message (Left Aligned, Blue Bubble - #52acff) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n-        background-color: #F0F4F9; /* Very light grey/blue card background */\r\n-        color: #1f1f1f;\r\n-        border: none;\r\n-        border-radius: 12px;\r\n-        padding: 15px; \r\n-        margin-left: 0; \r\n-        margin-right: 0; \r\n+        background-color: #52acff; /* Blue Bubble from your CSS */\r\n+        color: white;\r\n+        border-radius: 25px !important; /* Fully rounded */\r\n+        padding: 10px; \r\n+        margin-right: 30%; \r\n         text-align: left;\r\n-        box-shadow: none; /* No shadow for minimalist look */\r\n-        margin-bottom: 20px; /* संदेशों के बीच अधिक स्पेस */\r\n+        margin-left: 15px;\r\n+        margin-top: 10px;\r\n     }\r\n+    /* Assistant avatar/icon positioning */\r\n+    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child {\r\n+        display: flex;\r\n+        align-items: flex-start;\r\n+    }\r\n+    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stImage {\r\n+         margin-right: 10px; \r\n+         flex-shrink: 0;\r\n+         /* Matching user_img_msg style */\r\n+         height: 40px; \r\n+         width: 40px; \r\n+         border: 1.5px solid #f5f6fa;\r\n+         border-radius: 50%;\r\n+    }\r\n \r\n-    /* 6. चैट इनपुट बार (FIXED/निश्चित, गोल) */\r\n+\r\n+    /* 6. Time and Timestamp Styling */\r\n+    .timestamp { \r\n+        font-size: 10px; \r\n+        color: rgba(255,255,255,0.6); /* Lighter white for dark theme */\r\n+        margin-top: 5px; \r\n+        display: block; \r\n+        line-height: 1;\r\n+        /* Position as per your CSS: outside the bubble, but Streamlit makes this difficult. \r\n+           We'll keep it inside but small and aligned. */\r\n+        text-align: right;\r\n+    }\r\n+    /* Left-aligned time for assistant */\r\n+    .stChatMessage:has(> div > [data-testid=\"stImage\"]):last-child .timestamp {\r\n+        text-align: left;\r\n+    }\r\n+\r\n+    /* 7. Chat Input Bar (FIXED/Footer) */\r\n     .stChatInputContainer {\r\n-        position: fixed; \r\n-        bottom: 20px; /* नीचे से थोड़ा ऊपर */\r\n-        /* यह सुनिश्चित करने के लिए कि यह ठीक कंटेनर के अंदर फिट हो */\r\n+        position: sticky; \r\n+        bottom: 0; \r\n         width: 100%; \r\n-        max-width: 800px; \r\n-        background-color: white; \r\n-        padding: 10px 0;\r\n+        background-color: rgba(0,0,0,0.4) !important; /* Dark background */\r\n+        padding: 10px 15px;\r\n         z-index: 10; \r\n-        \r\n-        /* इनपुट बॉक्स को गोल करने के लिए */\r\n-        border-radius: 25px;\r\n-        border: 1px solid #ddd;\r\n-        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08); /* Soft shadow */\r\n+        border-radius: 0 0 15px 15px !important; /* Matching card-footer */\r\n     }\r\n-    /* Input field inside the container */\r\n+    /* Input field styling (type_msg) */\r\n     .stChatInput > div > div > input {\r\n-        border-radius: 25px !important;\r\n-        border: none !important;\r\n+        background-color: rgba(0,0,0,0.3) !important;\r\n+        border: 0 !important;\r\n+        color: white !important;\r\n+        height: 60px !important;\r\n         box-shadow: none !important;\r\n-        padding-left: 20px;\r\n+        border-radius: 15px 0 0 15px !important;\r\n     }\r\n-\r\n-    /* 7. Avatar (Optional but recommended for Gemini style) */\r\n+    /* Send button styling (send_btn) */\r\n     .stChatInput > div > div:last-child {\r\n-        background-color: #4285F4; /* Google Blue */\r\n-        border-radius: 50%;\r\n-        width: 32px;\r\n-        height: 32px;\r\n+        background-color: rgba(0,0,0,0.3) !important;\r\n+        border-radius: 0 15px 15px 0 !important;\r\n+        border: 0 !important;\r\n+        color: white !important;\r\n+        cursor: pointer;\r\n+    }\r\n+    /* Adjust input and button sizing to fit */\r\n+    .stChatInput > div > div:nth-child(1) {\r\n+        flex: 1;\r\n+    }\r\n+    .stChatInput > div > div:last-child {\r\n+        width: 60px; /* Width similar to input height */\r\n         display: flex;\r\n         align-items: center;\r\n         justify-content: center;\r\n     }\r\n-    \r\n-    /* 8. Timestamp (Hidden for minimalist Gemini look) */\r\n-    .timestamp { \r\n-        display: none !important;\r\n-    }\r\n-    \r\n-    /* 9. Scrollable Chat Content Area */\r\n+\r\n+\r\n+    /* 8. Scrollable Chat Content Area (msg_card_body) */\r\n     .main div[data-testid=\"stVerticalBlock\"] {\r\n-        max-height: 80vh; \r\n+        height: calc(85vh - 140px); /* Total container height minus header and input bar height */\r\n         overflow-y: auto; \r\n+        padding: 10px; /* Matching msg_card_body padding */\r\n+        color: white; /* Chat content color */\r\n     }\r\n-    \r\n-    /* RAG icon/reload button hide */\r\n-    .stChatMessage button {\r\n-        display: none;\r\n-    }\r\n     </style>\r\n     \"\"\",\r\n     unsafe_allow_html=True\r\n )\r\n \r\n+\r\n # 🕒 टाइमस्टैम्प फंक्शन\r\n def get_current_time():\r\n     return time.strftime(\"%I:%M %p\")\r\n \r\n-# Chat Header (Gemini Style)\r\n-st.markdown(\"<div class='chat-header'>Medical Chatbot</div>\", unsafe_allow_html=True)\r\n-st.caption(\"I am an AI assistant specialized in medical and health-related topics.\")\r\n+# Chat Header (HTML) - Fixed Header\r\n+st.markdown(\r\n+    \"\"\"\r\n+    <div class='chat-header'>\r\n+        <img src=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\" class=\"rounded-circle mr-2\" width=\"40\" height=\"40\" style=\"background-color: #f5f6fa; border: 1.5px solid #f5f6fa;\">\r\n+        <div>\r\n+            <strong>Medical Chatbot</strong><br>\r\n+            <small>Ask me anything!</small>\r\n+        </div>\r\n+    </div>\r\n+    \"\"\", \r\n+    unsafe_allow_html=True\r\n+)\r\n \r\n # 📜 4. चैट इतिहास (Session State)\r\n if \"messages\" not in st.session_state:\r\n     st.session_state.messages = [\r\n-        {\"role\": \"assistant\", \"content\": \"Hello! I am your Medical RAG Chatbot, ready to assist with your health-related queries.\", \"time\": get_current_time()}\r\n+        {\"role\": \"assistant\", \"content\": \"Hello. How can I assist you today, particularly in relation to medical topics?\", \"time\": get_current_time()}\r\n     ]\r\n \r\n # 💬 पुराने संदेशों को प्रदर्शित करें\r\n for message in st.session_state.messages:\r\n-    # Gemini Style: User avatar is typically a simple circle, Assistant is often a logo/icon.\r\n-    # User is represented by 'user', Assistant by 'model' (or 'assistant' in Streamlit)\r\n-    avatar = \"🧑‍⚕️\" if message[\"role\"] == \"assistant\" else \"👤\"\r\n+    # Set avatar for assistant (User has none)\r\n+    avatar = \"https://cdn-icons-png.flaticon.com/512/387/387569.png\" if message[\"role\"] == \"assistant\" else None\r\n+    \r\n     with st.chat_message(message[\"role\"], avatar=avatar):\r\n+        # Display message content\r\n         st.markdown(message[\"content\"])\r\n-        # Timestamp is hidden via CSS, but we keep it in session state\r\n-        # st.markdown(f'<span class=\"timestamp\">{message[\"time\"]}</span>', unsafe_allow_html=True) \r\n+        \r\n+        # Display timestamp\r\n+        st.markdown(f'<span class=\"timestamp\">{message[\"time\"]}</span>', unsafe_allow_html=True) \r\n \r\n \r\n # ⌨️ 5. नया उपयोगकर्ता इनपुट हैंडल करें\r\n-if prompt := st.chat_input(\"Ask a medical question...\"):\r\n+if prompt := st.chat_input(\"Type your message...\"):\r\n     current_time = get_current_time()\r\n     \r\n     # 1. User Message\r\n     user_message = {\"role\": \"user\", \"content\": prompt, \"time\": current_time}\r\n     st.session_state.messages.append(user_message)\r\n     \r\n-    with st.chat_message(\"user\", avatar=\"👤\"):\r\n+    with st.chat_message(\"user\"):\r\n         st.markdown(prompt)\r\n+        st.markdown(f'<span class=\"timestamp\">{current_time}</span>', unsafe_allow_html=True)\r\n \r\n     # 2. Assistant Response\r\n-    with st.chat_message(\"assistant\", avatar=\"🧑‍⚕️\"):\r\n-        with st.spinner(\"Getting medical insights...\"):\r\n+    with st.chat_message(\"assistant\", avatar=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\"):\r\n+        with st.spinner(\"Processing your query...\"):\r\n             try:\r\n+                # Invoke RAG Chain\r\n                 response = rag_chain.invoke({\"input\": prompt})\r\n                 if hasattr(response, \"content\"):\r\n                     answer = response.content\r\n                 else:\r\n@@ -409,17 +471,12 @@\n             except Exception as e:\r\n                 answer = f\"Error: Could not get response from RAG chain. Details: {e}\"\r\n                 st.error(answer)\r\n \r\n-        # Stream the response word by word (Gemini-like effect)\r\n-        full_response = \"\"\r\n-        placeholder = st.empty()\r\n-        for chunk in answer.split():\r\n-            full_response += chunk + \" \"\r\n-            placeholder.markdown(full_response + \"▌\") # Typing cursor\r\n-            time.sleep(0.02) # Fast typing speed\r\n-        placeholder.markdown(full_response) # Final response\r\n+        st.markdown(answer)\r\n+        assistant_time = get_current_time()\r\n+        st.markdown(f'<span class=\"timestamp\">{assistant_time}</span>', unsafe_allow_html=True)\r\n \r\n     # 3. Add to History\r\n-    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"time\": get_current_time()})\r\n+    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"time\": assistant_time})\r\n     \r\n     st.rerun()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765650383988,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -212,220 +212,203 @@\n \r\n # ⚙️ RAG Chain Initialization (Unchanged - uses st.cache_resource)\r\n @st.cache_resource\r\n def initialize_rag_chain_full():\r\n-    \"\"\"RAG Chain को सभी आवश्यक कॉम्पोनेंट्स के साथ इनिशियलाइज़ करता है।\"\"\"\r\n-    with st.spinner(\"Initializing Medical RAG Chatbot...\"):\r\n-        # ... (Your Pinecone/Groq RAG setup logic remains here) ...\r\n-        embeddings = download_embeddings()\r\n-        index_name = \"medical-chatbot\"\r\n-        try:\r\n-            docsearch = PineconeVectorStore.from_existing_index(\r\n-                index_name=index_name,\r\n-                embedding=embeddings\r\n-            )\r\n-            retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n-        except Exception as e:\r\n-            st.error(f\"Failed to connect to Pinecone index '{index_name}': {e}\")\r\n-            st.stop()\r\n-        chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n-        prompt = ChatPromptTemplate.from_messages([\r\n-            (\"system\", system_prompt),\r\n-            (\"human\", \"{input}\"),\r\n-        ])\r\n-        def format_docs(docs):\r\n-            return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n-        context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-        rag_chain = (\r\n-            {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-            | prompt\r\n-            | chatModel\r\n+    # ... (Your RAG setup logic here) ...\r\n+    embeddings = download_embeddings()\r\n+    index_name = \"medical-chatbot\"\r\n+    try:\r\n+        docsearch = PineconeVectorStore.from_existing_index(\r\n+            index_name=index_name,\r\n+            embedding=embeddings\r\n         )\r\n-        return rag_chain\r\n+        retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+    except Exception as e:\r\n+        st.error(f\"Failed to connect to Pinecone index '{index_name}': {e}\")\r\n+        st.stop()\r\n+    chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+    prompt = ChatPromptTemplate.from_messages([\r\n+        (\"system\", system_prompt),\r\n+        (\"human\", \"{input}\"),\r\n+    ])\r\n+    def format_docs(docs):\r\n+        return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+    context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+    rag_chain = (\r\n+        {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+        | prompt\r\n+        | chatModel\r\n+    )\r\n+    return rag_chain\r\n \r\n if PINECONE_API_KEY and GROQ_API_KEY and 'initialize_rag_chain_full' in locals():\r\n     rag_chain = initialize_rag_chain_full()\r\n else:\r\n     rag_chain = initialize_rag_chain()\r\n \r\n \r\n-# 🎨 3. Streamlit UI and Styling (DARK/MODERN STYLE)\r\n-st.set_page_config(page_title=\"Medical Chatbot (Dark Style)\", layout=\"centered\")\r\n+# 🎨 3. Streamlit UI and Styling (FIXED HEADER & DARK STYLE)\r\n+st.set_page_config(page_title=\"Medical Chatbot (Fixed Header)\", layout=\"centered\")\r\n \r\n-# 🖼️ CUSTOM CSS STYLING (Matching your provided CSS)\r\n+# 🖼️ CUSTOM CSS STYLING (Fixed Header Logic Added)\r\n st.markdown(\r\n     \"\"\"\r\n     <style>\r\n-    /* 1. Global Background (Matching your CSS gradient) */\r\n+    /* 1. Global Background (Dark Gradient) */\r\n     .stApp { \r\n         background: linear-gradient(to right, #26333d, #323741, #21214e); \r\n         color: white;\r\n     }\r\n \r\n     /* 2. Main Container (The Chatbox Border/Box) */\r\n     .main .block-container { \r\n         max-width: 600px;\r\n-        height: 85vh; \r\n+        height: 85vh; /* Fixed height for the entire card */\r\n         \r\n-        /* Dark Card Style */\r\n         background-color: rgba(0,0,0,0.4) !important; \r\n         border-radius: 15px !important; \r\n         box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5); \r\n         \r\n-        padding-top: 0 !important;\r\n-        padding-bottom: 0 !important;\r\n-        padding-left: 0 !important;\r\n-        padding-right: 0 !important;\r\n-        \r\n+        padding: 0 !important; /* Remove all internal padding */\r\n         margin-top: 30px; \r\n         margin-bottom: 30px;\r\n-        overflow: hidden; \r\n+        position: relative; /* Base for fixed/absolute children */\r\n     }\r\n     \r\n-    /* 3. Chat Header (FIXED) */\r\n+    /* 3. Chat Header (ABSOLUTELY POSITIONED within the fixed container) */\r\n     .chat-header {\r\n-        background-color: #007bff; /* Primary Blue from HTML */\r\n+        background-color: #007bff; \r\n         color: white; \r\n         padding: 15px; \r\n         display: flex; \r\n         align-items: center;\r\n         \r\n-        position: sticky; \r\n+        /* Make it stick to the top edge of the block-container */\r\n+        position: absolute; \r\n         top: 0; \r\n+        left: 0;\r\n+        right: 0;\r\n         width: 100%;\r\n         z-index: 11;\r\n         border-radius: 15px 15px 0 0 !important;\r\n-        border-bottom: 0 !important;\r\n     }\r\n     \r\n-    /* 4. USER Message (Right Aligned, Green Bubble - #58cc71) */\r\n+    /* 4. Chat Input Bar (ABSOLUTELY POSITIONED at the bottom) */\r\n+    .stChatInputContainer {\r\n+        position: absolute; /* Make it stick to the bottom edge */\r\n+        bottom: 0; \r\n+        left: 0;\r\n+        right: 0;\r\n+        width: 100%;\r\n+        background-color: rgba(0,0,0,0.4) !important;\r\n+        padding: 10px 15px;\r\n+        z-index: 10; \r\n+        border-radius: 0 0 15px 15px !important;\r\n+    }\r\n+    \r\n+    /* 5. Scrollable Chat Content Area (This holds all the messages) */\r\n+    .main div[data-testid=\"stVerticalBlock\"] {\r\n+        /* Height adjustment: Total height (85vh) - Header Height (~60px) - Footer Height (~80px) */\r\n+        height: calc(85vh - 140px); \r\n+        overflow-y: auto; /* Only messages scroll */\r\n+        padding-top: 75px; /* Add padding equal to the header height */\r\n+        padding-bottom: 75px; /* Add padding equal to the footer height */\r\n+        padding-left: 10px;\r\n+        padding-right: 10px;\r\n+        color: white;\r\n+        width: 100%; /* Ensure it spans the full width */\r\n+    }\r\n+\r\n+    /* --- Message Bubble Styles (Unchanged from Dark Theme) --- */\r\n+    \r\n+    /* USER Message (Right Aligned, Green Bubble - #58cc71) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n-        background-color: #58cc71; /* Green Bubble from your CSS */\r\n+        background-color: #58cc71; \r\n         color: white;\r\n-        border-radius: 25px !important; /* Fully rounded */\r\n+        border-radius: 25px !important; \r\n         padding: 10px; \r\n         margin-left: 30%; \r\n         text-align: right;\r\n-        margin-right: 15px;\r\n+        margin-right: 5px; /* Adjusted margin */\r\n         margin-top: 10px;\r\n     }\r\n-    /* Hide avatar for user messages */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stImage {\r\n         display: none;\r\n     }\r\n \r\n-    /* 5. ASSISTANT Message (Left Aligned, Blue Bubble - #52acff) */\r\n+    /* ASSISTANT Message (Left Aligned, Blue Bubble - #52acff) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n-        background-color: #52acff; /* Blue Bubble from your CSS */\r\n+        background-color: #52acff; \r\n         color: white;\r\n-        border-radius: 25px !important; /* Fully rounded */\r\n+        border-radius: 25px !important; \r\n         padding: 10px; \r\n         margin-right: 30%; \r\n         text-align: left;\r\n-        margin-left: 15px;\r\n+        margin-left: 5px; /* Adjusted margin */\r\n         margin-top: 10px;\r\n     }\r\n-    /* Assistant avatar/icon positioning */\r\n-    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child {\r\n-        display: flex;\r\n-        align-items: flex-start;\r\n-    }\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stImage {\r\n-         margin-right: 10px; \r\n-         flex-shrink: 0;\r\n-         /* Matching user_img_msg style */\r\n          height: 40px; \r\n          width: 40px; \r\n          border: 1.5px solid #f5f6fa;\r\n          border-radius: 50%;\r\n+         margin-right: 10px;\r\n     }\r\n \r\n-\r\n-    /* 6. Time and Timestamp Styling */\r\n-    .timestamp { \r\n-        font-size: 10px; \r\n-        color: rgba(255,255,255,0.6); /* Lighter white for dark theme */\r\n-        margin-top: 5px; \r\n-        display: block; \r\n-        line-height: 1;\r\n-        /* Position as per your CSS: outside the bubble, but Streamlit makes this difficult. \r\n-           We'll keep it inside but small and aligned. */\r\n-        text-align: right;\r\n-    }\r\n-    /* Left-aligned time for assistant */\r\n-    .stChatMessage:has(> div > [data-testid=\"stImage\"]):last-child .timestamp {\r\n-        text-align: left;\r\n-    }\r\n-\r\n-    /* 7. Chat Input Bar (FIXED/Footer) */\r\n-    .stChatInputContainer {\r\n-        position: sticky; \r\n-        bottom: 0; \r\n-        width: 100%; \r\n-        background-color: rgba(0,0,0,0.4) !important; /* Dark background */\r\n-        padding: 10px 15px;\r\n-        z-index: 10; \r\n-        border-radius: 0 0 15px 15px !important; /* Matching card-footer */\r\n-    }\r\n-    /* Input field styling (type_msg) */\r\n+    /* Input Field and Button Styles (Unchanged from Dark Theme) */\r\n     .stChatInput > div > div > input {\r\n         background-color: rgba(0,0,0,0.3) !important;\r\n         border: 0 !important;\r\n         color: white !important;\r\n         height: 60px !important;\r\n-        box-shadow: none !important;\r\n         border-radius: 15px 0 0 15px !important;\r\n     }\r\n-    /* Send button styling (send_btn) */\r\n     .stChatInput > div > div:last-child {\r\n         background-color: rgba(0,0,0,0.3) !important;\r\n         border-radius: 0 15px 15px 0 !important;\r\n-        border: 0 !important;\r\n         color: white !important;\r\n-        cursor: pointer;\r\n-    }\r\n-    /* Adjust input and button sizing to fit */\r\n-    .stChatInput > div > div:nth-child(1) {\r\n-        flex: 1;\r\n-    }\r\n-    .stChatInput > div > div:last-child {\r\n-        width: 60px; /* Width similar to input height */\r\n+        width: 60px;\r\n         display: flex;\r\n         align-items: center;\r\n         justify-content: center;\r\n     }\r\n \r\n-\r\n-    /* 8. Scrollable Chat Content Area (msg_card_body) */\r\n-    .main div[data-testid=\"stVerticalBlock\"] {\r\n-        height: calc(85vh - 140px); /* Total container height minus header and input bar height */\r\n-        overflow-y: auto; \r\n-        padding: 10px; /* Matching msg_card_body padding */\r\n-        color: white; /* Chat content color */\r\n+    /* Timestamp (Unchanged) */\r\n+    .timestamp { \r\n+        font-size: 10px; \r\n+        color: rgba(255,255,255,0.6);\r\n+        margin-top: 5px; \r\n+        display: block; \r\n+        line-height: 1;\r\n+        text-align: right;\r\n     }\r\n+    .stChatMessage:has(> div > [data-testid=\"stImage\"]):last-child .timestamp {\r\n+        text-align: left;\r\n+    }\r\n     </style>\r\n     \"\"\",\r\n     unsafe_allow_html=True\r\n )\r\n \r\n-\r\n # 🕒 टाइमस्टैम्प फंक्शन\r\n def get_current_time():\r\n     return time.strftime(\"%I:%M %p\")\r\n \r\n-# Chat Header (HTML) - Fixed Header\r\n-st.markdown(\r\n-    \"\"\"\r\n-    <div class='chat-header'>\r\n-        <img src=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\" class=\"rounded-circle mr-2\" width=\"40\" height=\"40\" style=\"background-color: #f5f6fa; border: 1.5px solid #f5f6fa;\">\r\n-        <div>\r\n-            <strong>Medical Chatbot</strong><br>\r\n-            <small>Ask me anything!</small>\r\n+# Chat Header (HTML) - Fixed Header (Added a surrounding container to ensure absolute positioning works)\r\n+header_placeholder = st.empty()\r\n+with header_placeholder.container():\r\n+    st.markdown(\r\n+        \"\"\"\r\n+        <div class='chat-header'>\r\n+            <img src=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\" class=\"rounded-circle mr-2\" width=\"40\" height=\"40\" style=\"background-color: #f5f6fa; border: 1.5px solid #f5f6fa;\">\r\n+            <div>\r\n+                <strong>Medical Chatbot</strong><br>\r\n+                <small>Ask me anything!</small>\r\n+            </div>\r\n         </div>\r\n-    </div>\r\n-    \"\"\", \r\n-    unsafe_allow_html=True\r\n-)\r\n+        \"\"\", \r\n+        unsafe_allow_html=True\r\n+    )\r\n \r\n # 📜 4. चैट इतिहास (Session State)\r\n if \"messages\" not in st.session_state:\r\n     st.session_state.messages = [\r\n@@ -433,21 +416,17 @@\n     ]\r\n \r\n # 💬 पुराने संदेशों को प्रदर्शित करें\r\n for message in st.session_state.messages:\r\n-    # Set avatar for assistant (User has none)\r\n     avatar = \"https://cdn-icons-png.flaticon.com/512/387/387569.png\" if message[\"role\"] == \"assistant\" else None\r\n     \r\n     with st.chat_message(message[\"role\"], avatar=avatar):\r\n-        # Display message content\r\n         st.markdown(message[\"content\"])\r\n-        \r\n-        # Display timestamp\r\n         st.markdown(f'<span class=\"timestamp\">{message[\"time\"]}</span>', unsafe_allow_html=True) \r\n \r\n \r\n # ⌨️ 5. नया उपयोगकर्ता इनपुट हैंडल करें\r\n-if prompt := st.chat_input(\"Type your message...\"):\r\n+if prompt := st.chat_input(\"Type your message...\", key=\"chat_input_fixed\"):\r\n     current_time = get_current_time()\r\n     \r\n     # 1. User Message\r\n     user_message = {\"role\": \"user\", \"content\": prompt, \"time\": current_time}\r\n@@ -460,9 +439,8 @@\n     # 2. Assistant Response\r\n     with st.chat_message(\"assistant\", avatar=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\"):\r\n         with st.spinner(\"Processing your query...\"):\r\n             try:\r\n-                # Invoke RAG Chain\r\n                 response = rag_chain.invoke({\"input\": prompt})\r\n                 if hasattr(response, \"content\"):\r\n                     answer = response.content\r\n                 else:\r\n"
                },
                {
                    "date": 1765650758052,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,101 +250,109 @@\n st.set_page_config(page_title=\"Medical Chatbot (Fixed Header)\", layout=\"centered\")\r\n \r\n # 🖼️ CUSTOM CSS STYLING (Fixed Header Logic Added)\r\n st.markdown(\r\n+    st.markdown(\r\n     \"\"\"\r\n     <style>\r\n     /* 1. Global Background (Dark Gradient) */\r\n     .stApp { \r\n         background: linear-gradient(to right, #26333d, #323741, #21214e); \r\n         color: white;\r\n     }\r\n \r\n-    /* 2. Main Container (The Chatbox Border/Box) */\r\n+    /* 2. Streamlit's Main Block Container (The Chatbox Card) */\r\n     .main .block-container { \r\n         max-width: 600px;\r\n-        height: 85vh; /* Fixed height for the entire card */\r\n+        /* vh (Viewport Height) का उपयोग करके ऊंचाई को फिक्स करें */\r\n+        height: 85vh; \r\n         \r\n         background-color: rgba(0,0,0,0.4) !important; \r\n         border-radius: 15px !important; \r\n         box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5); \r\n         \r\n-        padding: 0 !important; /* Remove all internal padding */\r\n+        /* कोई डिफ़ॉल्ट पैडिंग नहीं */\r\n+        padding: 0 !important; \r\n         margin-top: 30px; \r\n         margin-bottom: 30px;\r\n-        position: relative; /* Base for fixed/absolute children */\r\n+        position: relative; /* Fixed children इसी के सापेक्ष होंगे */\r\n     }\r\n     \r\n-    /* 3. Chat Header (ABSOLUTELY POSITIONED within the fixed container) */\r\n+    /* 3. Streamlit's main content wrapper (जो स्क्रॉल हो रहा होता है) को फिक्स करें */\r\n+    div[data-testid=\"stVerticalBlock\"] {\r\n+        /* हेडर और फूटर के लिए जगह छोड़ें: 85vh (Total) - 75px (Header) - 80px (Footer) = ~68.5vh */\r\n+        height: calc(100% - 145px); /* 100% of parent - fixed elements' height */\r\n+        overflow-y: auto; /* केवल मैसेज ही स्क्रॉल होंगे */\r\n+        \r\n+        /* हेडर और फूटर को ढकने से बचने के लिए पैडिंग */\r\n+        padding-top: 75px; \r\n+        padding-bottom: 80px; \r\n+        padding-left: 10px;\r\n+        padding-right: 10px;\r\n+        color: white;\r\n+        width: 100%;\r\n+        position: absolute; /* इसे कंटेनर के अंदर रखें */\r\n+        top: 0;\r\n+    }\r\n+    \r\n+    /* 4. Chat Header (ABSOLUTELY POSITIONED and FIXED) */\r\n     .chat-header {\r\n         background-color: #007bff; \r\n         color: white; \r\n         padding: 15px; \r\n         display: flex; \r\n         align-items: center;\r\n         \r\n-        /* Make it stick to the top edge of the block-container */\r\n-        position: absolute; \r\n+        position: absolute; /* कंटेनर के शीर्ष पर फिक्स */\r\n         top: 0; \r\n         left: 0;\r\n         right: 0;\r\n         width: 100%;\r\n+        height: 75px; /* हेडर की ऊंचाई फिक्स करें */\r\n         z-index: 11;\r\n         border-radius: 15px 15px 0 0 !important;\r\n     }\r\n     \r\n-    /* 4. Chat Input Bar (ABSOLUTELY POSITIONED at the bottom) */\r\n+    /* 5. Chat Input Bar (ABSOLUTELY POSITIONED at the bottom) */\r\n     .stChatInputContainer {\r\n-        position: absolute; /* Make it stick to the bottom edge */\r\n+        position: absolute; /* कंटेनर के निचले भाग पर फिक्स */\r\n         bottom: 0; \r\n         left: 0;\r\n         right: 0;\r\n         width: 100%;\r\n         background-color: rgba(0,0,0,0.4) !important;\r\n         padding: 10px 15px;\r\n+        height: 80px; /* फूटर/इनपुट बार की ऊंचाई फिक्स करें */\r\n         z-index: 10; \r\n         border-radius: 0 0 15px 15px !important;\r\n     }\r\n     \r\n-    /* 5. Scrollable Chat Content Area (This holds all the messages) */\r\n-    .main div[data-testid=\"stVerticalBlock\"] {\r\n-        /* Height adjustment: Total height (85vh) - Header Height (~60px) - Footer Height (~80px) */\r\n-        height: calc(85vh - 140px); \r\n-        overflow-y: auto; /* Only messages scroll */\r\n-        padding-top: 75px; /* Add padding equal to the header height */\r\n-        padding-bottom: 75px; /* Add padding equal to the footer height */\r\n-        padding-left: 10px;\r\n-        padding-right: 10px;\r\n-        color: white;\r\n-        width: 100%; /* Ensure it spans the full width */\r\n-    }\r\n-\r\n-    /* --- Message Bubble Styles (Unchanged from Dark Theme) --- */\r\n+    /* --- Message Bubble Styles (Unchanged) --- */\r\n     \r\n-    /* USER Message (Right Aligned, Green Bubble - #58cc71) */\r\n+    /* USER Message (Green Bubble) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n         background-color: #58cc71; \r\n         color: white;\r\n         border-radius: 25px !important; \r\n         padding: 10px; \r\n         margin-left: 30%; \r\n         text-align: right;\r\n-        margin-right: 5px; /* Adjusted margin */\r\n+        margin-right: 5px;\r\n         margin-top: 10px;\r\n     }\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stImage {\r\n         display: none;\r\n     }\r\n \r\n-    /* ASSISTANT Message (Left Aligned, Blue Bubble - #52acff) */\r\n+    /* ASSISTANT Message (Blue Bubble) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n         background-color: #52acff; \r\n         color: white;\r\n         border-radius: 25px !important; \r\n         padding: 10px; \r\n         margin-right: 30%; \r\n         text-align: left;\r\n-        margin-left: 5px; /* Adjusted margin */\r\n+        margin-left: 5px; \r\n         margin-top: 10px;\r\n     }\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stImage {\r\n          height: 40px; \r\n@@ -353,9 +361,9 @@\n          border-radius: 50%;\r\n          margin-right: 10px;\r\n     }\r\n \r\n-    /* Input Field and Button Styles (Unchanged from Dark Theme) */\r\n+    /* Input Field and Button Styles */\r\n     .stChatInput > div > div > input {\r\n         background-color: rgba(0,0,0,0.3) !important;\r\n         border: 0 !important;\r\n         color: white !important;\r\n@@ -371,9 +379,9 @@\n         align-items: center;\r\n         justify-content: center;\r\n     }\r\n \r\n-    /* Timestamp (Unchanged) */\r\n+    /* Timestamp */\r\n     .timestamp { \r\n         font-size: 10px; \r\n         color: rgba(255,255,255,0.6);\r\n         margin-top: 5px; \r\n@@ -386,8 +394,10 @@\n     }\r\n     </style>\r\n     \"\"\",\r\n     unsafe_allow_html=True\r\n+),\r\n+    unsafe_allow_html=True\r\n )\r\n \r\n # 🕒 टाइमस्टैम्प फंक्शन\r\n def get_current_time():\r\n"
                },
                {
                    "date": 1765650952157,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,109 +250,101 @@\n st.set_page_config(page_title=\"Medical Chatbot (Fixed Header)\", layout=\"centered\")\r\n \r\n # 🖼️ CUSTOM CSS STYLING (Fixed Header Logic Added)\r\n st.markdown(\r\n-    st.markdown(\r\n     \"\"\"\r\n     <style>\r\n     /* 1. Global Background (Dark Gradient) */\r\n     .stApp { \r\n         background: linear-gradient(to right, #26333d, #323741, #21214e); \r\n         color: white;\r\n     }\r\n \r\n-    /* 2. Streamlit's Main Block Container (The Chatbox Card) */\r\n+    /* 2. Main Container (The Chatbox Border/Box) */\r\n     .main .block-container { \r\n         max-width: 600px;\r\n-        /* vh (Viewport Height) का उपयोग करके ऊंचाई को फिक्स करें */\r\n-        height: 85vh; \r\n+        height: 85vh; /* Fixed height for the entire card */\r\n         \r\n         background-color: rgba(0,0,0,0.4) !important; \r\n         border-radius: 15px !important; \r\n         box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5); \r\n         \r\n-        /* कोई डिफ़ॉल्ट पैडिंग नहीं */\r\n-        padding: 0 !important; \r\n+        padding: 0 !important; /* Remove all internal padding */\r\n         margin-top: 30px; \r\n         margin-bottom: 30px;\r\n-        position: relative; /* Fixed children इसी के सापेक्ष होंगे */\r\n+        position: relative; /* Base for fixed/absolute children */\r\n     }\r\n     \r\n-    /* 3. Streamlit's main content wrapper (जो स्क्रॉल हो रहा होता है) को फिक्स करें */\r\n-    div[data-testid=\"stVerticalBlock\"] {\r\n-        /* हेडर और फूटर के लिए जगह छोड़ें: 85vh (Total) - 75px (Header) - 80px (Footer) = ~68.5vh */\r\n-        height: calc(100% - 145px); /* 100% of parent - fixed elements' height */\r\n-        overflow-y: auto; /* केवल मैसेज ही स्क्रॉल होंगे */\r\n-        \r\n-        /* हेडर और फूटर को ढकने से बचने के लिए पैडिंग */\r\n-        padding-top: 75px; \r\n-        padding-bottom: 80px; \r\n-        padding-left: 10px;\r\n-        padding-right: 10px;\r\n-        color: white;\r\n-        width: 100%;\r\n-        position: absolute; /* इसे कंटेनर के अंदर रखें */\r\n-        top: 0;\r\n-    }\r\n-    \r\n-    /* 4. Chat Header (ABSOLUTELY POSITIONED and FIXED) */\r\n+    /* 3. Chat Header (ABSOLUTELY POSITIONED within the fixed container) */\r\n     .chat-header {\r\n         background-color: #007bff; \r\n         color: white; \r\n         padding: 15px; \r\n         display: flex; \r\n         align-items: center;\r\n         \r\n-        position: absolute; /* कंटेनर के शीर्ष पर फिक्स */\r\n+        /* Make it stick to the top edge of the block-container */\r\n+        position: absolute; \r\n         top: 0; \r\n         left: 0;\r\n         right: 0;\r\n         width: 100%;\r\n-        height: 75px; /* हेडर की ऊंचाई फिक्स करें */\r\n         z-index: 11;\r\n         border-radius: 15px 15px 0 0 !important;\r\n     }\r\n     \r\n-    /* 5. Chat Input Bar (ABSOLUTELY POSITIONED at the bottom) */\r\n+    /* 4. Chat Input Bar (ABSOLUTELY POSITIONED at the bottom) */\r\n     .stChatInputContainer {\r\n-        position: absolute; /* कंटेनर के निचले भाग पर फिक्स */\r\n+        position: absolute; /* Make it stick to the bottom edge */\r\n         bottom: 0; \r\n         left: 0;\r\n         right: 0;\r\n         width: 100%;\r\n         background-color: rgba(0,0,0,0.4) !important;\r\n         padding: 10px 15px;\r\n-        height: 80px; /* फूटर/इनपुट बार की ऊंचाई फिक्स करें */\r\n         z-index: 10; \r\n         border-radius: 0 0 15px 15px !important;\r\n     }\r\n     \r\n-    /* --- Message Bubble Styles (Unchanged) --- */\r\n+    /* 5. Scrollable Chat Content Area (This holds all the messages) */\r\n+    .main div[data-testid=\"stVerticalBlock\"] {\r\n+        /* Height adjustment: Total height (85vh) - Header Height (~60px) - Footer Height (~80px) */\r\n+        height: calc(85vh - 140px); \r\n+        overflow-y: auto; /* Only messages scroll */\r\n+        padding-top: 75px; /* Add padding equal to the header height */\r\n+        padding-bottom: 75px; /* Add padding equal to the footer height */\r\n+        padding-left: 10px;\r\n+        padding-right: 10px;\r\n+        color: white;\r\n+        width: 100%; /* Ensure it spans the full width */\r\n+    }\r\n+\r\n+    /* --- Message Bubble Styles (Unchanged from Dark Theme) --- */\r\n     \r\n-    /* USER Message (Green Bubble) */\r\n+    /* USER Message (Right Aligned, Green Bubble - #58cc71) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n         background-color: #58cc71; \r\n         color: white;\r\n         border-radius: 25px !important; \r\n         padding: 10px; \r\n         margin-left: 30%; \r\n         text-align: right;\r\n-        margin-right: 5px;\r\n+        margin-right: 5px; /* Adjusted margin */\r\n         margin-top: 10px;\r\n     }\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stImage {\r\n         display: none;\r\n     }\r\n \r\n-    /* ASSISTANT Message (Blue Bubble) */\r\n+    /* ASSISTANT Message (Left Aligned, Blue Bubble - #52acff) */\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n         background-color: #52acff; \r\n         color: white;\r\n         border-radius: 25px !important; \r\n         padding: 10px; \r\n         margin-right: 30%; \r\n         text-align: left;\r\n-        margin-left: 5px; \r\n+        margin-left: 5px; /* Adjusted margin */\r\n         margin-top: 10px;\r\n     }\r\n     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stImage {\r\n          height: 40px; \r\n@@ -361,9 +353,9 @@\n          border-radius: 50%;\r\n          margin-right: 10px;\r\n     }\r\n \r\n-    /* Input Field and Button Styles */\r\n+    /* Input Field and Button Styles (Unchanged from Dark Theme) */\r\n     .stChatInput > div > div > input {\r\n         background-color: rgba(0,0,0,0.3) !important;\r\n         border: 0 !important;\r\n         color: white !important;\r\n@@ -379,9 +371,9 @@\n         align-items: center;\r\n         justify-content: center;\r\n     }\r\n \r\n-    /* Timestamp */\r\n+    /* Timestamp (Unchanged) */\r\n     .timestamp { \r\n         font-size: 10px; \r\n         color: rgba(255,255,255,0.6);\r\n         margin-top: 5px; \r\n@@ -394,10 +386,8 @@\n     }\r\n     </style>\r\n     \"\"\",\r\n     unsafe_allow_html=True\r\n-),\r\n-    unsafe_allow_html=True\r\n )\r\n \r\n # 🕒 टाइमस्टैम्प फंक्शन\r\n def get_current_time():\r\n"
                },
                {
                    "date": 1765651272223,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,84 +1,84 @@\n-# from flask import Flask, render_template, request\r\n-# from dotenv import load_dotenv\r\n-# import os\r\n-# from operator import itemgetter\r\n+from flask import Flask, render_template, request\r\n+from dotenv import load_dotenv\r\n+import os\r\n+from operator import itemgetter\r\n \r\n-# from src.helper import download_embeddings\r\n-# from src.prompt import system_prompt\r\n-# from langchain_pinecone import PineconeVectorStore\r\n-# from langchain_core.prompts import ChatPromptTemplate\r\n-# from langchain_core.runnables import RunnableLambda\r\n-# from langchain_groq import ChatGroq\r\n+from src.helper import download_embeddings\r\n+from src.prompt import system_prompt\r\n+from langchain_pinecone import PineconeVectorStore\r\n+from langchain_core.prompts import ChatPromptTemplate\r\n+from langchain_core.runnables import RunnableLambda\r\n+from langchain_groq import ChatGroq\r\n \r\n-# app = Flask(__name__)\r\n-# load_dotenv()\r\n+app = Flask(__name__)\r\n+load_dotenv()\r\n \r\n-# # API keys\r\n-# PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n-# GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n-# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n-# os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n+# API keys\r\n+PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n+GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n+os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\r\n+os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\r\n \r\n-# # Embeddings + Pinecone retriever\r\n-# embeddings = download_embeddings()\r\n-# index_name = \"medical-chatbot\"\r\n-# docsearch = PineconeVectorStore.from_existing_index(\r\n-#     index_name=index_name,\r\n-#     embedding=embeddings\r\n-# )\r\n-# retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+# Embeddings + Pinecone retriever\r\n+embeddings = download_embeddings()\r\n+index_name = \"medical-chatbot\"\r\n+docsearch = PineconeVectorStore.from_existing_index(\r\n+    index_name=index_name,\r\n+    embedding=embeddings\r\n+)\r\n+retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n \r\n-# # Groq LLM\r\n-# chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+# Groq LLM\r\n+chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n \r\n-# # Prompt template\r\n-# prompt = ChatPromptTemplate.from_messages([\r\n-#     (\"system\", system_prompt),\r\n-#     (\"human\", \"{input}\"),\r\n-# ])\r\n+# Prompt template\r\n+prompt = ChatPromptTemplate.from_messages([\r\n+    (\"system\", system_prompt),\r\n+    (\"human\", \"{input}\"),\r\n+])\r\n \r\n-# # Format docs function\r\n-# def format_docs(docs):\r\n-#     return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+# Format docs function\r\n+def format_docs(docs):\r\n+    return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n \r\n-# # Context branch\r\n-# context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+# Context branch\r\n+context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n \r\n-# # RAG chain\r\n-# rag_chain = (\r\n-#     {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-#     | prompt\r\n-#     | chatModel\r\n-# )\r\n+# RAG chain\r\n+rag_chain = (\r\n+    {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+    | prompt\r\n+    | chatModel\r\n+)\r\n \r\n-# @app.route(\"/\")\r\n-# def index():\r\n-#     return render_template(\"chat.html\")\r\n+@app.route(\"/\")\r\n+def index():\r\n+    return render_template(\"chat.html\")\r\n \r\n-# @app.route(\"/get\", methods=[\"POST\"])\r\n-# def chat():\r\n-#     msg = request.form[\"msg\"]\r\n-#     #print(\"User:\", msg)\r\n+@app.route(\"/get\", methods=[\"POST\"])\r\n+def chat():\r\n+    msg = request.form[\"msg\"]\r\n+    #print(\"User:\", msg)\r\n \r\n-#     try:\r\n-#         response = rag_chain.invoke({\"input\": msg})\r\n-#         #print(\"Raw response:\", response)\r\n+    try:\r\n+        response = rag_chain.invoke({\"input\": msg})\r\n+        #print(\"Raw response:\", response)\r\n \r\n-#         # AIMessage object → .content\r\n-#         if hasattr(response, \"content\"):\r\n-#             answer = response.content\r\n-#         else:\r\n-#             answer = str(response)\r\n+        # AIMessage object → .content\r\n+        if hasattr(response, \"content\"):\r\n+            answer = response.content\r\n+        else:\r\n+            answer = str(response)\r\n \r\n-#         #print(\"Final Answer:\", answer)\r\n-#         return str(answer)\r\n-#     except Exception as e:\r\n-#         print(\"Error in rag_chain:\", e)\r\n-#         return \"Error: Could not get response\"\r\n+        #print(\"Final Answer:\", answer)\r\n+        return str(answer)\r\n+    except Exception as e:\r\n+        print(\"Error in rag_chain:\", e)\r\n+        return \"Error: Could not get response\"\r\n \r\n-# if __name__ == \"__main__\":\r\n-#     app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n+if __name__ == \"__main__\":\r\n+    app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n \r\n \r\n \r\n # import streamlit as st\r\n@@ -180,281 +180,281 @@\n \r\n \r\n \r\n \r\n-import streamlit as st\r\n-import os\r\n-import time\r\n-from operator import itemgetter\r\n+# import streamlit as st\r\n+# import os\r\n+# import time\r\n+# from operator import itemgetter\r\n \r\n-# Langchain/RAG components\r\n-from dotenv import load_dotenv\r\n-from langchain_pinecone import PineconeVectorStore\r\n-from langchain_core.prompts import ChatPromptTemplate\r\n-from langchain_core.runnables import RunnableLambda\r\n-from langchain_groq import ChatGroq\r\n+# # Langchain/RAG components\r\n+# from dotenv import load_dotenv\r\n+# from langchain_pinecone import PineconeVectorStore\r\n+# from langchain_core.prompts import ChatPromptTemplate\r\n+# from langchain_core.runnables import RunnableLambda\r\n+# from langchain_groq import ChatGroq\r\n \r\n-# ⚠️ सुनिश्चित करें कि src/helper.py और src/prompt.py उपलब्ध हैं\r\n-try:\r\n-    from src.helper import download_embeddings\r\n-    from src.prompt import system_prompt\r\n-except ImportError:\r\n-    st.warning(\"RAG files (src/helper.py, src/prompt.py) not found. Using Dummy Chat Response.\")\r\n-    system_prompt = \"You are a helpful medical assistant.\"\r\n+# # ⚠️ सुनिश्चित करें कि src/helper.py और src/prompt.py उपलब्ध हैं\r\n+# try:\r\n+#     from src.helper import download_embeddings\r\n+#     from src.prompt import system_prompt\r\n+# except ImportError:\r\n+#     st.warning(\"RAG files (src/helper.py, src/prompt.py) not found. Using Dummy Chat Response.\")\r\n+#     system_prompt = \"You are a helpful medical assistant.\"\r\n     \r\n-    def initialize_rag_chain():\r\n-        return lambda x: f\"Dummy RAG Response: Please set up your RAG chain correctly. You asked: {x['input']}\"\r\n+#     def initialize_rag_chain():\r\n+#         return lambda x: f\"Dummy RAG Response: Please set up your RAG chain correctly. You asked: {x['input']}\"\r\n     \r\n-    initialize_rag_chain_full = initialize_rag_chain # Dummy assignment\r\n+#     initialize_rag_chain_full = initialize_rag_chain # Dummy assignment\r\n \r\n-load_dotenv()\r\n-PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n-GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n+# load_dotenv()\r\n+# PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\r\n+# GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\r\n \r\n-# ⚙️ RAG Chain Initialization (Unchanged - uses st.cache_resource)\r\n-@st.cache_resource\r\n-def initialize_rag_chain_full():\r\n-    # ... (Your RAG setup logic here) ...\r\n-    embeddings = download_embeddings()\r\n-    index_name = \"medical-chatbot\"\r\n-    try:\r\n-        docsearch = PineconeVectorStore.from_existing_index(\r\n-            index_name=index_name,\r\n-            embedding=embeddings\r\n-        )\r\n-        retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n-    except Exception as e:\r\n-        st.error(f\"Failed to connect to Pinecone index '{index_name}': {e}\")\r\n-        st.stop()\r\n-    chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n-    prompt = ChatPromptTemplate.from_messages([\r\n-        (\"system\", system_prompt),\r\n-        (\"human\", \"{input}\"),\r\n-    ])\r\n-    def format_docs(docs):\r\n-        return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n-    context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n-    rag_chain = (\r\n-        {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n-        | prompt\r\n-        | chatModel\r\n-    )\r\n-    return rag_chain\r\n+# # ⚙️ RAG Chain Initialization (Unchanged - uses st.cache_resource)\r\n+# @st.cache_resource\r\n+# def initialize_rag_chain_full():\r\n+#     # ... (Your RAG setup logic here) ...\r\n+#     embeddings = download_embeddings()\r\n+#     index_name = \"medical-chatbot\"\r\n+#     try:\r\n+#         docsearch = PineconeVectorStore.from_existing_index(\r\n+#             index_name=index_name,\r\n+#             embedding=embeddings\r\n+#         )\r\n+#         retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\r\n+#     except Exception as e:\r\n+#         st.error(f\"Failed to connect to Pinecone index '{index_name}': {e}\")\r\n+#         st.stop()\r\n+#     chatModel = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n+#     prompt = ChatPromptTemplate.from_messages([\r\n+#         (\"system\", system_prompt),\r\n+#         (\"human\", \"{input}\"),\r\n+#     ])\r\n+#     def format_docs(docs):\r\n+#         return \"\\n\\n\".join([getattr(d, \"page_content\", str(d)) for d in docs])\r\n+#     context_branch = itemgetter(\"input\") | retriever | RunnableLambda(format_docs)\r\n+#     rag_chain = (\r\n+#         {\"context\": context_branch, \"input\": itemgetter(\"input\")}\r\n+#         | prompt\r\n+#         | chatModel\r\n+#     )\r\n+#     return rag_chain\r\n \r\n-if PINECONE_API_KEY and GROQ_API_KEY and 'initialize_rag_chain_full' in locals():\r\n-    rag_chain = initialize_rag_chain_full()\r\n-else:\r\n-    rag_chain = initialize_rag_chain()\r\n+# if PINECONE_API_KEY and GROQ_API_KEY and 'initialize_rag_chain_full' in locals():\r\n+#     rag_chain = initialize_rag_chain_full()\r\n+# else:\r\n+#     rag_chain = initialize_rag_chain()\r\n \r\n \r\n-# 🎨 3. Streamlit UI and Styling (FIXED HEADER & DARK STYLE)\r\n-st.set_page_config(page_title=\"Medical Chatbot (Fixed Header)\", layout=\"centered\")\r\n+# # 🎨 3. Streamlit UI and Styling (FIXED HEADER & DARK STYLE)\r\n+# st.set_page_config(page_title=\"Medical Chatbot (Fixed Header)\", layout=\"centered\")\r\n \r\n-# 🖼️ CUSTOM CSS STYLING (Fixed Header Logic Added)\r\n-st.markdown(\r\n-    \"\"\"\r\n-    <style>\r\n-    /* 1. Global Background (Dark Gradient) */\r\n-    .stApp { \r\n-        background: linear-gradient(to right, #26333d, #323741, #21214e); \r\n-        color: white;\r\n-    }\r\n+# # 🖼️ CUSTOM CSS STYLING (Fixed Header Logic Added)\r\n+# st.markdown(\r\n+#     \"\"\"\r\n+#     <style>\r\n+#     /* 1. Global Background (Dark Gradient) */\r\n+#     .stApp { \r\n+#         background: linear-gradient(to right, #26333d, #323741, #21214e); \r\n+#         color: white;\r\n+#     }\r\n \r\n-    /* 2. Main Container (The Chatbox Border/Box) */\r\n-    .main .block-container { \r\n-        max-width: 600px;\r\n-        height: 85vh; /* Fixed height for the entire card */\r\n+#     /* 2. Main Container (The Chatbox Border/Box) */\r\n+#     .main .block-container { \r\n+#         max-width: 600px;\r\n+#         height: 85vh; /* Fixed height for the entire card */\r\n         \r\n-        background-color: rgba(0,0,0,0.4) !important; \r\n-        border-radius: 15px !important; \r\n-        box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5); \r\n+#         background-color: rgba(0,0,0,0.4) !important; \r\n+#         border-radius: 15px !important; \r\n+#         box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5); \r\n         \r\n-        padding: 0 !important; /* Remove all internal padding */\r\n-        margin-top: 30px; \r\n-        margin-bottom: 30px;\r\n-        position: relative; /* Base for fixed/absolute children */\r\n-    }\r\n+#         padding: 0 !important; /* Remove all internal padding */\r\n+#         margin-top: 30px; \r\n+#         margin-bottom: 30px;\r\n+#         position: relative; /* Base for fixed/absolute children */\r\n+#     }\r\n     \r\n-    /* 3. Chat Header (ABSOLUTELY POSITIONED within the fixed container) */\r\n-    .chat-header {\r\n-        background-color: #007bff; \r\n-        color: white; \r\n-        padding: 15px; \r\n-        display: flex; \r\n-        align-items: center;\r\n+#     /* 3. Chat Header (ABSOLUTELY POSITIONED within the fixed container) */\r\n+#     .chat-header {\r\n+#         background-color: #007bff; \r\n+#         color: white; \r\n+#         padding: 15px; \r\n+#         display: flex; \r\n+#         align-items: center;\r\n         \r\n-        /* Make it stick to the top edge of the block-container */\r\n-        position: absolute; \r\n-        top: 0; \r\n-        left: 0;\r\n-        right: 0;\r\n-        width: 100%;\r\n-        z-index: 11;\r\n-        border-radius: 15px 15px 0 0 !important;\r\n-    }\r\n+#         /* Make it stick to the top edge of the block-container */\r\n+#         position: absolute; \r\n+#         top: 0; \r\n+#         left: 0;\r\n+#         right: 0;\r\n+#         width: 100%;\r\n+#         z-index: 11;\r\n+#         border-radius: 15px 15px 0 0 !important;\r\n+#     }\r\n     \r\n-    /* 4. Chat Input Bar (ABSOLUTELY POSITIONED at the bottom) */\r\n-    .stChatInputContainer {\r\n-        position: absolute; /* Make it stick to the bottom edge */\r\n-        bottom: 0; \r\n-        left: 0;\r\n-        right: 0;\r\n-        width: 100%;\r\n-        background-color: rgba(0,0,0,0.4) !important;\r\n-        padding: 10px 15px;\r\n-        z-index: 10; \r\n-        border-radius: 0 0 15px 15px !important;\r\n-    }\r\n+#     /* 4. Chat Input Bar (ABSOLUTELY POSITIONED at the bottom) */\r\n+#     .stChatInputContainer {\r\n+#         position: absolute; /* Make it stick to the bottom edge */\r\n+#         bottom: 0; \r\n+#         left: 0;\r\n+#         right: 0;\r\n+#         width: 100%;\r\n+#         background-color: rgba(0,0,0,0.4) !important;\r\n+#         padding: 10px 15px;\r\n+#         z-index: 10; \r\n+#         border-radius: 0 0 15px 15px !important;\r\n+#     }\r\n     \r\n-    /* 5. Scrollable Chat Content Area (This holds all the messages) */\r\n-    .main div[data-testid=\"stVerticalBlock\"] {\r\n-        /* Height adjustment: Total height (85vh) - Header Height (~60px) - Footer Height (~80px) */\r\n-        height: calc(85vh - 140px); \r\n-        overflow-y: auto; /* Only messages scroll */\r\n-        padding-top: 75px; /* Add padding equal to the header height */\r\n-        padding-bottom: 75px; /* Add padding equal to the footer height */\r\n-        padding-left: 10px;\r\n-        padding-right: 10px;\r\n-        color: white;\r\n-        width: 100%; /* Ensure it spans the full width */\r\n-    }\r\n+#     /* 5. Scrollable Chat Content Area (This holds all the messages) */\r\n+#     .main div[data-testid=\"stVerticalBlock\"] {\r\n+#         /* Height adjustment: Total height (85vh) - Header Height (~60px) - Footer Height (~80px) */\r\n+#         height: calc(85vh - 140px); \r\n+#         overflow-y: auto; /* Only messages scroll */\r\n+#         padding-top: 75px; /* Add padding equal to the header height */\r\n+#         padding-bottom: 75px; /* Add padding equal to the footer height */\r\n+#         padding-left: 10px;\r\n+#         padding-right: 10px;\r\n+#         color: white;\r\n+#         width: 100%; /* Ensure it spans the full width */\r\n+#     }\r\n \r\n-    /* --- Message Bubble Styles (Unchanged from Dark Theme) --- */\r\n+#     /* --- Message Bubble Styles (Unchanged from Dark Theme) --- */\r\n     \r\n-    /* USER Message (Right Aligned, Green Bubble - #58cc71) */\r\n-    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n-        background-color: #58cc71; \r\n-        color: white;\r\n-        border-radius: 25px !important; \r\n-        padding: 10px; \r\n-        margin-left: 30%; \r\n-        text-align: right;\r\n-        margin-right: 5px; /* Adjusted margin */\r\n-        margin-top: 10px;\r\n-    }\r\n-    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stImage {\r\n-        display: none;\r\n-    }\r\n+#     /* USER Message (Right Aligned, Green Bubble - #58cc71) */\r\n+#     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stMarkdown {\r\n+#         background-color: #58cc71; \r\n+#         color: white;\r\n\\ No newline at end of file\n+#         border-radius: 25px !important; \r\n+#         padding: 10px; \r\n+#         margin-left: 30%; \r\n+#         text-align: right;\r\n+#         margin-right: 5px; /* Adjusted margin */\r\n+#         margin-top: 10px;\r\n+#     }\r\n+#     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):first-child .stImage {\r\n+#         display: none;\r\n+#     }\r\n \r\n-    /* ASSISTANT Message (Left Aligned, Blue Bubble - #52acff) */\r\n-    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n-        background-color: #52acff; \r\n-        color: white;\r\n-        border-radius: 25px !important; \r\n-        padding: 10px; \r\n-        margin-right: 30%; \r\n-        text-align: left;\r\n-        margin-left: 5px; /* Adjusted margin */\r\n-        margin-top: 10px;\r\n-    }\r\n-    .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stImage {\r\n-         height: 40px; \r\n-         width: 40px; \r\n-         border: 1.5px solid #f5f6fa;\r\n-         border-radius: 50%;\r\n-         margin-right: 10px;\r\n-    }\r\n+#     /* ASSISTANT Message (Left Aligned, Blue Bubble - #52acff) */\r\n+#     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stMarkdown {\r\n+#         background-color: #52acff; \r\n+#         color: white;\r\n+#         border-radius: 25px !important; \r\n+#         padding: 10px; \r\n+#         margin-right: 30%; \r\n+#         text-align: left;\r\n+#         margin-left: 5px; /* Adjusted margin */\r\n+#         margin-top: 10px;\r\n+#     }\r\n+#     .stChatMessage[data-testid=\"stChatMessage\"]:has(> div > [data-testid=\"stImage\"]):last-child .stImage {\r\n+#          height: 40px; \r\n+#          width: 40px; \r\n+#          border: 1.5px solid #f5f6fa;\r\n+#          border-radius: 50%;\r\n+#          margin-right: 10px;\r\n+#     }\r\n \r\n-    /* Input Field and Button Styles (Unchanged from Dark Theme) */\r\n-    .stChatInput > div > div > input {\r\n-        background-color: rgba(0,0,0,0.3) !important;\r\n-        border: 0 !important;\r\n-        color: white !important;\r\n-        height: 60px !important;\r\n-        border-radius: 15px 0 0 15px !important;\r\n-    }\r\n-    .stChatInput > div > div:last-child {\r\n-        background-color: rgba(0,0,0,0.3) !important;\r\n-        border-radius: 0 15px 15px 0 !important;\r\n-        color: white !important;\r\n-        width: 60px;\r\n-        display: flex;\r\n-        align-items: center;\r\n-        justify-content: center;\r\n-    }\r\n+#     /* Input Field and Button Styles (Unchanged from Dark Theme) */\r\n+#     .stChatInput > div > div > input {\r\n+#         background-color: rgba(0,0,0,0.3) !important;\r\n+#         border: 0 !important;\r\n+#         color: white !important;\r\n+#         height: 60px !important;\r\n+#         border-radius: 15px 0 0 15px !important;\r\n+#     }\r\n+#     .stChatInput > div > div:last-child {\r\n+#         background-color: rgba(0,0,0,0.3) !important;\r\n+#         border-radius: 0 15px 15px 0 !important;\r\n+#         color: white !important;\r\n+#         width: 60px;\r\n+#         display: flex;\r\n+#         align-items: center;\r\n+#         justify-content: center;\r\n+#     }\r\n \r\n-    /* Timestamp (Unchanged) */\r\n-    .timestamp { \r\n-        font-size: 10px; \r\n-        color: rgba(255,255,255,0.6);\r\n-        margin-top: 5px; \r\n-        display: block; \r\n-        line-height: 1;\r\n-        text-align: right;\r\n-    }\r\n-    .stChatMessage:has(> div > [data-testid=\"stImage\"]):last-child .timestamp {\r\n-        text-align: left;\r\n-    }\r\n-    </style>\r\n-    \"\"\",\r\n-    unsafe_allow_html=True\r\n-)\r\n+#     /* Timestamp (Unchanged) */\r\n+#     .timestamp { \r\n+#         font-size: 10px; \r\n+#         color: rgba(255,255,255,0.6);\r\n+#         margin-top: 5px; \r\n+#         display: block; \r\n+#         line-height: 1;\r\n+#         text-align: right;\r\n+#     }\r\n+#     .stChatMessage:has(> div > [data-testid=\"stImage\"]):last-child .timestamp {\r\n+#         text-align: left;\r\n+#     }\r\n+#     </style>\r\n+#     \"\"\",\r\n+#     unsafe_allow_html=True\r\n+# )\r\n \r\n-# 🕒 टाइमस्टैम्प फंक्शन\r\n-def get_current_time():\r\n-    return time.strftime(\"%I:%M %p\")\r\n+# # 🕒 टाइमस्टैम्प फंक्शन\r\n+# def get_current_time():\r\n+#     return time.strftime(\"%I:%M %p\")\r\n \r\n-# Chat Header (HTML) - Fixed Header (Added a surrounding container to ensure absolute positioning works)\r\n-header_placeholder = st.empty()\r\n-with header_placeholder.container():\r\n-    st.markdown(\r\n-        \"\"\"\r\n-        <div class='chat-header'>\r\n-            <img src=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\" class=\"rounded-circle mr-2\" width=\"40\" height=\"40\" style=\"background-color: #f5f6fa; border: 1.5px solid #f5f6fa;\">\r\n-            <div>\r\n-                <strong>Medical Chatbot</strong><br>\r\n-                <small>Ask me anything!</small>\r\n-            </div>\r\n-        </div>\r\n-        \"\"\", \r\n-        unsafe_allow_html=True\r\n-    )\r\n+# # Chat Header (HTML) - Fixed Header (Added a surrounding container to ensure absolute positioning works)\r\n+# header_placeholder = st.empty()\r\n+# with header_placeholder.container():\r\n+#     st.markdown(\r\n+#         \"\"\"\r\n+#         <div class='chat-header'>\r\n+#             <img src=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\" class=\"rounded-circle mr-2\" width=\"40\" height=\"40\" style=\"background-color: #f5f6fa; border: 1.5px solid #f5f6fa;\">\r\n+#             <div>\r\n+#                 <strong>Medical Chatbot</strong><br>\r\n+#                 <small>Ask me anything!</small>\r\n+#             </div>\r\n+#         </div>\r\n+#         \"\"\", \r\n+#         unsafe_allow_html=True\r\n+#     )\r\n \r\n-# 📜 4. चैट इतिहास (Session State)\r\n-if \"messages\" not in st.session_state:\r\n-    st.session_state.messages = [\r\n-        {\"role\": \"assistant\", \"content\": \"Hello. How can I assist you today, particularly in relation to medical topics?\", \"time\": get_current_time()}\r\n-    ]\r\n+# # 📜 4. चैट इतिहास (Session State)\r\n+# if \"messages\" not in st.session_state:\r\n+#     st.session_state.messages = [\r\n+#         {\"role\": \"assistant\", \"content\": \"Hello. How can I assist you today, particularly in relation to medical topics?\", \"time\": get_current_time()}\r\n+#     ]\r\n \r\n-# 💬 पुराने संदेशों को प्रदर्शित करें\r\n-for message in st.session_state.messages:\r\n-    avatar = \"https://cdn-icons-png.flaticon.com/512/387/387569.png\" if message[\"role\"] == \"assistant\" else None\r\n+# # 💬 पुराने संदेशों को प्रदर्शित करें\r\n+# for message in st.session_state.messages:\r\n+#     avatar = \"https://cdn-icons-png.flaticon.com/512/387/387569.png\" if message[\"role\"] == \"assistant\" else None\r\n     \r\n-    with st.chat_message(message[\"role\"], avatar=avatar):\r\n-        st.markdown(message[\"content\"])\r\n-        st.markdown(f'<span class=\"timestamp\">{message[\"time\"]}</span>', unsafe_allow_html=True) \r\n+#     with st.chat_message(message[\"role\"], avatar=avatar):\r\n+#         st.markdown(message[\"content\"])\r\n+#         st.markdown(f'<span class=\"timestamp\">{message[\"time\"]}</span>', unsafe_allow_html=True) \r\n \r\n \r\n-# ⌨️ 5. नया उपयोगकर्ता इनपुट हैंडल करें\r\n-if prompt := st.chat_input(\"Type your message...\", key=\"chat_input_fixed\"):\r\n-    current_time = get_current_time()\r\n+# # ⌨️ 5. नया उपयोगकर्ता इनपुट हैंडल करें\r\n+# if prompt := st.chat_input(\"Type your message...\", key=\"chat_input_fixed\"):\r\n+#     current_time = get_current_time()\r\n     \r\n-    # 1. User Message\r\n-    user_message = {\"role\": \"user\", \"content\": prompt, \"time\": current_time}\r\n-    st.session_state.messages.append(user_message)\r\n+#     # 1. User Message\r\n+#     user_message = {\"role\": \"user\", \"content\": prompt, \"time\": current_time}\r\n+#     st.session_state.messages.append(user_message)\r\n     \r\n-    with st.chat_message(\"user\"):\r\n-        st.markdown(prompt)\r\n-        st.markdown(f'<span class=\"timestamp\">{current_time}</span>', unsafe_allow_html=True)\r\n+#     with st.chat_message(\"user\"):\r\n+#         st.markdown(prompt)\r\n+#         st.markdown(f'<span class=\"timestamp\">{current_time}</span>', unsafe_allow_html=True)\r\n \r\n-    # 2. Assistant Response\r\n-    with st.chat_message(\"assistant\", avatar=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\"):\r\n-        with st.spinner(\"Processing your query...\"):\r\n-            try:\r\n-                response = rag_chain.invoke({\"input\": prompt})\r\n-                if hasattr(response, \"content\"):\r\n-                    answer = response.content\r\n-                else:\r\n-                    answer = str(response)\r\n+#     # 2. Assistant Response\r\n+#     with st.chat_message(\"assistant\", avatar=\"https://cdn-icons-png.flaticon.com/512/387/387569.png\"):\r\n+#         with st.spinner(\"Processing your query...\"):\r\n+#             try:\r\n+#                 response = rag_chain.invoke({\"input\": prompt})\r\n+#                 if hasattr(response, \"content\"):\r\n+#                     answer = response.content\r\n+#                 else:\r\n+#                     answer = str(response)\r\n \r\n-            except Exception as e:\r\n-                answer = f\"Error: Could not get response from RAG chain. Details: {e}\"\r\n-                st.error(answer)\r\n+#             except Exception as e:\r\n+#                 answer = f\"Error: Could not get response from RAG chain. Details: {e}\"\r\n+#                 st.error(answer)\r\n \r\n-        st.markdown(answer)\r\n-        assistant_time = get_current_time()\r\n-        st.markdown(f'<span class=\"timestamp\">{assistant_time}</span>', unsafe_allow_html=True)\r\n+#         st.markdown(answer)\r\n+#         assistant_time = get_current_time()\r\n+#         st.markdown(f'<span class=\"timestamp\">{assistant_time}</span>', unsafe_allow_html=True)\r\n \r\n-    # 3. Add to History\r\n-    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"time\": assistant_time})\r\n+#     # 3. Add to History\r\n+#     st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"time\": assistant_time})\r\n     \r\n-    st.rerun()\n+#     st.rerun()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1765655379801,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -80,8 +80,10 @@\n     app.run(host=\"0.0.0.0\", port=8080, debug=True)\r\n \r\n \r\n \r\n+\r\n+\r\n # import streamlit as st\r\n # from dotenv import load_dotenv\r\n # import os\r\n # from operator import itemgetter\r\n"
                }
            ],
            "date": 1765626353157,
            "name": "Commit-0",
            "content": "\r\nfrom flask import Flask, render_template, jsonify, request\r\nfrom dotenv import load_dotenv\r\n\r\n\r\n\r\n\r\n\r\napp=Flask(__name__)\r\n\r\n\r\nload_dotenv()\r\n\r\n\r\n"
        }
    ]
}